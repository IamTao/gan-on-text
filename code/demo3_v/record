2017:03:02 23:33:47	reading and processing the text file
2017:03:02 23:33:58	flatmap a list of sentence list to a list of sentence.
2017:03:02 23:37:06	reading and processing the text file
2017:03:02 23:37:17	flatmap a list of sentence list to a list of sentence.
2017:03:02 23:37:35	mapping from index to word.
2017:03:02 23:37:35	mapping from word to index.
2017:03:02 23:39:30	reading and processing the text file
2017:03:02 23:39:30	preprocess the dataset.
2017:03:02 23:39:36	build a vocabulary.
2017:03:02 23:39:36	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:39:53	...mapping from index to word.
2017:03:02 23:39:53	...mapping from word to index.
2017:03:02 23:39:53	...map word to index.
2017:03:02 23:41:00	reading and processing the text file.
2017:03:02 23:41:00	preprocess the dataset.
2017:03:02 23:41:05	build a vocabulary.
2017:03:02 23:41:05	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:41:23	...mapping from index to word.
2017:03:02 23:41:23	...mapping from word to index.
2017:03:02 23:41:23	...map word to index.
2017:03:02 23:41:27	reading and processing the text file.
2017:03:02 23:41:27	preprocess the dataset.
2017:03:02 23:41:31	build a vocabulary.
2017:03:02 23:41:31	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:41:51	reading and processing the text file.
2017:03:02 23:41:51	preprocess the dataset.
2017:03:02 23:41:55	build a vocabulary.
2017:03:02 23:41:55	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:42:30	reading and processing the text file.
2017:03:02 23:42:30	preprocess the dataset.
2017:03:02 23:42:34	build a vocabulary.
2017:03:02 23:42:34	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:42:35	...mapping from index to word.
2017:03:02 23:42:35	...mapping from word to index.
2017:03:02 23:42:35	...map word to index.
2017:03:02 23:43:05	reading and processing the text file.
2017:03:02 23:43:05	preprocess the dataset.
2017:03:02 23:43:09	build a vocabulary.
2017:03:02 23:43:09	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:43:10	...mapping from index to word.
2017:03:02 23:43:10	...mapping from word to index.
2017:03:02 23:43:10	...map word to index.
2017:03:02 23:43:37	reading and processing the text file.
2017:03:02 23:43:37	preprocess the dataset.
2017:03:02 23:43:42	build a vocabulary.
2017:03:02 23:43:42	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:43:43	...mapping from index to word.
2017:03:02 23:43:43	...mapping from word to index.
2017:03:02 23:43:43	...map word to index.
2017:03:02 23:44:45	reading and processing the text file.
2017:03:02 23:44:45	preprocess the dataset.
2017:03:02 23:44:49	build a vocabulary.
2017:03:02 23:44:49	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:44:50	...mapping from index to word.
2017:03:02 23:44:50	...mapping from word to index.
2017:03:02 23:44:50	...map word to index.
2017:03:02 23:45:10	reading and processing the text file.
2017:03:02 23:45:10	preprocess the dataset.
2017:03:02 23:45:14	build a vocabulary.
2017:03:02 23:45:14	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:45:15	...mapping from index to word.
2017:03:02 23:45:15	...mapping from word to index.
2017:03:02 23:45:15	...map word to index.
2017:03:02 23:45:37	reading and processing the text file.
2017:03:02 23:45:37	preprocess the dataset.
2017:03:02 23:45:41	build a vocabulary.
2017:03:02 23:45:41	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:45:42	...mapping from index to word.
2017:03:02 23:45:42	...mapping from word to index.
2017:03:02 23:45:43	...map word to index.
2017:03:02 23:47:16	reading and processing the text file.
2017:03:02 23:47:16	preprocess the dataset.
2017:03:02 23:47:20	build a vocabulary.
2017:03:02 23:47:20	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:47:22	...mapping from index to word.
2017:03:02 23:47:22	...mapping from word to index.
2017:03:02 23:47:22	...map word to index.
2017:03:02 23:48:16	reading and processing the text file.
2017:03:02 23:48:16	preprocess the dataset.
2017:03:02 23:48:20	build a vocabulary.
2017:03:02 23:48:20	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:48:21	...mapping from index to word.
2017:03:02 23:48:22	...mapping from word to index.
2017:03:02 23:48:22	...map word to index.
2017:03:02 23:48:50	reading and processing the text file.
2017:03:02 23:48:50	preprocess the dataset.
2017:03:02 23:48:54	build a vocabulary.
2017:03:02 23:48:54	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:48:56	...mapping from index to word.
2017:03:02 23:48:56	...mapping from word to index.
2017:03:02 23:48:56	...map word to index.
2017:03:02 23:50:52	reading and processing the text file.
2017:03:02 23:50:52	preprocess the dataset.
2017:03:02 23:50:56	build a vocabulary.
2017:03:02 23:50:56	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:50:58	...mapping from index to word.
2017:03:02 23:50:58	...mapping from word to index.
2017:03:02 23:50:58	...map word to index.
2017:03:02 23:51:12	reading and processing the text file.
2017:03:02 23:51:12	preprocess the dataset.
2017:03:02 23:51:17	build a vocabulary.
2017:03:02 23:51:17	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:51:18	...mapping from index to word.
2017:03:02 23:51:18	...mapping from word to index.
2017:03:02 23:51:18	...map word to index.
2017:03:02 23:51:53	reading and processing the text file.
2017:03:02 23:51:53	preprocess the dataset.
2017:03:02 23:52:06	reading and processing the text file.
2017:03:02 23:52:06	preprocess the dataset.
2017:03:02 23:52:16	reading and processing the text file.
2017:03:02 23:52:16	preprocess the dataset.
2017:03:02 23:52:22	build a vocabulary.
2017:03:02 23:52:22	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:52:23	...mapping from index to word.
2017:03:02 23:52:23	...mapping from word to index.
2017:03:02 23:52:23	...map word to index.
2017:03:02 23:53:08	reading and processing the text file.
2017:03:02 23:53:08	preprocess the dataset.
2017:03:02 23:53:16	build a vocabulary.
2017:03:02 23:53:16	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:53:18	...mapping from index to word.
2017:03:02 23:53:18	...mapping from word to index.
2017:03:02 23:53:18	...map word to index.
2017:03:02 23:53:33	reading and processing the text file.
2017:03:02 23:53:33	preprocess the dataset.
2017:03:02 23:53:41	build a vocabulary.
2017:03:02 23:53:41	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:53:42	...mapping from index to word.
2017:03:02 23:53:42	...mapping from word to index.
2017:03:02 23:53:42	...map word to index.
2017:03:02 23:54:31	reading and processing the text file.
2017:03:02 23:54:31	preprocess the dataset.
2017:03:02 23:54:39	build a vocabulary.
2017:03:02 23:54:39	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:54:40	...mapping from index to word.
2017:03:02 23:54:40	...mapping from word to index.
2017:03:02 23:54:40	...map word to index.
2017:03:02 23:56:49	reading and processing the text file.
2017:03:02 23:56:49	preprocess the dataset.
2017:03:02 23:56:58	build a vocabulary.
2017:03:02 23:56:58	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:56:59	...mapping from index to word.
2017:03:02 23:56:59	...mapping from word to index.
2017:03:02 23:56:59	...map word to index.
2017:03:02 23:57:37	reading and processing the text file.
2017:03:02 23:57:37	preprocess the dataset.
2017:03:02 23:57:45	build a vocabulary.
2017:03:02 23:57:45	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:57:47	...mapping from index to word.
2017:03:02 23:57:47	...mapping from word to index.
2017:03:02 23:57:47	...map word to index.
2017:03:02 23:58:10	reading and processing the text file.
2017:03:02 23:58:10	preprocess the dataset.
2017:03:02 23:58:23	reading and processing the text file.
2017:03:02 23:58:23	preprocess the dataset.
2017:03:02 23:58:32	reading and processing the text file.
2017:03:02 23:58:32	preprocess the dataset.
2017:03:02 23:58:41	build a vocabulary.
2017:03:02 23:58:41	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:58:42	...mapping from index to word.
2017:03:02 23:58:42	...mapping from word to index.
2017:03:02 23:58:42	...map word to index.
2017:03:02 23:58:56	reading and processing the text file.
2017:03:02 23:58:56	preprocess the dataset.
2017:03:02 23:59:05	build a vocabulary.
2017:03:02 23:59:05	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:59:07	...mapping from index to word.
2017:03:02 23:59:07	...mapping from word to index.
2017:03:02 23:59:07	...map word to index.
2017:03:02 23:59:41	reading and processing the text file.
2017:03:02 23:59:41	preprocess the dataset.
2017:03:02 23:59:51	build a vocabulary.
2017:03:02 23:59:51	...flatmap a list of sentence list to a list of sentence.
2017:03:02 23:59:52	...mapping from index to word.
2017:03:02 23:59:53	...mapping from word to index.
2017:03:02 23:59:53	...map word to index.
2017:03:03 00:00:30	reading and processing the text file.
2017:03:03 00:00:30	preprocess the dataset.
2017:03:03 00:00:41	build a vocabulary.
2017:03:03 00:00:41	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:00:43	...mapping from index to word.
2017:03:03 00:00:43	...mapping from word to index.
2017:03:03 00:00:43	...map word to index.
2017:03:03 00:01:13	reading and processing the text file.
2017:03:03 00:01:13	preprocess the dataset.
2017:03:03 00:01:25	build a vocabulary.
2017:03:03 00:01:25	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:01:26	...mapping from index to word.
2017:03:03 00:01:26	...mapping from word to index.
2017:03:03 00:01:26	...map word to index.
2017:03:03 00:02:08	reading and processing the text file.
2017:03:03 00:02:08	preprocess the dataset.
2017:03:03 00:02:19	build a vocabulary.
2017:03:03 00:02:19	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:02:20	...mapping from index to word.
2017:03:03 00:02:20	...mapping from word to index.
2017:03:03 00:02:21	...map word to index.
2017:03:03 00:03:35	reading and processing the text file.
2017:03:03 00:03:35	preprocess the dataset.
2017:03:03 00:03:40	build a vocabulary.
2017:03:03 00:03:40	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:03:41	...mapping from index to word.
2017:03:03 00:03:41	...mapping from word to index.
2017:03:03 00:03:41	...map word to index.
2017:03:03 00:05:19	reading and processing the text file.
2017:03:03 00:05:19	preprocess the dataset.
2017:03:03 00:05:35	reading and processing the text file.
2017:03:03 00:05:35	preprocess the dataset.
2017:03:03 00:05:51	reading and processing the text file.
2017:03:03 00:05:51	preprocess the dataset.
2017:03:03 00:05:57	build a vocabulary.
2017:03:03 00:05:57	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:05:58	...mapping from index to word.
2017:03:03 00:05:58	...mapping from word to index.
2017:03:03 00:05:58	...map word to index.
2017:03:03 00:06:30	reading and processing the text file.
2017:03:03 00:06:30	preprocess the dataset.
2017:03:03 00:06:35	build a vocabulary.
2017:03:03 00:06:35	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:06:36	...mapping from index to word.
2017:03:03 00:06:36	...mapping from word to index.
2017:03:03 00:06:36	...map word to index.
2017:03:03 00:08:22	reading and processing the text file.
2017:03:03 00:08:22	preprocess the dataset.
2017:03:03 00:08:28	build a vocabulary.
2017:03:03 00:08:28	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:08:29	...mapping from index to word.
2017:03:03 00:08:29	...mapping from word to index.
2017:03:03 00:08:29	...map word to index.
2017:03:03 00:08:50	reading and processing the text file.
2017:03:03 00:08:50	preprocess the dataset.
2017:03:03 00:08:56	build a vocabulary.
2017:03:03 00:08:56	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:08:57	...mapping from index to word.
2017:03:03 00:08:57	...mapping from word to index.
2017:03:03 00:08:57	...map word to index.
2017:03:03 00:09:04	reading and processing the text file.
2017:03:03 00:09:04	preprocess the dataset.
2017:03:03 00:09:09	build a vocabulary.
2017:03:03 00:09:09	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:09:11	...mapping from index to word.
2017:03:03 00:09:11	...mapping from word to index.
2017:03:03 00:09:11	...map word to index.
2017:03:03 00:09:33	reading and processing the text file.
2017:03:03 00:09:33	preprocess the dataset.
2017:03:03 00:09:38	build a vocabulary.
2017:03:03 00:09:38	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:09:39	...mapping from index to word.
2017:03:03 00:09:39	...mapping from word to index.
2017:03:03 00:09:40	...map word to index.
2017:03:03 00:10:48	reading and processing the text file.
2017:03:03 00:10:48	preprocess the dataset.
2017:03:03 00:10:54	build a vocabulary.
2017:03:03 00:10:54	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:10:54	...mapping from index to word.
2017:03:03 00:10:54	...mapping from word to index.
2017:03:03 00:10:54	...map word to index.
2017:03:03 00:12:17	reading and processing the text file.
2017:03:03 00:12:17	preprocess the dataset.
2017:03:03 00:12:24	build a vocabulary.
2017:03:03 00:12:24	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:12:25	...mapping from index to word.
2017:03:03 00:12:25	...mapping from word to index.
2017:03:03 00:12:25	...map word to index.
2017:03:03 00:12:56	reading and processing the text file.
2017:03:03 00:12:56	preprocess the dataset.
2017:03:03 00:13:02	build a vocabulary.
2017:03:03 00:13:02	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:13:03	...mapping from index to word.
2017:03:03 00:13:03	...mapping from word to index.
2017:03:03 00:13:03	...map word to index.
2017:03:03 00:13:26	reading and processing the text file.
2017:03:03 00:13:26	preprocess the dataset.
2017:03:03 00:14:05	reading and processing the text file.
2017:03:03 00:14:05	preprocess the dataset.
2017:03:03 00:14:08	build a vocabulary.
2017:03:03 00:14:08	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:14:08	...mapping from index to word.
2017:03:03 00:14:08	...mapping from word to index.
2017:03:03 00:14:08	...map word to index.
2017:03:03 00:14:31	reading and processing the text file.
2017:03:03 00:14:31	preprocess the dataset.
2017:03:03 00:14:33	build a vocabulary.
2017:03:03 00:14:33	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:14:33	...mapping from index to word.
2017:03:03 00:14:33	...mapping from word to index.
2017:03:03 00:14:33	...map word to index.
2017:03:03 00:16:29	reading and processing the text file.
2017:03:03 00:16:29	preprocess the dataset.
2017:03:03 00:16:35	build a vocabulary.
2017:03:03 00:16:35	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:16:36	...mapping from index to word.
2017:03:03 00:16:36	...mapping from word to index.
2017:03:03 00:16:36	...map word to index.
2017:03:03 00:21:05	reading and processing the text file.
2017:03:03 00:21:05	preprocess the dataset.
2017:03:03 00:21:11	build a vocabulary.
2017:03:03 00:21:11	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:21:12	...mapping from index to word.
2017:03:03 00:21:12	...mapping from word to index.
2017:03:03 00:21:12	...map word to index.
2017:03:03 00:21:38	reading and processing the text file.
2017:03:03 00:21:38	preprocess the dataset.
2017:03:03 00:21:44	build a vocabulary.
2017:03:03 00:21:44	...flatmap a list of sentence list to a list of sentence.
2017:03:03 00:21:46	...mapping from index to word.
2017:03:03 00:21:46	...mapping from word to index.
2017:03:03 00:21:46	...map word to index.
2017:03:03 09:53:37	loading preprocessed files.
2017:03:03 09:54:07	loading preprocessed files.
2017:03:03 09:54:56	loading preprocessed files.
2017:03:03 09:55:36	loading preprocessed files.
2017:03:03 09:59:26	reading and processing the text file.
2017:03:03 09:59:26	preprocess the dataset.
2017:03:03 09:59:34	build a vocabulary.
2017:03:03 09:59:34	...flatmap a list of sentence list to a list of sentence.
2017:03:03 09:59:39	...mapping from index to word.
2017:03:03 09:59:39	...mapping from word to index.
2017:03:03 09:59:39	...map word to index.
2017:03:03 10:02:52	reading and processing the text file.
2017:03:03 10:02:52	preprocess the dataset.
2017:03:03 10:02:58	build a vocabulary.
2017:03:03 10:02:58	...flatmap a list of sentence list to a list of sentence.
2017:03:03 10:03:02	...mapping from index to word.
2017:03:03 10:03:02	...mapping from word to index.
2017:03:03 10:03:02	...map word to index.
2017:03:03 10:07:36	reading and processing the text file.
2017:03:03 10:07:36	preprocess the dataset.
2017:03:03 10:07:41	build a vocabulary.
2017:03:03 10:07:41	...flatmap a list of sentence list to a list of sentence.
2017:03:03 10:07:45	...mapping from index to word.
2017:03:03 10:07:45	...mapping from word to index.
2017:03:03 10:07:45	...map word to index.
2017:03:03 10:10:24	reading and processing the text file.
2017:03:03 10:10:24	preprocess the dataset.
2017:03:03 10:10:30	build a vocabulary.
2017:03:03 10:10:30	...flatmap a list of sentence list to a list of sentence.
2017:03:03 10:10:34	...mapping from index to word.
2017:03:03 10:10:34	...mapping from word to index.
2017:03:03 10:10:34	...map word to index.
2017:03:03 10:12:18	reading and processing the text file.
2017:03:03 10:12:18	preprocess the dataset.
2017:03:03 10:12:24	build a vocabulary.
2017:03:03 10:12:24	...flatmap a list of sentence list to a list of sentence.
2017:03:03 10:12:29	...mapping from index to word.
2017:03:03 10:12:29	...mapping from word to index.
2017:03:03 10:12:29	...map word to index.
2017:03:03 10:13:56	reading and processing the text file.
2017:03:03 10:13:56	preprocess the dataset.
2017:03:03 10:14:04	build a vocabulary.
2017:03:03 10:14:04	...flatmap a list of sentence list to a list of sentence.
2017:03:03 10:14:09	...mapping from index to word.
2017:03:03 10:14:09	...mapping from word to index.
2017:03:03 10:14:09	...map word to index.
2017:03:03 10:15:59	reading and processing the text file.
2017:03:03 10:15:59	preprocess the dataset.
2017:03:03 10:16:06	build a vocabulary.
2017:03:03 10:16:06	...flatmap a list of sentence list to a list of sentence.
2017:03:03 10:16:11	...mapping from index to word.
2017:03:03 10:16:11	...mapping from word to index.
2017:03:03 10:16:11	...map word to index.
2017:03:03 11:05:37	reading and processing the text file.
2017:03:03 11:05:37	preprocess the dataset.
2017:03:03 11:05:45	build a vocabulary.
2017:03:03 11:05:45	...flatmap a list of sentence list to a list of sentence.
2017:03:03 11:05:50	...mapping from index to word.
2017:03:03 11:05:50	...mapping from word to index.
2017:03:03 11:05:50	...map word to index.
2017:03:03 11:06:51	the vocabulary size is 40
2017:03:03 11:09:54	reading and processing the text file.
2017:03:03 11:09:54	preprocess the dataset.
2017:03:03 11:10:01	build a vocabulary.
2017:03:03 11:10:01	...flatmap a list of sentence list to a list of sentence.
2017:03:03 11:10:05	...mapping from index to word.
2017:03:03 11:10:05	...mapping from word to index.
2017:03:03 11:10:05	...map word to index.
2017:03:03 11:11:37	reading and processing the text file.
2017:03:03 11:11:37	preprocess the dataset.
2017:03:03 11:11:43	build a vocabulary.
2017:03:03 11:11:43	...flatmap a list of sentence list to a list of sentence.
2017:03:03 11:11:44	...mapping from index to word.
2017:03:03 11:11:44	...mapping from word to index.
2017:03:03 11:11:44	...map word to index.
2017:03:03 11:12:05	reading and processing the text file.
2017:03:03 11:12:05	preprocess the dataset.
2017:03:03 11:12:10	build a vocabulary.
2017:03:03 11:12:10	...flatmap a list of sentence list to a list of sentence.
2017:03:03 11:12:11	...mapping from index to word.
2017:03:03 11:12:11	...mapping from word to index.
2017:03:03 11:12:11	...map word to index.
2017:03:03 11:12:58	reading and processing the text file.
2017:03:03 11:12:58	preprocess the dataset.
2017:03:03 11:13:05	build a vocabulary.
2017:03:03 11:13:05	...flatmap a list of sentence list to a list of sentence.
2017:03:03 11:13:06	...mapping from index to word.
2017:03:03 11:13:06	...mapping from word to index.
2017:03:03 11:13:06	...map word to index.
2017:03:03 11:13:23	reading and processing the text file.
2017:03:03 11:13:23	preprocess the dataset.
2017:03:03 11:13:30	build a vocabulary.
2017:03:03 11:13:30	...flatmap a list of sentence list to a list of sentence.
2017:03:03 11:13:31	...mapping from index to word.
2017:03:03 11:13:32	...mapping from word to index.
2017:03:03 11:13:32	...map word to index.
2017:03:03 11:14:41	the vocabulary size is 29264
2017:03:03 13:49:46	loading preprocessed files.
2017:03:03 13:50:37	the vocabulary size is 29264.
2017:03:03 14:02:25	loading preprocessed files.
2017:03:03 14:07:19	reading and processing the text file.
2017:03:03 14:07:19	preprocess the dataset.
2017:03:03 14:07:24	build a vocabulary.
2017:03:03 14:07:24	...flatmap a list of sentence list to a list of sentence.
2017:03:03 14:07:25	...mapping from index to word.
2017:03:03 14:07:26	...mapping from word to index.
2017:03:03 14:07:26	...map word to index.
2017:03:03 14:13:54	reading and processing the text file.
2017:03:03 14:13:54	preprocess the dataset.
2017:03:03 14:14:00	build a vocabulary.
2017:03:03 14:14:00	...flatmap a list of sentence list to a list of sentence.
2017:03:03 14:14:02	...mapping from index to word.
2017:03:03 14:14:02	...mapping from word to index.
2017:03:03 14:14:02	...map word to index.
2017:03:03 14:15:07	the vocabulary size is 29264.
2017:03:03 14:25:50	reading and processing the text file.
2017:03:03 14:25:50	preprocess the dataset.
2017:03:03 14:26:25	reading and processing the text file.
2017:03:03 14:26:25	preprocess the dataset.
2017:03:03 14:33:12	reading and processing the text file.
2017:03:03 14:33:12	preprocess the dataset and extract the valid content.
2017:03:03 14:34:33	reading and processing the text file.
2017:03:03 14:34:33	preprocess the dataset and extract the valid content.
2017:03:03 14:34:33	init data from the raw dataset.
2017:03:03 14:36:19	reading and processing the text file.
2017:03:03 14:36:19	preprocess the dataset and extract the valid content.
2017:03:03 14:36:19	init data from the raw dataset.
2017:03:03 14:36:44	...Parse the emails into a list email objects
2017:03:03 14:38:10	...Get fields from parsed email objects
2017:03:03 14:38:31	...Parse content from emails
2017:03:03 14:38:34	...Split multiple email addresses
2017:03:03 14:38:41	...Extract the root of 'file' as 'user'
2017:03:03 14:38:42	extract content from email data frame.
2017:03:03 14:39:14	reading and processing the text file.
2017:03:03 14:39:14	preprocess the dataset and extract the valid content.
2017:03:03 14:39:49	reading and processing the text file.
2017:03:03 14:39:49	preprocess the dataset and extract the valid content.
2017:03:03 14:40:00	reading and processing the text file.
2017:03:03 14:40:00	preprocess the dataset and extract the valid content.
2017:03:03 14:40:00	init data from the raw dataset.
2017:03:03 14:40:24	...Parse the emails into a list email objects
2017:03:03 14:41:44	...Get fields from parsed email objects
2017:03:03 14:42:03	...Parse content from emails
2017:03:03 14:42:05	...Split multiple email addresses
2017:03:03 14:42:10	...Extract the root of 'file' as 'user'
2017:03:03 14:42:11	extract content from email data frame.
2017:03:03 14:45:37	reading and processing the text file.
2017:03:03 14:45:37	preprocess the dataset and extract the valid content.
2017:03:03 14:45:37	init data from the raw dataset.
2017:03:03 14:46:01	...Parse the emails into a list email objects
2017:03:03 14:47:22	...Get fields from parsed email objects
2017:03:03 14:47:42	...Parse content from emails
2017:03:03 14:47:44	...Split multiple email addresses
2017:03:03 14:47:49	...Extract the root of 'file' as 'user'
2017:03:03 14:47:50	extract content from email data frame.
2017:03:03 14:49:06	reading and processing the text file.
2017:03:03 14:49:06	preprocess the dataset and extract the valid content.
2017:03:03 14:49:47	reading and processing the text file.
2017:03:03 14:49:47	preprocess the dataset and extract the valid content.
2017:03:03 14:49:47	init data from the raw dataset.
2017:03:03 14:50:11	...Parse the emails into a list email objects
2017:03:03 14:51:34	...Get fields from parsed email objects
2017:03:03 14:51:54	...Parse content from emails
2017:03:03 14:51:57	...Split multiple email addresses
2017:03:03 14:52:01	...Extract the root of 'file' as 'user'
2017:03:03 14:52:03	extract content from email data frame.
2017:03:03 15:07:09	reading and processing the text file.
2017:03:03 15:07:09	preprocess the dataset and extract the valid content.
2017:03:03 15:07:09	init data from the raw dataset.
2017:03:03 15:08:27	reading and processing the text file.
2017:03:03 15:08:27	preprocess the dataset and extract the valid content.
2017:03:03 15:08:27	init data from the raw dataset.
2017:03:03 15:09:43	reading and processing the text file.
2017:03:03 15:09:43	preprocess the dataset and extract the valid content.
2017:03:03 15:10:11	reading and processing the text file.
2017:03:03 15:10:11	preprocess the dataset and extract the valid content.
2017:03:03 15:10:11	init data from the raw dataset.
2017:03:03 15:10:43	reading and processing the text file.
2017:03:03 15:10:43	preprocess the dataset and extract the valid content.
2017:03:03 15:11:59	reading and processing the text file.
2017:03:03 15:11:59	preprocess the dataset and extract the valid content.
2017:03:03 15:11:59	init data from the raw dataset.
2017:03:03 15:13:17	reading and processing the text file.
2017:03:03 15:13:17	preprocess the dataset and extract the valid content.
2017:03:03 15:13:29	reading and processing the text file.
2017:03:03 15:13:29	preprocess the dataset and extract the valid content.
2017:03:03 15:13:29	init data from the raw dataset.
2017:03:03 15:14:56	reading and processing the text file.
2017:03:03 15:14:56	preprocess the dataset and extract the valid content.
2017:03:03 15:15:05	reading and processing the text file.
2017:03:03 15:15:05	preprocess the dataset and extract the valid content.
2017:03:03 15:15:05	init data from the raw dataset.
2017:03:03 15:16:57	reading and processing the text file.
2017:03:03 15:16:57	preprocess the dataset and extract the valid content.
2017:03:03 15:17:07	reading and processing the text file.
2017:03:03 15:17:07	preprocess the dataset and extract the valid content.
2017:03:03 15:17:07	init data from the raw dataset.
2017:03:03 15:17:22	reading and processing the text file.
2017:03:03 15:17:22	preprocess the dataset and extract the valid content.
2017:03:03 15:17:30	reading and processing the text file.
2017:03:03 15:17:30	preprocess the dataset and extract the valid content.
2017:03:03 15:17:30	init data from the raw dataset.
2017:03:03 15:19:05	reading and processing the text file.
2017:03:03 15:19:05	preprocess the dataset and extract the valid content.
2017:03:03 15:19:13	reading and processing the text file.
2017:03:03 15:19:13	preprocess the dataset and extract the valid content.
2017:03:03 15:19:13	init data from the raw dataset.
2017:03:03 15:19:54	reading and processing the text file.
2017:03:03 15:19:54	preprocess the dataset and extract the valid content.
2017:03:03 15:19:54	init data from the raw dataset.
2017:03:03 15:22:10	reading and processing the text file.
2017:03:03 15:22:10	preprocess the dataset and extract the valid content.
2017:03:03 15:22:10	...load data.
2017:03:03 15:22:10	...clean data.
2017:03:03 15:22:20	reading and processing the text file.
2017:03:03 15:22:20	preprocess the dataset and extract the valid content.
2017:03:03 15:22:20	...load data.
2017:03:03 15:22:20	...clean data.
2017:03:03 15:22:39	reading and processing the text file.
2017:03:03 15:22:39	preprocess the dataset and extract the valid content.
2017:03:03 15:22:39	...load data.
2017:03:03 15:22:39	...clean data.
2017:03:03 15:23:45	reading and processing the text file.
2017:03:03 15:23:45	preprocess the dataset and extract the valid content.
2017:03:03 15:23:45	...load data.
2017:03:03 15:23:45	...clean data.
2017:03:03 15:24:59	reading and processing the text file.
2017:03:03 15:24:59	preprocess the dataset and extract the valid content.
2017:03:03 15:24:59	...load data.
2017:03:03 15:24:59	...clean data.
2017:03:03 15:25:10	reading and processing the text file.
2017:03:03 15:25:10	preprocess the dataset and extract the valid content.
2017:03:03 15:25:10	...load data.
2017:03:03 15:25:10	...clean data.
2017:03:03 15:25:26	reading and processing the text file.
2017:03:03 15:25:26	preprocess the dataset and extract the valid content.
2017:03:03 15:25:26	...load data.
2017:03:03 15:25:26	...clean data.
2017:03:03 15:25:49	reading and processing the text file.
2017:03:03 15:25:49	preprocess the dataset and extract the valid content.
2017:03:03 15:25:49	...load data.
2017:03:03 15:25:49	...clean data.
2017:03:03 15:26:41	reading and processing the text file.
2017:03:03 15:26:41	preprocess the dataset and extract the valid content.
2017:03:03 15:26:41	...load data.
2017:03:03 15:26:41	...clean data.
2017:03:03 15:27:15	reading and processing the text file.
2017:03:03 15:27:15	preprocess the dataset and extract the valid content.
2017:03:03 15:27:15	...load data.
2017:03:03 15:27:15	...clean data.
2017:03:03 15:27:39	reading and processing the text file.
2017:03:03 15:27:39	preprocess the dataset and extract the valid content.
2017:03:03 15:27:39	...load data.
2017:03:03 15:27:39	...clean data.
2017:03:03 15:29:06	reading and processing the text file.
2017:03:03 15:29:06	preprocess the dataset and extract the valid content.
2017:03:03 15:29:06	...load data.
2017:03:03 15:29:06	...clean data.
2017:03:03 15:30:18	reading and processing the text file.
2017:03:03 15:30:18	preprocess the dataset and extract the valid content.
2017:03:03 15:30:18	...load data.
2017:03:03 15:30:18	...clean data.
2017:03:03 15:33:23	reading and processing the text file.
2017:03:03 15:33:23	preprocess the dataset and extract the valid content.
2017:03:03 15:33:23	...load data.
2017:03:03 15:33:23	...clean data.
2017:03:03 15:34:20	reading and processing the text file.
2017:03:03 15:34:20	preprocess the dataset and extract the valid content.
2017:03:03 15:34:20	...load data.
2017:03:03 15:34:20	...clean data.
2017:03:03 15:34:25	reading and processing the text file.
2017:03:03 15:34:25	preprocess the dataset and extract the valid content.
2017:03:03 15:34:25	...load data.
2017:03:03 15:34:25	...clean data.
2017:03:03 15:35:45	reading and processing the text file.
2017:03:03 15:35:45	preprocess the dataset and extract the valid content.
2017:03:03 15:35:45	...load data.
2017:03:03 15:35:45	...clean data.
2017:03:03 15:36:06	reading and processing the text file.
2017:03:03 15:36:06	preprocess the dataset and extract the valid content.
2017:03:03 15:36:06	...load data.
2017:03:03 15:36:06	...clean data.
2017:03:03 15:36:20	reading and processing the text file.
2017:03:03 15:36:20	preprocess the dataset and extract the valid content.
2017:03:03 15:36:20	...load data.
2017:03:03 15:36:20	...clean data.
2017:03:03 15:36:37	reading and processing the text file.
2017:03:03 15:36:37	preprocess the dataset and extract the valid content.
2017:03:03 15:36:37	...load data.
2017:03:03 15:36:37	...clean data.
2017:03:03 15:37:12	reading and processing the text file.
2017:03:03 15:37:12	preprocess the dataset and extract the valid content.
2017:03:03 15:37:12	...load data.
2017:03:03 15:37:12	...clean data.
2017:03:03 15:37:28	reading and processing the text file.
2017:03:03 15:37:28	preprocess the dataset and extract the valid content.
2017:03:03 15:37:28	...load data.
2017:03:03 15:37:28	...clean data.
2017:03:03 15:38:08	reading and processing the text file.
2017:03:03 15:38:08	preprocess the dataset and extract the valid content.
2017:03:03 15:38:08	...load data.
2017:03:03 15:38:08	...clean data.
2017:03:03 15:38:39	reading and processing the text file.
2017:03:03 15:38:39	preprocess the dataset and extract the valid content.
2017:03:03 15:38:39	...load data.
2017:03:03 15:38:39	...clean data.
2017:03:03 15:38:54	reading and processing the text file.
2017:03:03 15:38:54	preprocess the dataset and extract the valid content.
2017:03:03 15:38:54	...load data.
2017:03:03 15:38:54	...clean data.
2017:03:03 15:39:09	reading and processing the text file.
2017:03:03 15:39:09	preprocess the dataset and extract the valid content.
2017:03:03 15:39:09	...load data.
2017:03:03 15:39:09	...clean data.
2017:03:03 15:39:32	reading and processing the text file.
2017:03:03 15:39:32	preprocess the dataset and extract the valid content.
2017:03:03 15:39:32	...load data.
2017:03:03 15:39:32	...clean data.
2017:03:03 15:40:03	reading and processing the text file.
2017:03:03 15:40:03	preprocess the dataset and extract the valid content.
2017:03:03 15:40:03	...load data.
2017:03:03 15:40:03	...clean data.
2017:03:03 15:41:35	reading and processing the text file.
2017:03:03 15:41:35	preprocess the dataset and extract the valid content.
2017:03:03 15:41:35	...load data.
2017:03:03 15:41:35	...clean data.
2017:03:03 15:41:54	reading and processing the text file.
2017:03:03 15:41:54	preprocess the dataset and extract the valid content.
2017:03:03 15:41:54	...load data.
2017:03:03 15:41:54	...clean data.
2017:03:03 15:42:16	reading and processing the text file.
2017:03:03 15:42:16	preprocess the dataset and extract the valid content.
2017:03:03 15:42:16	...load data.
2017:03:03 15:42:16	...clean data.
2017:03:03 15:42:51	reading and processing the text file.
2017:03:03 15:42:51	preprocess the dataset and extract the valid content.
2017:03:03 15:42:51	...load data.
2017:03:03 15:42:51	...clean data.
2017:03:03 15:44:22	reading and processing the text file.
2017:03:03 15:44:22	preprocess the dataset and extract the valid content.
2017:03:03 15:44:22	...load data.
2017:03:03 15:44:22	...clean data.
2017:03:03 15:44:45	reading and processing the text file.
2017:03:03 15:44:45	preprocess the dataset and extract the valid content.
2017:03:03 15:44:45	...load data.
2017:03:03 15:44:45	...clean data.
2017:03:03 15:45:46	reading and processing the text file.
2017:03:03 15:45:46	preprocess the dataset and extract the valid content.
2017:03:03 15:45:46	...load data.
2017:03:03 15:45:46	...clean data.
2017:03:03 15:45:53	reading and processing the text file.
2017:03:03 15:45:53	preprocess the dataset and extract the valid content.
2017:03:03 15:45:53	...load data.
2017:03:03 15:45:53	...clean data.
2017:03:03 15:46:54	reading and processing the text file.
2017:03:03 15:46:54	preprocess the dataset and extract the valid content.
2017:03:03 15:46:54	...load data.
2017:03:03 15:46:54	...clean data.
2017:03:03 15:47:38	reading and processing the text file.
2017:03:03 15:47:38	preprocess the dataset and extract the valid content.
2017:03:03 15:47:38	...load data.
2017:03:03 15:47:38	...clean data.
2017:03:03 15:47:54	reading and processing the text file.
2017:03:03 15:47:54	preprocess the dataset and extract the valid content.
2017:03:03 15:47:54	...load data.
2017:03:03 15:47:54	...clean data.
2017:03:03 15:48:02	reading and processing the text file.
2017:03:03 15:48:02	preprocess the dataset and extract the valid content.
2017:03:03 15:48:02	...load data.
2017:03:03 15:48:02	...clean data.
2017:03:03 15:48:59	reading and processing the text file.
2017:03:03 15:48:59	preprocess the dataset and extract the valid content.
2017:03:03 15:48:59	...load data.
2017:03:03 15:48:59	...clean data.
2017:03:03 15:50:26	reading and processing the text file.
2017:03:03 15:50:26	preprocess the dataset and extract the valid content.
2017:03:03 15:50:26	...load data.
2017:03:03 15:50:26	...clean data.
2017:03:03 15:51:11	reading and processing the text file.
2017:03:03 15:51:11	preprocess the dataset and extract the valid content.
2017:03:03 15:51:11	...load data.
2017:03:03 15:51:11	...clean data.
2017:03:03 15:51:12	build a vocabulary.
2017:03:03 15:51:12	...flatmap a list of sentence list to a list of sentence.
2017:03:03 15:51:14	...mapping from index to word.
2017:03:03 15:51:14	...mapping from word to index.
2017:03:03 15:51:14	...map word to index.
2017:03:03 15:51:14	the vocabulary size is 62.
2017:03:03 15:51:52	reading and processing the text file.
2017:03:03 15:51:52	preprocess the dataset and extract the valid content.
2017:03:03 15:51:52	...load data.
2017:03:03 15:51:52	...clean data.
2017:03:03 15:51:53	build a vocabulary.
2017:03:03 15:51:53	...flatmap a list of sentence list to a list of sentence.
2017:03:03 15:51:54	...mapping from index to word.
2017:03:03 15:51:54	...mapping from word to index.
2017:03:03 15:51:54	...map word to index.
2017:03:03 15:51:54	the vocabulary size is 62.
2017:03:03 15:53:14	reading and processing the text file.
2017:03:03 15:53:14	preprocess the dataset and extract the valid content.
2017:03:03 15:53:14	...load data.
2017:03:03 15:53:14	...clean data.
2017:03:03 15:54:32	reading and processing the text file.
2017:03:03 15:54:32	preprocess the dataset and extract the valid content.
2017:03:03 15:54:32	...load data.
2017:03:03 15:54:32	...clean data.
2017:03:03 15:54:33	build a vocabulary.
2017:03:03 15:54:33	...flatmap a list of sentence list to a list of sentence.
2017:03:03 15:54:33	...mapping from index to word.
2017:03:03 15:54:33	...mapping from word to index.
2017:03:03 15:54:33	...map word to index.
2017:03:03 15:54:33	the vocabulary size is 34472.
2017:03:03 15:56:56	reading and processing the text file.
2017:03:03 15:56:56	preprocess the dataset.
2017:03:03 15:57:01	build a vocabulary.
2017:03:03 15:57:01	...flatmap a list of sentence list to a list of sentence.
2017:03:03 15:57:02	...mapping from index to word.
2017:03:03 15:57:02	...mapping from word to index.
2017:03:03 15:57:02	...map word to index.
2017:03:03 15:57:12	reading and processing the text file.
2017:03:03 15:57:12	preprocess the dataset.
2017:03:03 15:57:17	build a vocabulary.
2017:03:03 15:57:17	...flatmap a list of sentence list to a list of sentence.
2017:03:03 15:57:18	...mapping from index to word.
2017:03:03 15:57:18	...mapping from word to index.
2017:03:03 15:57:18	...map word to index.
2017:03:03 15:58:10	the vocabulary size is 29265.
2017:03:03 16:00:13	reading and processing the text file.
2017:03:03 16:00:13	preprocess the dataset and extract the valid content.
2017:03:03 16:00:13	...load data.
2017:03:03 16:00:13	...clean data.
2017:03:03 16:00:13	build a vocabulary.
2017:03:03 16:00:13	...flatmap a list of sentence list to a list of sentence.
2017:03:03 16:00:14	...mapping from index to word.
2017:03:03 16:00:14	...mapping from word to index.
2017:03:03 16:00:14	...map word to index.
2017:03:03 16:00:29	the vocabulary size is 34472.
2017:03:03 16:01:22	use DataLoaderBBC to init data.
2017:03:03 16:01:22	reading and processing the text file.
2017:03:03 16:01:22	preprocess the dataset and extract the valid content.
2017:03:03 16:01:22	...load data.
2017:03:03 16:01:22	...clean data.
2017:03:03 16:01:23	build a vocabulary.
2017:03:03 16:01:23	...flatmap a list of sentence list to a list of sentence.
2017:03:03 16:01:23	...mapping from index to word.
2017:03:03 16:01:24	...mapping from word to index.
2017:03:03 16:01:24	...map word to index.
2017:03:03 16:01:42	use DataLoaderBBC to init data.
2017:03:03 16:01:42	reading and processing the text file.
2017:03:03 16:01:42	preprocess the dataset and extract the valid content.
2017:03:03 16:01:42	...load data.
2017:03:03 16:01:42	...clean data.
2017:03:03 16:01:43	build a vocabulary.
2017:03:03 16:01:43	...flatmap a list of sentence list to a list of sentence.
2017:03:03 16:01:43	...mapping from index to word.
2017:03:03 16:01:43	...mapping from word to index.
2017:03:03 16:01:43	...map word to index.
2017:03:03 16:02:00	the vocabulary size is 34472.
2017:03:03 16:02:11	use DataLoaderBBC to init data.
2017:03:03 16:02:11	reading and processing the text file.
2017:03:03 16:02:11	preprocess the dataset and extract the valid content.
2017:03:03 16:02:11	...load data.
2017:03:03 16:02:11	...clean data.
2017:03:03 16:02:12	build a vocabulary.
2017:03:03 16:02:12	...flatmap a list of sentence list to a list of sentence.
2017:03:03 16:02:12	...mapping from index to word.
2017:03:03 16:02:12	...mapping from word to index.
2017:03:03 16:02:12	...map word to index.
2017:03:03 16:02:27	the vocabulary size is 34472.
2017:03:03 16:02:46	use DataLoaderChildrenStory to init data.
2017:03:03 16:02:46	reading and processing the text file.
2017:03:03 16:02:46	preprocess the dataset.
2017:03:03 16:02:51	build a vocabulary.
2017:03:03 16:02:51	...flatmap a list of sentence list to a list of sentence.
2017:03:03 16:02:52	...mapping from index to word.
2017:03:03 16:02:52	...mapping from word to index.
2017:03:03 16:02:52	...map word to index.
2017:03:03 16:03:37	the vocabulary size is 29265.
2017:03:03 16:04:12	use DataLoaderChildrenStory to init data.
2017:03:03 16:04:12	reading and processing the text file.
2017:03:03 16:04:12	preprocess the dataset.
2017:03:03 16:04:17	build a vocabulary.
2017:03:03 16:04:17	...flatmap a list of sentence list to a list of sentence.
2017:03:03 16:04:18	...mapping from index to word.
2017:03:03 16:04:18	...mapping from word to index.
2017:03:03 16:04:18	...map word to index.
2017:03:03 16:05:10	the vocabulary size is 29265.
2017:03:03 16:07:22	use DataLoaderChildrenStory to init data.
2017:03:03 16:07:22	reading and processing the text file.
2017:03:03 16:07:22	preprocess the dataset.
2017:03:03 16:07:27	build a vocabulary.
2017:03:03 16:07:27	...flatmap a list of sentence list to a list of sentence.
2017:03:03 16:07:28	...mapping from index to word.
2017:03:03 16:07:29	...mapping from word to index.
2017:03:03 16:07:29	...map word to index.
2017:03:03 16:08:29	the vocabulary size is 29265.
2017:03:03 16:08:47	use DataLoaderBBC to init data.
2017:03:03 16:08:47	reading and processing the text file.
2017:03:03 16:08:47	preprocess the dataset and extract the valid content.
2017:03:03 16:08:47	...load data.
2017:03:03 16:08:47	...clean data.
2017:03:03 16:08:48	build a vocabulary.
2017:03:03 16:08:48	...flatmap a list of sentence list to a list of sentence.
2017:03:03 16:08:48	...mapping from index to word.
2017:03:03 16:08:49	...mapping from word to index.
2017:03:03 16:08:49	...map word to index.
2017:03:03 16:09:18	the vocabulary size is 34472.
2017:03:03 16:11:02	use DataLoaderBBC to init data.
2017:03:03 16:11:02	reading and processing the text file.
2017:03:03 16:11:02	preprocess the dataset and extract the valid content.
2017:03:03 16:11:02	...load data.
2017:03:03 16:11:02	...clean data.
2017:03:03 16:11:03	build a vocabulary.
2017:03:03 16:11:03	...flatmap a list of sentence list to a list of sentence.
2017:03:03 16:11:03	...mapping from index to word.
2017:03:03 16:11:04	...mapping from word to index.
2017:03:03 16:11:04	...map word to index.
2017:03:03 16:11:27	the vocabulary size is 34472.
2017:03:03 16:12:52	use DataLoaderBBC to init data.
2017:03:03 16:12:52	reading and processing the text file.
2017:03:03 16:12:52	preprocess the dataset and extract the valid content.
2017:03:03 16:12:52	...load data.
2017:03:03 16:12:52	...clean data.
2017:03:03 16:12:53	build a vocabulary.
2017:03:03 16:12:53	...flatmap a list of sentence list to a list of sentence.
2017:03:03 16:12:53	...mapping from index to word.
2017:03:03 16:12:53	...mapping from word to index.
2017:03:03 16:12:53	...map word to index.
2017:03:03 16:13:09	the vocabulary size is 34472.
2017:03:03 16:13:44	use DataLoaderChildrenStory to init data.
2017:03:03 16:13:44	reading and processing the text file.
2017:03:03 16:13:44	preprocess the dataset.
2017:03:03 16:13:49	build a vocabulary.
2017:03:03 16:13:49	...flatmap a list of sentence list to a list of sentence.
2017:03:03 16:13:50	...mapping from index to word.
2017:03:03 16:13:51	...mapping from word to index.
2017:03:03 16:13:51	...map word to index.
2017:03:03 16:14:50	the vocabulary size is 29265.
2017:03:03 16:16:20	use DataLoaderEnron to init data.
2017:03:03 16:16:20	reading and processing the text file.
2017:03:03 16:16:20	preprocess the dataset and extract the valid content.
2017:03:03 16:17:33	use DataLoaderChildrenStory to init data.
2017:03:03 16:17:33	reading and processing the text file.
2017:03:03 16:17:33	preprocess the dataset.
2017:03:03 16:17:38	build a vocabulary.
2017:03:03 16:17:38	...flatmap a list of sentence list to a list of sentence.
2017:03:03 16:17:39	...mapping from index to word.
2017:03:03 16:17:39	...mapping from word to index.
2017:03:03 16:17:39	...map word to index.
2017:03:03 16:18:39	the vocabulary size is 29265.
2017:03:03 16:19:50	use DataLoaderBBC to init data.
2017:03:03 16:19:50	reading and processing the text file.
2017:03:03 16:19:50	preprocess the dataset and extract the valid content.
2017:03:03 16:19:50	...load data.
2017:03:03 16:19:50	...clean data.
2017:03:03 16:19:51	build a vocabulary.
2017:03:03 16:19:51	...flatmap a list of sentence list to a list of sentence.
2017:03:03 16:19:52	...mapping from index to word.
2017:03:03 16:19:52	...mapping from word to index.
2017:03:03 16:19:52	...map word to index.
2017:03:03 16:20:10	the vocabulary size is 34472.
2017:03:03 16:20:45	use DataLoaderEnron to init data.
2017:03:03 16:20:45	reading and processing the text file.
2017:03:03 16:20:45	preprocess the dataset and extract the valid content.
2017:03:03 16:20:45	init data from the raw dataset.
2017:03:03 16:21:07	...Parse the emails into a list email objects
2017:03:03 16:22:40	...Get fields from parsed email objects
2017:03:03 16:23:00	...Parse content from emails
2017:03:03 16:23:03	...Split multiple email addresses
2017:03:03 16:23:07	...Extract the root of 'file' as 'user'
2017:03:03 16:23:09	extract content from email data frame.
2017:03:03 16:29:27	use DataLoaderEnron to init data.
2017:03:03 16:29:27	reading and processing the text file.
2017:03:03 16:29:27	preprocess the dataset and extract the valid content.
2017:03:03 16:29:27	...load data.
2017:03:03 16:29:27	load context for further preprocessing.
2017:03:03 16:29:34	...clean data.
2017:03:03 16:31:00	use DataLoaderEnron to init data.
2017:03:03 16:31:00	reading and processing the text file.
2017:03:03 16:31:00	preprocess the dataset and extract the valid content.
2017:03:03 16:31:00	...load data.
2017:03:03 16:31:00	......load context for further preprocessing.
2017:03:03 16:31:01	...clean data.
2017:03:03 16:32:10	use DataLoaderEnron to init data.
2017:03:03 16:32:10	reading and processing the text file.
2017:03:03 16:32:10	preprocess the dataset and extract the valid content.
2017:03:03 16:32:10	...load data.
2017:03:03 16:32:10	......load context for further preprocessing.
2017:03:03 16:32:10	...clean data.
2017:03:03 16:32:47	use DataLoaderEnron to init data.
2017:03:03 16:32:47	reading and processing the text file.
2017:03:03 16:32:47	preprocess the dataset and extract the valid content.
2017:03:03 16:32:47	...load data.
2017:03:03 16:32:47	......load context for further preprocessing.
2017:03:03 16:32:48	...clean data.
2017:03:03 16:33:54	use DataLoaderEnron to init data.
2017:03:03 16:33:54	reading and processing the text file.
2017:03:03 16:33:54	preprocess the dataset and extract the valid content.
2017:03:03 16:33:54	...load data.
2017:03:03 16:33:54	......load context for further preprocessing.
2017:03:03 16:33:55	...clean data.
2017:03:03 16:36:11	use DataLoaderEnron to init data.
2017:03:03 16:36:11	reading and processing the text file.
2017:03:03 16:36:11	preprocess the dataset and extract the valid content.
2017:03:03 16:36:11	...load data.
2017:03:03 16:36:11	......load context for further preprocessing.
2017:03:03 16:36:12	...clean data.
2017:03:03 16:36:23	use DataLoaderEnron to init data.
2017:03:03 16:36:23	reading and processing the text file.
2017:03:03 16:36:23	preprocess the dataset and extract the valid content.
2017:03:03 16:36:23	...load data.
2017:03:03 16:36:23	......load context for further preprocessing.
2017:03:03 16:36:23	...clean data.
2017:03:03 16:36:40	use DataLoaderEnron to init data.
2017:03:03 16:36:40	reading and processing the text file.
2017:03:03 16:36:40	preprocess the dataset and extract the valid content.
2017:03:03 16:36:40	...load data.
2017:03:03 16:36:40	......load context for further preprocessing.
2017:03:03 16:36:40	...clean data.
2017:03:03 16:56:39	use DataLoaderWikiText to init data.
2017:03:03 16:56:39	reading and processing the text file.
2017:03:03 16:56:39	preprocess the dataset.
2017:03:03 16:57:35	use DataLoaderWikiText to init data.
2017:03:03 16:57:35	reading and processing the text file.
2017:03:03 16:57:35	preprocess the dataset.
2017:03:03 16:59:38	use DataLoaderWikiText to init data.
2017:03:03 16:59:38	reading and processing the text file.
2017:03:03 16:59:38	preprocess the dataset.
2017:03:03 16:59:49	use DataLoaderWikiText to init data.
2017:03:03 16:59:49	reading and processing the text file.
2017:03:03 16:59:49	preprocess the dataset.
2017:03:03 17:00:23	use DataLoaderWikiText to init data.
2017:03:03 17:00:23	reading and processing the text file.
2017:03:03 17:00:23	preprocess the dataset.
2017:03:03 17:00:45	use DataLoaderWikiText to init data.
2017:03:03 17:00:45	reading and processing the text file.
2017:03:03 17:00:45	preprocess the dataset.
2017:03:03 17:01:47	use DataLoaderWikiText to init data.
2017:03:03 17:01:47	reading and processing the text file.
2017:03:03 17:01:47	preprocess the dataset.
2017:03:03 17:02:32	use DataLoaderWikiText to init data.
2017:03:03 17:02:32	reading and processing the text file.
2017:03:03 17:02:32	preprocess the dataset.
2017:03:03 17:02:58	use DataLoaderWikiText to init data.
2017:03:03 17:02:58	reading and processing the text file.
2017:03:03 17:02:58	preprocess the dataset.
2017:03:03 17:03:07	use DataLoaderWikiText to init data.
2017:03:03 17:03:07	reading and processing the text file.
2017:03:03 17:03:07	preprocess the dataset.
2017:03:03 17:03:44	use DataLoaderWikiText to init data.
2017:03:03 17:03:44	reading and processing the text file.
2017:03:03 17:03:44	preprocess the dataset.
2017:03:03 17:04:12	use DataLoaderWikiText to init data.
2017:03:03 17:04:12	reading and processing the text file.
2017:03:03 17:04:12	preprocess the dataset.
2017:03:03 17:04:54	use DataLoaderWikiText to init data.
2017:03:03 17:04:54	reading and processing the text file.
2017:03:03 17:04:54	preprocess the dataset.
2017:03:03 17:05:22	use DataLoaderWikiText to init data.
2017:03:03 17:05:22	reading and processing the text file.
2017:03:03 17:05:22	preprocess the dataset.
2017:03:03 17:05:52	use DataLoaderWikiText to init data.
2017:03:03 17:05:52	reading and processing the text file.
2017:03:03 17:05:52	preprocess the dataset.
2017:03:03 17:06:04	use DataLoaderWikiText to init data.
2017:03:03 17:06:04	reading and processing the text file.
2017:03:03 17:06:04	preprocess the dataset.
2017:03:03 17:06:50	use DataLoaderWikiText to init data.
2017:03:03 17:06:50	reading and processing the text file.
2017:03:03 17:06:50	preprocess the dataset.
2017:03:03 17:07:21	use DataLoaderWikiText to init data.
2017:03:03 17:07:21	reading and processing the text file.
2017:03:03 17:07:21	preprocess the dataset.
2017:03:03 17:08:02	use DataLoaderWikiText to init data.
2017:03:03 17:08:02	reading and processing the text file.
2017:03:03 17:08:02	preprocess the dataset.
2017:03:03 17:08:12	use DataLoaderWikiText to init data.
2017:03:03 17:08:12	reading and processing the text file.
2017:03:03 17:08:12	preprocess the dataset.
2017:03:03 17:09:02	use DataLoaderWikiText to init data.
2017:03:03 17:09:02	reading and processing the text file.
2017:03:03 17:09:02	preprocess the dataset.
2017:03:03 17:09:38	use DataLoaderWikiText to init data.
2017:03:03 17:09:38	reading and processing the text file.
2017:03:03 17:09:38	preprocess the dataset.
2017:03:03 17:10:05	use DataLoaderWikiText to init data.
2017:03:03 17:10:05	reading and processing the text file.
2017:03:03 17:10:05	preprocess the dataset.
2017:03:03 17:11:04	use DataLoaderWikiText to init data.
2017:03:03 17:11:04	reading and processing the text file.
2017:03:03 17:11:04	preprocess the dataset.
2017:03:03 17:11:20	use DataLoaderWikiText to init data.
2017:03:03 17:11:20	reading and processing the text file.
2017:03:03 17:11:20	preprocess the dataset.
2017:03:03 17:12:25	use DataLoaderWikiText to init data.
2017:03:03 17:12:25	reading and processing the text file.
2017:03:03 17:12:25	preprocess the dataset.
2017:03:03 17:12:56	use DataLoaderWikiText to init data.
2017:03:03 17:12:56	reading and processing the text file.
2017:03:03 17:12:56	preprocess the dataset.
2017:03:03 17:13:31	use DataLoaderWikiText to init data.
2017:03:03 17:13:31	reading and processing the text file.
2017:03:03 17:13:31	preprocess the dataset.
2017:03:03 17:14:54	use DataLoaderWikiText to init data.
2017:03:03 17:14:54	reading and processing the text file.
2017:03:03 17:14:54	preprocess the dataset.
2017:03:03 17:14:55	build a vocabulary.
2017:03:03 17:14:55	...flatmap a list of sentence list to a list of sentence.
2017:03:03 17:14:56	...mapping from index to word.
2017:03:03 17:14:56	...mapping from word to index.
2017:03:03 17:14:56	...map word to index.
2017:03:03 17:16:45	use DataLoaderWikiText to init data.
2017:03:03 17:16:45	loading preprocessed files.
2017:03:03 17:17:24	use DataLoaderWikiText to init data.
2017:03:03 17:17:24	loading preprocessed files.
2017:03:03 17:18:52	use DataLoaderWikiText to init data.
2017:03:03 17:18:52	reading and processing the text file.
2017:03:03 17:18:52	preprocess the dataset.
2017:03:03 17:18:53	build a vocabulary.
2017:03:03 17:18:53	...flatmap a list of sentence list to a list of sentence.
2017:03:03 17:18:54	...mapping from index to word.
2017:03:03 17:18:54	...mapping from word to index.
2017:03:03 17:18:54	...map word to index.
2017:03:03 17:19:34	the vocabulary size is 34093.
2017:03:03 21:13:39	use DataLoaderChildrenStory to init data.
2017:03:03 21:13:39	reading and processing the text file.
2017:03:03 21:13:39	preprocess the dataset.
2017:03:03 21:14:03	use DataLoaderChildrenStory to init data.
2017:03:03 21:14:03	reading and processing the text file.
2017:03:03 21:14:03	preprocess the dataset.
2017:03:03 21:14:08	build a vocabulary.
2017:03:03 21:14:08	...flatmap a list of sentence list to a list of sentence.
2017:03:03 21:14:09	...mapping from index to word.
2017:03:03 21:14:09	...mapping from word to index.
2017:03:03 21:14:09	...map word to index.
2017:03:03 21:15:25	use DataLoaderChildrenStory to init data.
2017:03:03 21:15:25	loading preprocessed files.
2017:03:03 21:16:37	use DataLoaderChildrenStory to init data.
2017:03:03 21:16:37	loading preprocessed files.
2017:03:03 21:17:24	use DataLoaderChildrenStory to init data.
2017:03:03 21:17:24	reading and processing the text file.
2017:03:03 21:17:24	preprocess the dataset.
2017:03:03 21:17:29	build a vocabulary.
2017:03:03 21:17:29	...flatmap a list of sentence list to a list of sentence.
2017:03:03 21:17:30	...mapping from index to word.
2017:03:03 21:17:30	...mapping from word to index.
2017:03:03 21:17:30	...map word to index.
2017:03:03 21:18:28	the vocabulary size is 29265.
2017:03:03 21:19:27	use DataLoaderChildrenStory to init data.
2017:03:03 21:19:27	loading preprocessed files.
2017:03:03 21:20:15	the vocabulary size is 29265.
2017:03:07 14:51:41	use DataLoaderChildrenStory to init data.
2017:03:07 14:51:41	reading and processing the text file.
2017:03:07 14:51:41	preprocess the dataset.
2017:03:07 14:51:46	build a vocabulary.
2017:03:07 14:51:46	...flatmap a list of sentence list to a list of sentence.
2017:03:07 14:53:08	use DataLoaderChildrenStory to init data.
2017:03:07 14:53:08	reading and processing the text file.
2017:03:07 14:53:08	preprocess the dataset.
2017:03:07 14:53:13	build a vocabulary.
2017:03:07 14:53:13	...flatmap a list of sentence list to a list of sentence.
2017:03:07 14:53:20	use DataLoaderChildrenStory to init data.
2017:03:07 14:53:20	reading and processing the text file.
2017:03:07 14:53:20	preprocess the dataset.
2017:03:07 14:53:24	build a vocabulary.
2017:03:07 14:53:24	...flatmap a list of sentence list to a list of sentence.
2017:03:07 14:53:29	use DataLoaderChildrenStory to init data.
2017:03:07 14:53:29	reading and processing the text file.
2017:03:07 14:53:29	preprocess the dataset.
2017:03:07 14:53:33	build a vocabulary.
2017:03:07 14:53:33	...flatmap a list of sentence list to a list of sentence.
2017:03:07 14:54:06	use DataLoaderChildrenStory to init data.
2017:03:07 14:54:06	reading and processing the text file.
2017:03:07 14:54:06	preprocess the dataset.
2017:03:07 14:54:10	build a vocabulary.
2017:03:07 14:54:10	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:01:50	use DataLoaderChildrenStory to init data.
2017:03:07 15:01:50	reading and processing the text file.
2017:03:07 15:01:50	preprocess the dataset.
2017:03:07 15:17:53	use DataLoaderChildrenStory to init data.
2017:03:07 15:17:53	reading and processing the text file.
2017:03:07 15:17:53	preprocess the dataset.
2017:03:07 15:17:59	build a vocabulary.
2017:03:07 15:17:59	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:18:00	...mapping from index to word.
2017:03:07 15:18:00	...mapping from word to index.
2017:03:07 15:18:00	...map word to index.
2017:03:07 15:19:37	use DataLoaderChildrenStory to init data.
2017:03:07 15:19:37	reading and processing the text file.
2017:03:07 15:19:37	preprocess the dataset.
2017:03:07 15:19:42	build a vocabulary.
2017:03:07 15:19:42	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:19:42	...mapping from index to word.
2017:03:07 15:19:42	...mapping from word to index.
2017:03:07 15:19:42	...map word to index.
2017:03:07 15:22:54	use DataLoaderChildrenStory to init data.
2017:03:07 15:22:54	reading and processing the text file.
2017:03:07 15:22:54	preprocess the dataset.
2017:03:07 15:22:58	build a vocabulary.
2017:03:07 15:22:58	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:22:59	...mapping from index to word.
2017:03:07 15:22:59	...mapping from word to index.
2017:03:07 15:22:59	...map word to index.
2017:03:07 15:23:33	use DataLoaderChildrenStory to init data.
2017:03:07 15:23:33	reading and processing the text file.
2017:03:07 15:23:33	preprocess the dataset.
2017:03:07 15:23:38	build a vocabulary.
2017:03:07 15:23:38	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:23:38	...mapping from index to word.
2017:03:07 15:23:38	...mapping from word to index.
2017:03:07 15:23:38	...map word to index.
2017:03:07 15:23:52	use DataLoaderChildrenStory to init data.
2017:03:07 15:23:52	reading and processing the text file.
2017:03:07 15:23:52	preprocess the dataset.
2017:03:07 15:23:56	build a vocabulary.
2017:03:07 15:23:56	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:23:56	...mapping from index to word.
2017:03:07 15:23:56	...mapping from word to index.
2017:03:07 15:23:56	...map word to index.
2017:03:07 15:24:21	use DataLoaderChildrenStory to init data.
2017:03:07 15:24:21	reading and processing the text file.
2017:03:07 15:24:21	preprocess the dataset.
2017:03:07 15:24:25	build a vocabulary.
2017:03:07 15:24:25	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:24:25	...mapping from index to word.
2017:03:07 15:24:25	...mapping from word to index.
2017:03:07 15:24:25	...map word to index.
2017:03:07 15:25:28	use DataLoaderChildrenStory to init data.
2017:03:07 15:25:28	reading and processing the text file.
2017:03:07 15:25:28	preprocess the dataset.
2017:03:07 15:25:32	build a vocabulary.
2017:03:07 15:25:32	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:25:32	...mapping from index to word.
2017:03:07 15:25:32	...mapping from word to index.
2017:03:07 15:25:32	...map word to index.
2017:03:07 15:27:23	use DataLoaderChildrenStory to init data.
2017:03:07 15:27:23	reading and processing the text file.
2017:03:07 15:27:23	preprocess the dataset.
2017:03:07 15:27:27	build a vocabulary.
2017:03:07 15:27:27	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:28:29	use DataLoaderChildrenStory to init data.
2017:03:07 15:28:29	reading and processing the text file.
2017:03:07 15:28:29	preprocess the dataset.
2017:03:07 15:28:32	build a vocabulary.
2017:03:07 15:28:32	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:28:32	...mapping from index to word.
2017:03:07 15:28:32	...mapping from word to index.
2017:03:07 15:28:32	...map word to index.
2017:03:07 15:28:49	use DataLoaderChildrenStory to init data.
2017:03:07 15:28:49	reading and processing the text file.
2017:03:07 15:28:49	preprocess the dataset.
2017:03:07 15:28:53	build a vocabulary.
2017:03:07 15:28:53	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:28:53	...mapping from index to word.
2017:03:07 15:28:53	...mapping from word to index.
2017:03:07 15:28:53	...map word to index.
2017:03:07 15:30:54	use DataLoaderChildrenStory to init data.
2017:03:07 15:30:54	reading and processing the text file.
2017:03:07 15:30:54	preprocess the dataset.
2017:03:07 15:30:58	build a vocabulary.
2017:03:07 15:30:58	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:30:58	...mapping from index to word.
2017:03:07 15:30:58	...mapping from word to index.
2017:03:07 15:30:58	...map word to index.
2017:03:07 15:33:58	use DataLoaderChildrenStory to init data.
2017:03:07 15:33:58	reading and processing the text file.
2017:03:07 15:33:58	preprocess the dataset.
2017:03:07 15:34:03	build a vocabulary.
2017:03:07 15:34:03	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:34:04	...mapping from index to word.
2017:03:07 15:34:04	...mapping from word to index.
2017:03:07 15:34:04	...map word to index.
2017:03:07 15:34:32	use DataLoaderChildrenStory to init data.
2017:03:07 15:34:32	reading and processing the text file.
2017:03:07 15:34:32	preprocess the dataset.
2017:03:07 15:34:37	build a vocabulary.
2017:03:07 15:34:37	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:34:37	...mapping from index to word.
2017:03:07 15:34:37	...mapping from word to index.
2017:03:07 15:34:37	...map word to index.
2017:03:07 15:36:09	use DataLoaderChildrenStory to init data.
2017:03:07 15:36:09	reading and processing the text file.
2017:03:07 15:36:09	preprocess the dataset.
2017:03:07 15:36:16	build a vocabulary.
2017:03:07 15:36:16	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:36:16	...mapping from index to word.
2017:03:07 15:36:16	...mapping from word to index.
2017:03:07 15:36:16	...map word to index.
2017:03:07 15:37:14	use DataLoaderChildrenStory to init data.
2017:03:07 15:37:14	reading and processing the text file.
2017:03:07 15:37:14	preprocess the dataset.
2017:03:07 15:37:20	build a vocabulary.
2017:03:07 15:37:20	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:37:21	...mapping from index to word.
2017:03:07 15:37:21	...mapping from word to index.
2017:03:07 15:37:21	...map word to index.
2017:03:07 15:37:33	use DataLoaderChildrenStory to init data.
2017:03:07 15:37:33	reading and processing the text file.
2017:03:07 15:37:33	preprocess the dataset.
2017:03:07 15:37:39	build a vocabulary.
2017:03:07 15:37:39	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:37:39	...mapping from index to word.
2017:03:07 15:37:39	...mapping from word to index.
2017:03:07 15:37:39	...map word to index.
2017:03:07 15:37:55	use DataLoaderChildrenStory to init data.
2017:03:07 15:37:55	reading and processing the text file.
2017:03:07 15:37:55	preprocess the dataset.
2017:03:07 15:38:01	build a vocabulary.
2017:03:07 15:38:01	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:38:01	...mapping from index to word.
2017:03:07 15:38:01	...mapping from word to index.
2017:03:07 15:38:01	...map word to index.
2017:03:07 15:43:08	use DataLoaderChildrenStory to init data.
2017:03:07 15:43:08	reading and processing the text file.
2017:03:07 15:43:08	preprocess the dataset.
2017:03:07 15:43:14	...mask and pad the sentence.
2017:03:07 15:43:14	build a vocabulary.
2017:03:07 15:43:14	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:43:14	...mapping from index to word.
2017:03:07 15:43:14	...mapping from word to index.
2017:03:07 15:43:14	...map word to index.
2017:03:07 15:44:44	use DataLoaderChildrenStory to init data.
2017:03:07 15:44:44	reading and processing the text file.
2017:03:07 15:44:44	preprocess the dataset.
2017:03:07 15:44:50	...mask and pad the sentence.
2017:03:07 15:44:50	build a vocabulary.
2017:03:07 15:44:50	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:44:50	...mapping from index to word.
2017:03:07 15:44:50	...mapping from word to index.
2017:03:07 15:44:50	...map word to index.
2017:03:07 15:54:01	use DataLoaderChildrenStory to init data.
2017:03:07 15:54:01	reading and processing the text file.
2017:03:07 15:54:01	preprocess the dataset.
2017:03:07 15:54:12	...mask and pad the sentence.
2017:03:07 15:55:33	use DataLoaderChildrenStory to init data.
2017:03:07 15:55:33	reading and processing the text file.
2017:03:07 15:55:33	preprocess the dataset.
2017:03:07 15:55:38	...mask and pad the sentence.
2017:03:07 15:55:38	build a vocabulary.
2017:03:07 15:55:38	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:55:39	...mapping from index to word.
2017:03:07 15:55:39	...mapping from word to index.
2017:03:07 15:55:39	...map word to index.
2017:03:07 15:55:53	use DataLoaderChildrenStory to init data.
2017:03:07 15:55:53	reading and processing the text file.
2017:03:07 15:55:53	preprocess the dataset.
2017:03:07 15:55:58	...mask and pad the sentence.
2017:03:07 15:55:58	build a vocabulary.
2017:03:07 15:55:58	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:55:58	...mapping from index to word.
2017:03:07 15:55:58	...mapping from word to index.
2017:03:07 15:55:58	...map word to index.
2017:03:07 15:56:05	use DataLoaderChildrenStory to init data.
2017:03:07 15:56:05	reading and processing the text file.
2017:03:07 15:56:05	preprocess the dataset.
2017:03:07 15:56:12	...mask and pad the sentence.
2017:03:07 15:56:12	build a vocabulary.
2017:03:07 15:56:12	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:56:12	...mapping from index to word.
2017:03:07 15:56:12	...mapping from word to index.
2017:03:07 15:56:12	...map word to index.
2017:03:07 15:56:27	use DataLoaderChildrenStory to init data.
2017:03:07 15:56:27	reading and processing the text file.
2017:03:07 15:56:27	preprocess the dataset.
2017:03:07 15:56:32	...mask and pad the sentence.
2017:03:07 15:56:32	build a vocabulary.
2017:03:07 15:56:32	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:56:32	...mapping from index to word.
2017:03:07 15:56:32	...mapping from word to index.
2017:03:07 15:56:32	...map word to index.
2017:03:07 15:58:30	use DataLoaderChildrenStory to init data.
2017:03:07 15:58:31	reading and processing the text file.
2017:03:07 15:58:31	preprocess the dataset.
2017:03:07 15:58:37	...mask and pad the sentence.
2017:03:07 15:58:56	use DataLoaderChildrenStory to init data.
2017:03:07 15:58:56	reading and processing the text file.
2017:03:07 15:58:56	preprocess the dataset.
2017:03:07 15:59:00	...mask and pad the sentence.
2017:03:07 15:59:00	......max len:19, median len:9.0, min len:2
2017:03:07 15:59:00	build a vocabulary.
2017:03:07 15:59:00	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:59:00	...mapping from index to word.
2017:03:07 15:59:00	...mapping from word to index.
2017:03:07 15:59:00	...map word to index.
2017:03:07 15:59:18	use DataLoaderChildrenStory to init data.
2017:03:07 15:59:18	reading and processing the text file.
2017:03:07 15:59:18	preprocess the dataset.
2017:03:07 15:59:24	...mask and pad the sentence.
2017:03:07 15:59:24	......max len:19, median len:9.0, min len:2
2017:03:07 15:59:24	build a vocabulary.
2017:03:07 15:59:24	...flatmap a list of sentence list to a list of sentence.
2017:03:07 15:59:24	...mapping from index to word.
2017:03:07 15:59:24	...mapping from word to index.
2017:03:07 15:59:24	...map word to index.
2017:03:07 16:00:30	use DataLoaderChildrenStory to init data.
2017:03:07 16:00:30	reading and processing the text file.
2017:03:07 16:00:30	preprocess the dataset.
2017:03:07 16:00:34	...mask and pad the sentence.
2017:03:07 16:00:35	......max len:19, median len:9.0, min len:2
2017:03:07 16:00:35	build a vocabulary.
2017:03:07 16:00:35	...flatmap a list of sentence list to a list of sentence.
2017:03:07 16:00:35	...mapping from index to word.
2017:03:07 16:00:35	...mapping from word to index.
2017:03:07 16:00:35	...map word to index.
2017:03:07 16:06:24	use DataLoaderChildrenStory to init data.
2017:03:07 16:06:24	reading and processing the text file.
2017:03:07 16:06:24	preprocess the dataset.
2017:03:07 16:06:30	...mask and pad the sentence.
2017:03:07 16:06:30	......max len:18, median len:8.0, min len:1
2017:03:07 16:06:31	build a vocabulary.
2017:03:07 16:06:31	...flatmap a list of sentence list to a list of sentence.
2017:03:07 16:06:31	...mapping from index to word.
2017:03:07 16:06:31	...mapping from word to index.
2017:03:07 16:06:31	...map word to index.
2017:03:07 16:07:35	use DataLoaderChildrenStory to init data.
2017:03:07 16:07:35	reading and processing the text file.
2017:03:07 16:07:35	preprocess the dataset.
2017:03:07 16:07:42	...mask and pad the sentence.
2017:03:07 16:07:42	......max len:18, median len:8.0, min len:1
2017:03:07 16:07:42	build a vocabulary.
2017:03:07 16:07:42	...flatmap a list of sentence list to a list of sentence.
2017:03:07 16:07:42	...mapping from index to word.
2017:03:07 16:07:43	...mapping from word to index.
2017:03:07 16:07:43	...map word to index.
2017:03:07 16:14:26	use DataLoaderChildrenStory to init data.
2017:03:07 16:14:26	reading and processing the text file.
2017:03:07 16:14:26	preprocess the dataset.
2017:03:07 16:14:37	...mask and pad the sentence.
2017:03:07 16:14:37	......max len:18, median len:8.0, min len:1
2017:03:07 16:14:37	build a vocabulary.
2017:03:07 16:14:37	...flatmap a list of sentence list to a list of sentence.
2017:03:07 16:14:38	...mapping from index to word.
2017:03:07 16:14:38	...mapping from word to index.
2017:03:07 16:14:38	...map word to index.
2017:03:07 16:17:17	use DataLoaderChildrenStory to init data.
2017:03:07 16:17:17	reading and processing the text file.
2017:03:07 16:17:17	preprocess the dataset.
2017:03:07 16:17:26	...mask and pad the sentence.
2017:03:07 16:17:26	......max len:18, median len:8.0, min len:1
2017:03:07 16:17:27	build a vocabulary.
2017:03:07 16:17:27	...flatmap a list of sentence list to a list of sentence.
2017:03:07 16:17:27	...mapping from index to word.
2017:03:07 16:17:28	...mapping from word to index.
2017:03:07 16:17:28	...map word to index.
2017:03:07 16:25:58	use DataLoaderChildrenStory to init data.
2017:03:07 16:25:58	reading and processing the text file.
2017:03:07 16:25:58	preprocess the dataset.
2017:03:07 16:26:06	...mask and pad the sentence.
2017:03:07 16:26:06	......max len:18, median len:8.0, min len:1
2017:03:07 16:26:06	build a vocabulary.
2017:03:07 16:26:06	...flatmap a list of sentence list to a list of sentence.
2017:03:07 16:26:07	...mapping from index to word.
2017:03:07 16:26:07	...mapping from word to index.
2017:03:07 16:26:07	...map word to index.
2017:03:07 16:26:08	...some data statistics.
2017:03:07 16:26:08	...save processed data to file.
2017:03:07 16:26:10	the sample size is 78290, the vocab size is 17612
2017:03:07 20:34:49	use DataLoaderChildrenStory to init data.
2017:03:07 20:34:49	reading and processing the text file.
2017:03:07 20:34:49	preprocess the dataset.
2017:03:07 20:34:59	...mask and pad the sentence.
2017:03:07 20:34:59	......max len:19, median len:9.0, min len:2
2017:03:07 20:35:00	build a vocabulary.
2017:03:07 20:35:00	...flatmap a list of sentence list to a list of sentence.
2017:03:07 20:35:01	...mapping from index to word.
2017:03:07 20:35:01	...mapping from word to index.
2017:03:07 20:35:01	...map word to index.
2017:03:07 20:35:02	...some data statistics.
2017:03:07 20:35:02	...save processed data to file.
2017:03:07 20:35:05	the sample size is 95182, the vocab size is 18626
2017:03:07 20:36:30	use DataLoaderChildrenStory to init data.
2017:03:07 20:36:30	reading and processing the text file.
2017:03:07 20:36:30	preprocess the dataset.
2017:03:07 20:36:34	...mask and pad the sentence.
2017:03:07 20:36:34	......max len:19, median len:8.0, min len:2
2017:03:07 20:36:35	build a vocabulary.
2017:03:07 20:36:35	...flatmap a list of sentence list to a list of sentence.
2017:03:07 20:36:35	...mapping from index to word.
2017:03:07 20:36:36	...mapping from word to index.
2017:03:07 20:36:36	...map word to index.
2017:03:07 20:36:37	...some data statistics.
2017:03:07 20:36:37	...save processed data to file.
2017:03:07 20:36:40	the sample size is 95080, the vocab size is 22861
2017:03:07 20:37:14	use DataLoaderChildrenStory to init data.
2017:03:07 20:37:14	reading and processing the text file.
2017:03:07 20:37:14	preprocess the dataset.
2017:03:07 20:37:18	...mask and pad the sentence.
2017:03:07 20:37:18	......max len:19, median len:8.0, min len:2
2017:03:07 20:37:19	build a vocabulary.
2017:03:07 20:37:19	...flatmap a list of sentence list to a list of sentence.
2017:03:07 20:37:20	...mapping from index to word.
2017:03:07 20:37:20	...mapping from word to index.
2017:03:07 20:37:20	...map word to index.
2017:03:07 20:37:20	...some data statistics.
2017:03:07 20:37:20	...save processed data to file.
2017:03:07 20:37:24	the sample size is 95080, the vocab size is 22861
2017:03:07 20:37:27	use DataLoaderChildrenStory to init data.
2017:03:07 20:37:27	reading and processing the text file.
2017:03:07 20:37:27	preprocess the dataset.
2017:03:07 20:37:37	...mask and pad the sentence.
2017:03:07 20:37:37	......max len:19, median len:9.0, min len:2
2017:03:07 20:37:38	build a vocabulary.
2017:03:07 20:37:38	...flatmap a list of sentence list to a list of sentence.
2017:03:07 20:37:39	...mapping from index to word.
2017:03:07 20:37:39	...mapping from word to index.
2017:03:07 20:37:39	...map word to index.
2017:03:07 20:37:40	...some data statistics.
2017:03:07 20:37:40	...save processed data to file.
2017:03:07 20:37:43	the sample size is 95182, the vocab size is 18626
2017:03:07 20:38:48	use DataLoaderChildrenStory to init data.
2017:03:07 20:38:48	reading and processing the text file.
2017:03:07 20:38:48	preprocess the dataset.
2017:03:07 20:38:58	...mask and pad the sentence.
2017:03:07 20:38:58	......max len:19, median len:9.0, min len:2
2017:03:07 20:38:58	build a vocabulary.
2017:03:07 20:38:58	...flatmap a list of sentence list to a list of sentence.
2017:03:07 20:38:59	...mapping from index to word.
2017:03:07 20:38:59	...mapping from word to index.
2017:03:07 20:38:59	...map word to index.
2017:03:07 20:39:00	...some data statistics.
2017:03:07 20:39:00	...save processed data to file.
2017:03:07 20:39:03	the sample size is 95182, the vocab size is 18626
2017:03:07 20:40:34	use DataLoaderChildrenStory to init data.
2017:03:07 20:40:34	reading and processing the text file.
2017:03:07 20:40:34	preprocess the dataset.
2017:03:07 20:40:44	...mask and pad the sentence.
2017:03:07 20:40:44	......max len:19, median len:9.0, min len:2
2017:03:07 20:40:44	build a vocabulary.
2017:03:07 20:40:44	...flatmap a list of sentence list to a list of sentence.
2017:03:07 20:40:45	...mapping from index to word.
2017:03:07 20:40:45	...mapping from word to index.
2017:03:07 20:40:45	...map word to index.
2017:03:07 20:40:46	...some data statistics.
2017:03:07 20:40:46	...save processed data to file.
2017:03:07 20:40:49	the sample size is 95182, the vocab size is 18626
2017:03:07 20:42:08	use DataLoaderChildrenStory to init data.
2017:03:07 20:42:08	reading and processing the text file.
2017:03:07 20:42:08	preprocess the dataset.
2017:03:07 20:42:17	...mask and pad the sentence.
2017:03:07 20:42:17	......max len:19, median len:9.0, min len:2
2017:03:07 20:42:18	build a vocabulary.
2017:03:07 20:42:18	...flatmap a list of sentence list to a list of sentence.
2017:03:07 20:42:19	...mapping from index to word.
2017:03:07 20:42:19	...mapping from word to index.
2017:03:07 20:42:19	...map word to index.
2017:03:07 20:42:19	...some data statistics.
2017:03:07 20:42:19	...save processed data to file.
2017:03:07 20:42:22	the sample size is 95182, the vocab size is 18626
2017:03:07 21:52:40	use DataLoaderChildrenStory to init data.
2017:03:07 21:52:40	reading and processing the text file.
2017:03:07 21:52:40	preprocess the dataset.
2017:03:07 21:52:48	...mask and pad the sentence.
2017:03:07 21:52:48	......max len:19, median len:9.0, min len:2
2017:03:07 21:52:48	build a vocabulary.
2017:03:07 21:52:48	...flatmap a list of sentence list to a list of sentence.
2017:03:07 21:52:49	...mapping from index to word.
2017:03:07 21:52:49	...mapping from word to index.
2017:03:07 21:52:49	...map word to index.
2017:03:07 21:52:50	...some data statistics.
2017:03:07 21:52:50	...save processed data to file.
2017:03:07 21:52:53	the sample size is 95182, the vocab size is 18627
2017:03:07 21:56:38	use DataLoaderChildrenStory to init data.
2017:03:07 21:56:38	loading preprocessed files.
2017:03:07 21:56:43	the sample size is 95182, the vocab size is 18627
2017:03:07 21:57:30	use DataLoaderChildrenStory to init data.
2017:03:07 21:57:30	loading preprocessed files.
2017:03:07 21:57:36	the sample size is 95182, the vocab size is 18627
2017:03:07 21:58:33	use DataLoaderChildrenStory to init data.
2017:03:07 21:58:33	loading preprocessed files.
2017:03:07 21:58:38	the sample size is 95182, the vocab size is 18627
2017:03:07 22:08:37	use DataLoaderChildrenStory to init data.
2017:03:07 22:08:37	loading preprocessed files.
2017:03:07 22:08:42	the sample size is 95182, the vocab size is 18627
2017:03:07 22:19:08	use DataLoaderChildrenStory to init data.
2017:03:07 22:19:08	loading preprocessed files.
2017:03:07 22:19:13	the sample size is 95182, the vocab size is 18627
2017:03:07 22:21:11	use DataLoaderChildrenStory to init data.
2017:03:07 22:21:11	loading preprocessed files.
2017:03:07 22:21:16	the sample size is 95182, the vocab size is 18627
2017:03:07 22:25:46	use DataLoaderChildrenStory to init data.
2017:03:07 22:25:46	loading preprocessed files.
2017:03:07 22:25:50	the sample size is 95182, the vocab size is 18627
2017:03:07 22:27:14	use DataLoaderChildrenStory to init data.
2017:03:07 22:27:14	loading preprocessed files.
2017:03:07 22:27:18	the sample size is 95182, the vocab size is 18627
2017:03:07 22:27:40	use DataLoaderChildrenStory to init data.
2017:03:07 22:27:40	loading preprocessed files.
2017:03:07 22:27:44	the sample size is 95182, the vocab size is 18627
2017:03:07 23:24:29	use DataLoaderChildrenStory to init data.
2017:03:07 23:24:29	loading preprocessed files.
2017:03:07 23:24:33	the sample size is 95182, the vocab size is 18627
2017:03:07 23:26:00	use DataLoaderChildrenStory to init data.
2017:03:07 23:26:00	loading preprocessed files.
2017:03:07 23:26:04	the sample size is 95182, the vocab size is 18627
2017:03:07 23:49:16	use DataLoaderChildrenStory to init data.
2017:03:07 23:49:16	loading preprocessed files.
2017:03:08 23:42:48	use DataLoaderChildrenStory to init data.
2017:03:08 23:42:48	loading preprocessed files.
2017:03:08 23:42:52	the sample size is 95182, the vocab size is 18627
2017:03:08 23:44:30	use DataLoaderChildrenStory to init data.
2017:03:08 23:44:30	reading and processing the text file.
2017:03:08 23:44:30	preprocess the dataset.
2017:03:08 23:44:35	...mask and pad the sentence.
2017:03:08 23:44:35	......max len:19, median len:9.0, min len:2
2017:03:08 23:44:35	build a vocabulary.
2017:03:08 23:44:35	...flatmap a list of sentence list to a list of sentence.
2017:03:08 23:44:36	...mapping from index to word.
2017:03:08 23:44:36	...mapping from word to index.
2017:03:08 23:44:36	...map word to index.
2017:03:08 23:44:37	...some data statistics.
2017:03:08 23:44:37	...save processed data to file.
2017:03:08 23:44:40	the sample size is 95182, the vocab size is 18628
2017:03:08 23:45:20	use DataLoaderChildrenStory to init data.
2017:03:08 23:45:20	loading preprocessed files.
2017:03:08 23:45:27	the sample size is 95182, the vocab size is 18628
2017:03:08 23:46:58	use DataLoaderChildrenStory to init data.
2017:03:08 23:46:58	loading preprocessed files.
2017:03:08 23:47:04	the sample size is 95182, the vocab size is 18628
2017:03:08 23:47:16	use DataLoaderChildrenStory to init data.
2017:03:08 23:47:16	loading preprocessed files.
2017:03:08 23:47:23	the sample size is 95182, the vocab size is 18628
2017:03:08 23:48:11	use DataLoaderChildrenStory to init data.
2017:03:08 23:48:11	loading preprocessed files.
2017:03:08 23:48:18	the sample size is 95182, the vocab size is 18628
2017:03:08 23:48:45	use DataLoaderChildrenStory to init data.
2017:03:08 23:48:45	loading preprocessed files.
2017:03:08 23:48:51	the sample size is 95182, the vocab size is 18628
2017:03:08 23:49:08	use DataLoaderChildrenStory to init data.
2017:03:08 23:49:08	loading preprocessed files.
2017:03:08 23:49:14	the sample size is 95182, the vocab size is 18628
2017:03:08 23:49:36	use DataLoaderChildrenStory to init data.
2017:03:08 23:49:36	loading preprocessed files.
2017:03:08 23:49:43	the sample size is 95182, the vocab size is 18628
2017:03:08 23:49:58	use DataLoaderChildrenStory to init data.
2017:03:08 23:49:58	loading preprocessed files.
2017:03:08 23:50:05	the sample size is 95182, the vocab size is 18628
2017:03:08 23:52:24	use DataLoaderChildrenStory to init data.
2017:03:08 23:52:24	loading preprocessed files.
2017:03:08 23:52:31	the sample size is 95182, the vocab size is 18628
2017:03:08 23:53:22	use DataLoaderChildrenStory to init data.
2017:03:08 23:53:22	loading preprocessed files.
2017:03:08 23:53:28	the sample size is 95182, the vocab size is 18628
2017:03:08 23:55:43	use DataLoaderChildrenStory to init data.
2017:03:08 23:55:43	loading preprocessed files.
2017:03:08 23:55:49	the sample size is 95182, the vocab size is 18628
2017:03:08 23:56:43	use DataLoaderChildrenStory to init data.
2017:03:08 23:56:43	loading preprocessed files.
2017:03:08 23:56:49	the sample size is 95182, the vocab size is 18628
2017:03:08 23:57:17	use DataLoaderChildrenStory to init data.
2017:03:08 23:57:17	loading preprocessed files.
2017:03:08 23:57:24	the sample size is 95182, the vocab size is 18628
2017:03:08 23:58:38	use DataLoaderChildrenStory to init data.
2017:03:08 23:58:38	loading preprocessed files.
2017:03:08 23:58:45	the sample size is 95182, the vocab size is 18628
2017:03:09 00:00:36	use DataLoaderChildrenStory to init data.
2017:03:09 00:00:36	loading preprocessed files.
2017:03:09 00:00:42	the sample size is 95182, the vocab size is 18628
2017:03:09 00:01:10	use DataLoaderChildrenStory to init data.
2017:03:09 00:01:10	loading preprocessed files.
2017:03:09 00:01:17	the sample size is 95182, the vocab size is 18628
2017:03:09 00:02:37	use DataLoaderChildrenStory to init data.
2017:03:09 00:02:37	loading preprocessed files.
2017:03:09 00:02:44	the sample size is 95182, the vocab size is 18628
2017:03:09 00:03:02	use DataLoaderChildrenStory to init data.
2017:03:09 00:03:02	loading preprocessed files.
2017:03:09 00:03:09	the sample size is 95182, the vocab size is 18628
2017:03:09 00:06:41	use DataLoaderChildrenStory to init data.
2017:03:09 00:06:41	loading preprocessed files.
2017:03:09 00:06:48	the sample size is 95182, the vocab size is 18628
2017:03:09 00:07:02	use DataLoaderChildrenStory to init data.
2017:03:09 00:07:02	loading preprocessed files.
2017:03:09 00:07:09	the sample size is 95182, the vocab size is 18628
2017:03:09 00:07:44	use DataLoaderChildrenStory to init data.
2017:03:09 00:07:44	loading preprocessed files.
2017:03:09 00:07:50	the sample size is 95182, the vocab size is 18628
2017:03:09 00:08:29	use DataLoaderChildrenStory to init data.
2017:03:09 00:08:29	loading preprocessed files.
2017:03:09 00:08:36	the sample size is 95182, the vocab size is 18628
2017:03:09 00:09:51	use DataLoaderChildrenStory to init data.
2017:03:09 00:09:51	loading preprocessed files.
2017:03:09 00:09:57	the sample size is 95182, the vocab size is 18628
2017:03:09 00:10:31	use DataLoaderChildrenStory to init data.
2017:03:09 00:10:31	loading preprocessed files.
2017:03:09 00:10:38	the sample size is 95182, the vocab size is 18628
2017:03:09 00:11:37	use DataLoaderChildrenStory to init data.
2017:03:09 00:11:37	loading preprocessed files.
2017:03:09 00:11:43	the sample size is 95182, the vocab size is 18628
2017:03:09 00:13:27	use DataLoaderChildrenStory to init data.
2017:03:09 00:13:27	loading preprocessed files.
2017:03:09 00:13:34	the sample size is 95182, the vocab size is 18628
2017:03:09 00:14:10	use DataLoaderChildrenStory to init data.
2017:03:09 00:14:10	loading preprocessed files.
2017:03:09 00:14:16	the sample size is 95182, the vocab size is 18628
2017:03:09 00:19:02	use DataLoaderChildrenStory to init data.
2017:03:09 00:19:02	loading preprocessed files.
2017:03:09 00:19:09	the sample size is 95182, the vocab size is 18628
2017:03:09 00:20:20	use DataLoaderChildrenStory to init data.
2017:03:09 00:20:20	loading preprocessed files.
2017:03:09 00:20:27	the sample size is 95182, the vocab size is 18628
2017:03:09 00:21:09	use DataLoaderChildrenStory to init data.
2017:03:09 00:21:09	loading preprocessed files.
2017:03:09 00:21:15	the sample size is 95182, the vocab size is 18628
2017:03:09 00:23:13	use DataLoaderChildrenStory to init data.
2017:03:09 00:23:13	loading preprocessed files.
2017:03:09 00:23:19	the sample size is 95182, the vocab size is 18628
2017:03:09 00:23:46	use DataLoaderChildrenStory to init data.
2017:03:09 00:23:46	loading preprocessed files.
2017:03:09 00:23:53	the sample size is 95182, the vocab size is 18628
2017:03:09 00:25:09	use DataLoaderChildrenStory to init data.
2017:03:09 00:25:09	loading preprocessed files.
2017:03:09 00:25:16	the sample size is 95182, the vocab size is 18628
2017:03:09 00:25:55	use DataLoaderChildrenStory to init data.
2017:03:09 00:25:55	loading preprocessed files.
2017:03:09 00:26:01	the sample size is 95182, the vocab size is 18628
2017:03:09 00:27:10	use DataLoaderChildrenStory to init data.
2017:03:09 00:27:10	loading preprocessed files.
2017:03:09 00:27:16	the sample size is 95182, the vocab size is 18628
2017:03:09 00:28:00	use DataLoaderChildrenStory to init data.
2017:03:09 00:28:00	loading preprocessed files.
2017:03:09 00:28:06	the sample size is 95182, the vocab size is 18628
2017:03:09 15:36:56	use DataLoaderChildrenStory to init data.
2017:03:09 15:36:56	reading and processing the text file.
2017:03:09 15:36:56	preprocess the dataset.
2017:03:09 15:37:01	...mask and pad the sentence.
2017:03:09 15:37:01	......max len:19, median len:9.0, min len:2
2017:03:09 15:37:02	build a vocabulary.
2017:03:09 15:37:02	...flatmap a list of sentence list to a list of sentence.
2017:03:09 15:37:03	...mapping from index to word.
2017:03:09 15:37:03	...mapping from word to index.
2017:03:09 15:37:03	...map word to index.
2017:03:09 15:37:03	...some data statistics.
2017:03:09 15:37:03	...save processed data to file.
2017:03:09 15:37:07	the sample size is 95182, the vocab size is 18628
2017:03:09 15:39:15	use DataLoaderChildrenStory to init data.
2017:03:09 15:39:15	loading preprocessed files.
2017:03:09 15:39:23	the sample size is 95182, the vocab size is 18628
2017:03:09 15:40:58	use DataLoaderChildrenStory to init data.
2017:03:09 15:40:58	loading preprocessed files.
2017:03:09 15:41:05	the sample size is 95182, the vocab size is 18628
2017:03:09 16:23:00	use DataLoaderChildrenStory to init data.
2017:03:09 16:23:00	loading preprocessed files.
2017:03:09 16:23:07	the sample size is 95182, the vocab size is 18628
2017:03:09 16:23:10	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489076590

2017:03:09 16:24:52	use DataLoaderChildrenStory to init data.
2017:03:09 16:24:52	loading preprocessed files.
2017:03:09 16:24:59	the sample size is 95182, the vocab size is 18628
2017:03:09 16:25:03	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489076703

2017:03:09 16:25:33	use DataLoaderChildrenStory to init data.
2017:03:09 16:25:33	loading preprocessed files.
2017:03:09 16:25:40	the sample size is 95182, the vocab size is 18628
2017:03:09 16:25:43	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489076743

2017:03:09 16:26:13	use DataLoaderChildrenStory to init data.
2017:03:09 16:26:13	loading preprocessed files.
2017:03:09 16:26:20	the sample size is 95182, the vocab size is 18628
2017:03:09 16:26:23	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489076783

2017:03:09 17:02:45	use DataLoaderChildrenStory to init data.
2017:03:09 17:02:45	loading preprocessed files.
2017:03:09 17:02:54	the sample size is 95182, the vocab size is 18628
2017:03:09 17:02:57	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489078977

2017:03:09 17:03:25	use DataLoaderChildrenStory to init data.
2017:03:09 17:03:25	loading preprocessed files.
2017:03:09 17:03:32	the sample size is 95182, the vocab size is 18628
2017:03:09 17:03:35	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489079015

2017:03:09 17:04:17	use DataLoaderChildrenStory to init data.
2017:03:09 17:04:17	loading preprocessed files.
2017:03:09 17:04:24	the sample size is 95182, the vocab size is 18628
2017:03:09 17:04:28	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489079067

2017:03:09 18:32:49	use DataLoaderChildrenStory to init data.
2017:03:09 18:32:49	loading preprocessed files.
2017:03:09 18:32:56	the sample size is 95182, the vocab size is 18628
2017:03:09 18:32:59	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489084379

2017:03:09 19:43:35	use DataLoaderChildrenStory to init data.
2017:03:09 19:43:35	loading preprocessed files.
2017:03:09 19:43:42	the sample size is 95182, the vocab size is 18628
2017:03:09 19:43:46	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489088626

2017:03:09 19:53:00	use DataLoaderChildrenStory to init data.
2017:03:09 19:53:00	loading preprocessed files.
2017:03:09 19:53:10	the sample size is 95182, the vocab size is 18628
2017:03:09 19:53:53	use DataLoaderChildrenStory to init data.
2017:03:09 19:53:53	loading preprocessed files.
2017:03:09 19:53:59	the sample size is 95182, the vocab size is 18628
2017:03:09 19:55:09	use DataLoaderChildrenStory to init data.
2017:03:09 19:55:09	loading preprocessed files.
2017:03:09 19:55:17	the sample size is 95182, the vocab size is 18628
2017:03:09 19:55:26	use DataLoaderChildrenStory to init data.
2017:03:09 19:55:26	loading preprocessed files.
2017:03:09 19:55:33	the sample size is 95182, the vocab size is 18628
2017:03:09 19:55:36	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489089336

2017:03:09 19:56:35	use DataLoaderChildrenStory to init data.
2017:03:09 19:56:35	loading preprocessed files.
2017:03:09 19:56:41	the sample size is 95182, the vocab size is 18628
2017:03:09 19:56:44	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489089404

2017:03:09 19:58:36	use DataLoaderChildrenStory to init data.
2017:03:09 19:58:36	loading preprocessed files.
2017:03:09 19:58:44	the sample size is 95182, the vocab size is 18628
2017:03:09 19:58:47	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489089527

2017:03:09 19:59:32	use DataLoaderChildrenStory to init data.
2017:03:09 19:59:32	loading preprocessed files.
2017:03:09 19:59:39	the sample size is 95182, the vocab size is 18628
2017:03:09 19:59:42	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489089582

2017:03:09 20:00:30	use DataLoaderChildrenStory to init data.
2017:03:09 20:00:30	loading preprocessed files.
2017:03:09 20:00:37	the sample size is 95182, the vocab size is 18628
2017:03:09 20:00:40	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489089640

2017:03:09 20:02:00	use DataLoaderChildrenStory to init data.
2017:03:09 20:02:00	loading preprocessed files.
2017:03:09 20:02:07	the sample size is 95182, the vocab size is 18628
2017:03:09 20:02:36	use DataLoaderChildrenStory to init data.
2017:03:09 20:02:36	loading preprocessed files.
2017:03:09 20:02:43	the sample size is 95182, the vocab size is 18628
2017:03:09 20:02:47	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489089767

2017:03:09 20:03:45	use DataLoaderChildrenStory to init data.
2017:03:09 20:03:45	loading preprocessed files.
2017:03:09 20:03:54	the sample size is 95182, the vocab size is 18628
2017:03:09 20:03:57	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489089837

2017:03:09 20:04:38	use DataLoaderChildrenStory to init data.
2017:03:09 20:04:38	loading preprocessed files.
2017:03:09 20:04:44	the sample size is 95182, the vocab size is 18628
2017:03:09 20:05:17	use DataLoaderChildrenStory to init data.
2017:03:09 20:05:17	loading preprocessed files.
2017:03:09 20:05:24	the sample size is 95182, the vocab size is 18628
2017:03:09 20:06:13	use DataLoaderChildrenStory to init data.
2017:03:09 20:06:13	loading preprocessed files.
2017:03:09 20:06:20	the sample size is 95182, the vocab size is 18628
2017:03:09 20:06:43	use DataLoaderChildrenStory to init data.
2017:03:09 20:06:43	loading preprocessed files.
2017:03:09 20:06:49	the sample size is 95182, the vocab size is 18628
2017:03:09 20:08:02	use DataLoaderChildrenStory to init data.
2017:03:09 20:08:02	loading preprocessed files.
2017:03:09 20:08:09	the sample size is 95182, the vocab size is 18628
2017:03:09 20:08:17	use DataLoaderChildrenStory to init data.
2017:03:09 20:08:17	loading preprocessed files.
2017:03:09 20:08:23	the sample size is 95182, the vocab size is 18628
2017:03:09 20:08:26	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489090106

2017:03:09 20:11:05	use DataLoaderChildrenStory to init data.
2017:03:09 20:11:05	loading preprocessed files.
2017:03:09 20:11:13	the sample size is 95182, the vocab size is 18628
2017:03:09 20:11:46	use DataLoaderChildrenStory to init data.
2017:03:09 20:11:46	loading preprocessed files.
2017:03:09 20:11:53	the sample size is 95182, the vocab size is 18628
2017:03:09 20:11:56	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489090316

2017:03:09 20:13:00	use DataLoaderChildrenStory to init data.
2017:03:09 20:13:00	loading preprocessed files.
2017:03:09 20:13:08	the sample size is 95182, the vocab size is 18628
2017:03:09 20:13:12	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489090391

2017:03:09 20:21:58	use DataLoaderChildrenStory to init data.
2017:03:09 20:21:58	reading and processing the text file.
2017:03:09 20:21:58	preprocess the dataset.
2017:03:09 20:22:03	...mask and pad the sentence.
2017:03:09 20:22:03	......max len:20, median len:10.0, min len:3
2017:03:09 20:22:04	build a vocabulary.
2017:03:09 20:22:04	...flatmap a list of sentence list to a list of sentence.
2017:03:09 20:22:05	...mapping from index to word.
2017:03:09 20:22:05	...mapping from word to index.
2017:03:09 20:22:05	...map word to index.
2017:03:09 20:22:07	...some data statistics.
2017:03:09 20:22:07	...save processed data to file.
2017:03:09 20:22:12	the sample size is 97314, the vocab size is 18739
2017:03:09 20:22:15	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489090935

2017:03:09 20:27:28	use DataLoaderChildrenStory to init data.
2017:03:09 20:27:28	loading preprocessed files.
2017:03:09 20:27:41	the sample size is 97314, the vocab size is 18739
2017:03:09 20:27:44	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489091264

2017:03:10 10:05:00	use DataLoaderChildrenStory to init data.
2017:03:10 10:05:00	loading preprocessed files.
2017:03:10 10:05:08	the sample size is 97314, the vocab size is 18739
2017:03:10 10:09:00	use DataLoaderChildrenStory to init data.
2017:03:10 10:09:00	loading preprocessed files.
2017:03:10 10:09:08	the sample size is 97314, the vocab size is 18739
2017:03:10 10:17:11	use DataLoaderChildrenStory to init data.
2017:03:10 10:17:11	loading preprocessed files.
2017:03:10 10:17:19	the sample size is 97314, the vocab size is 18739
2017:03:10 10:22:41	use DataLoaderChildrenStory to init data.
2017:03:10 10:22:41	loading preprocessed files.
2017:03:10 10:22:49	the sample size is 97314, the vocab size is 18739
2017:03:10 10:23:06	use DataLoaderChildrenStory to init data.
2017:03:10 10:23:06	loading preprocessed files.
2017:03:10 10:23:13	the sample size is 97314, the vocab size is 18739
2017:03:10 12:52:55	use DataLoaderChildrenStory to init data.
2017:03:10 12:52:55	loading preprocessed files.
2017:03:10 12:53:03	the sample size is 97314, the vocab size is 18739
2017:03:10 12:53:37	use DataLoaderChildrenStory to init data.
2017:03:10 12:53:37	loading preprocessed files.
2017:03:10 12:53:43	the sample size is 97314, the vocab size is 18739
2017:03:10 13:24:25	use DataLoaderChildrenStory to init data.
2017:03:10 13:24:25	loading preprocessed files.
2017:03:10 13:24:33	the sample size is 97314, the vocab size is 18739
2017:03:10 13:25:03	use DataLoaderChildrenStory to init data.
2017:03:10 13:25:03	loading preprocessed files.
2017:03:10 13:25:12	the sample size is 97314, the vocab size is 18739
2017:03:10 13:27:03	use DataLoaderChildrenStory to init data.
2017:03:10 13:27:03	loading preprocessed files.
2017:03:10 13:27:11	the sample size is 97314, the vocab size is 18739
2017:03:10 13:30:02	use DataLoaderChildrenStory to init data.
2017:03:10 13:30:02	loading preprocessed files.
2017:03:10 13:30:10	the sample size is 97314, the vocab size is 18739
2017:03:10 13:31:53	use DataLoaderChildrenStory to init data.
2017:03:10 13:31:53	loading preprocessed files.
2017:03:10 13:32:01	the sample size is 97314, the vocab size is 18739
2017:03:10 14:04:41	use DataLoaderChildrenStory to init data.
2017:03:10 14:04:41	loading preprocessed files.
2017:03:10 14:04:48	the sample size is 97314, the vocab size is 18739
2017:03:10 14:06:44	use DataLoaderChildrenStory to init data.
2017:03:10 14:06:44	loading preprocessed files.
2017:03:10 14:06:51	the sample size is 97314, the vocab size is 18739
2017:03:10 14:08:09	use DataLoaderChildrenStory to init data.
2017:03:10 14:08:09	loading preprocessed files.
2017:03:10 14:08:17	the sample size is 97314, the vocab size is 18739
2017:03:10 14:09:06	use DataLoaderChildrenStory to init data.
2017:03:10 14:09:06	loading preprocessed files.
2017:03:10 14:09:14	the sample size is 97314, the vocab size is 18739
2017:03:10 14:16:31	use DataLoaderChildrenStory to init data.
2017:03:10 14:16:31	loading preprocessed files.
2017:03:10 14:16:39	the sample size is 97314, the vocab size is 18739
2017:03:10 14:18:23	use DataLoaderChildrenStory to init data.
2017:03:10 14:18:23	loading preprocessed files.
2017:03:10 14:18:30	the sample size is 97314, the vocab size is 18739
2017:03:10 14:21:17	use DataLoaderChildrenStory to init data.
2017:03:10 14:21:17	loading preprocessed files.
2017:03:10 14:21:24	the sample size is 97314, the vocab size is 18739
2017:03:10 14:21:46	use DataLoaderChildrenStory to init data.
2017:03:10 14:21:46	loading preprocessed files.
2017:03:10 14:21:55	the sample size is 97314, the vocab size is 18739
2017:03:10 14:22:01	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489155721

2017:03:10 14:22:03	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489155721

2017:03:10 14:22:05	------ do the pretrain ------ 

2017:03:10 14:24:47	use DataLoaderChildrenStory to init data.
2017:03:10 14:24:47	loading preprocessed files.
2017:03:10 14:24:55	the sample size is 97314, the vocab size is 18739
2017:03:10 14:25:01	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489155901

2017:03:10 14:25:05	------ do the pretrain ------ 

2017:03:10 15:18:57	use DataLoaderChildrenStory to init data.
2017:03:10 15:18:57	loading preprocessed files.
2017:03:10 15:19:05	the sample size is 97314, the vocab size is 18739
2017:03:10 15:19:24	use DataLoaderChildrenStory to init data.
2017:03:10 15:19:24	loading preprocessed files.
2017:03:10 15:19:31	the sample size is 97314, the vocab size is 18739
2017:03:10 15:19:40	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489159179

2017:03:10 15:19:43	------ do the pretrain ------ 

2017:03:10 15:25:16	use DataLoaderChildrenStory to init data.
2017:03:10 15:25:16	loading preprocessed files.
2017:03:10 15:25:24	the sample size is 97314, the vocab size is 18739
2017:03:10 15:26:05	use DataLoaderChildrenStory to init data.
2017:03:10 15:26:05	loading preprocessed files.
2017:03:10 15:26:12	the sample size is 97314, the vocab size is 18739
2017:03:10 15:27:10	use DataLoaderChildrenStory to init data.
2017:03:10 15:27:10	loading preprocessed files.
2017:03:10 15:27:17	the sample size is 97314, the vocab size is 18739
2017:03:10 15:27:23	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489159643

2017:03:10 15:27:27	------ do the pretrain ------ 

2017:03:10 15:28:54	use DataLoaderChildrenStory to init data.
2017:03:10 15:28:54	loading preprocessed files.
2017:03:10 15:29:01	the sample size is 97314, the vocab size is 18739
2017:03:10 15:29:07	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489159747

2017:03:10 15:31:03	use DataLoaderChildrenStory to init data.
2017:03:10 15:31:03	loading preprocessed files.
2017:03:10 15:31:10	the sample size is 97314, the vocab size is 18739
2017:03:10 15:31:17	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489159877

2017:03:10 15:31:49	use DataLoaderChildrenStory to init data.
2017:03:10 15:31:49	loading preprocessed files.
2017:03:10 15:31:57	the sample size is 97314, the vocab size is 18739
2017:03:10 15:32:19	use DataLoaderChildrenStory to init data.
2017:03:10 15:32:19	loading preprocessed files.
2017:03:10 15:32:26	the sample size is 97314, the vocab size is 18739
2017:03:10 15:32:32	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489159952

2017:03:10 15:33:32	use DataLoaderChildrenStory to init data.
2017:03:10 15:33:32	loading preprocessed files.
2017:03:10 15:33:40	the sample size is 97314, the vocab size is 18739
2017:03:10 15:34:06	use DataLoaderChildrenStory to init data.
2017:03:10 15:34:06	loading preprocessed files.
2017:03:10 15:34:25	use DataLoaderChildrenStory to init data.
2017:03:10 15:34:25	loading preprocessed files.
2017:03:10 15:34:32	the sample size is 97314, the vocab size is 18739
2017:03:10 15:35:07	use DataLoaderChildrenStory to init data.
2017:03:10 15:35:07	loading preprocessed files.
2017:03:10 15:35:15	the sample size is 97314, the vocab size is 18739
2017:03:10 15:36:35	use DataLoaderChildrenStory to init data.
2017:03:10 15:36:35	loading preprocessed files.
2017:03:10 15:36:43	the sample size is 97314, the vocab size is 18739
2017:03:10 15:37:23	use DataLoaderChildrenStory to init data.
2017:03:10 15:37:23	loading preprocessed files.
2017:03:10 15:37:31	the sample size is 97314, the vocab size is 18739
2017:03:10 15:37:59	use DataLoaderChildrenStory to init data.
2017:03:10 15:37:59	loading preprocessed files.
2017:03:10 15:38:06	the sample size is 97314, the vocab size is 18739
2017:03:10 15:38:13	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489160292

2017:03:10 15:38:16	------ do the pretrain ------ 

2017:03:10 15:39:12	use DataLoaderChildrenStory to init data.
2017:03:10 15:39:12	loading preprocessed files.
2017:03:10 15:39:19	the sample size is 97314, the vocab size is 18739
2017:03:10 15:40:22	use DataLoaderChildrenStory to init data.
2017:03:10 15:40:22	loading preprocessed files.
2017:03:10 15:40:30	the sample size is 97314, the vocab size is 18739
2017:03:10 15:41:50	use DataLoaderChildrenStory to init data.
2017:03:10 15:41:50	loading preprocessed files.
2017:03:10 15:41:57	the sample size is 97314, the vocab size is 18739
2017:03:10 16:52:18	use DataLoaderChildrenStory to init data.
2017:03:10 16:52:18	loading preprocessed files.
2017:03:10 16:52:25	the sample size is 97314, the vocab size is 18739
2017:03:10 16:52:46	use DataLoaderChildrenStory to init data.
2017:03:10 16:52:46	loading preprocessed files.
2017:03:10 16:52:52	the sample size is 97314, the vocab size is 18739
2017:03:10 16:54:08	use DataLoaderChildrenStory to init data.
2017:03:10 16:54:08	loading preprocessed files.
2017:03:10 16:54:15	the sample size is 97314, the vocab size is 18739
2017:03:10 16:55:27	use DataLoaderChildrenStory to init data.
2017:03:10 16:55:27	loading preprocessed files.
2017:03:10 16:55:34	the sample size is 97314, the vocab size is 18739
2017:03:10 16:55:58	use DataLoaderChildrenStory to init data.
2017:03:10 16:55:58	loading preprocessed files.
2017:03:10 16:56:05	the sample size is 97314, the vocab size is 18739
2017:03:10 17:03:23	use DataLoaderChildrenStory to init data.
2017:03:10 17:03:23	loading preprocessed files.
2017:03:10 17:03:30	the sample size is 97314, the vocab size is 18739
2017:03:10 17:03:37	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489165416

2017:03:10 17:03:41	------ do the pretrain ------ 

2017:03:10 17:04:59	use DataLoaderChildrenStory to init data.
2017:03:10 17:04:59	loading preprocessed files.
2017:03:10 17:05:06	the sample size is 97314, the vocab size is 18739
2017:03:10 17:05:40	use DataLoaderChildrenStory to init data.
2017:03:10 17:05:40	loading preprocessed files.
2017:03:10 17:05:47	the sample size is 97314, the vocab size is 18739
2017:03:10 17:06:29	use DataLoaderChildrenStory to init data.
2017:03:10 17:06:29	loading preprocessed files.
2017:03:10 17:06:36	the sample size is 97314, the vocab size is 18739
2017:03:10 17:07:44	use DataLoaderChildrenStory to init data.
2017:03:10 17:07:44	loading preprocessed files.
2017:03:10 17:07:50	the sample size is 97314, the vocab size is 18739
2017:03:10 17:08:01	use DataLoaderChildrenStory to init data.
2017:03:10 17:08:01	loading preprocessed files.
2017:03:10 17:08:08	the sample size is 97314, the vocab size is 18739
2017:03:10 17:10:51	use DataLoaderChildrenStory to init data.
2017:03:10 17:10:51	loading preprocessed files.
2017:03:10 17:10:58	the sample size is 97314, the vocab size is 18739
2017:03:10 17:11:20	use DataLoaderChildrenStory to init data.
2017:03:10 17:11:20	loading preprocessed files.
2017:03:10 17:11:27	the sample size is 97314, the vocab size is 18739
2017:03:10 17:16:09	use DataLoaderChildrenStory to init data.
2017:03:10 17:16:09	loading preprocessed files.
2017:03:10 17:16:16	the sample size is 97314, the vocab size is 18739
2017:03:10 17:16:26	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV1.TextGANV1/1489166186

2017:03:10 17:16:32	------ do the pretrain ------ 

2017:03:10 17:16:48	use DataLoaderChildrenStory to init data.
2017:03:10 17:16:48	loading preprocessed files.
2017:03:10 17:16:55	the sample size is 97314, the vocab size is 18739
2017:03:10 17:17:07	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV1.TextGANV1/1489166227

2017:03:10 17:17:13	------ do the pretrain ------ 

2017:03:10 17:18:18	use DataLoaderChildrenStory to init data.
2017:03:10 17:18:18	loading preprocessed files.
2017:03:10 17:18:25	the sample size is 97314, the vocab size is 18739
2017:03:10 17:18:34	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV1.TextGANV1/1489166314

2017:03:10 17:18:39	------ do the pretrain ------ 

2017:03:10 17:19:43	use DataLoaderChildrenStory to init data.
2017:03:10 17:19:43	loading preprocessed files.
2017:03:10 17:19:50	the sample size is 97314, the vocab size is 18739
2017:03:10 17:19:59	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV1.TextGANV1/1489166399

2017:03:10 17:20:04	------ do the pretrain ------ 

2017:03:10 17:21:52	use DataLoaderChildrenStory to init data.
2017:03:10 17:21:52	loading preprocessed files.
2017:03:10 17:21:58	the sample size is 97314, the vocab size is 18739
2017:03:10 17:22:05	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489166524

2017:03:10 17:22:08	------ do the pretrain ------ 

2017:03:11 14:08:57	use DataLoaderChildrenStory to init data.
2017:03:11 14:08:57	loading preprocessed files.
2017:03:11 14:09:06	the sample size is 97314, the vocab size is 18739
2017:03:11 14:09:38	use DataLoaderChildrenStory to init data.
2017:03:11 14:09:38	loading preprocessed files.
2017:03:11 14:09:46	the sample size is 97314, the vocab size is 18739
2017:03:11 14:13:02	use DataLoaderChildrenStory to init data.
2017:03:11 14:13:02	loading preprocessed files.
2017:03:11 14:13:09	the sample size is 97314, the vocab size is 18739
2017:03:11 14:22:59	use DataLoaderChildrenStory to init data.
2017:03:11 14:22:59	loading preprocessed files.
2017:03:11 14:23:06	the sample size is 97314, the vocab size is 18739
2017:03:11 14:23:16	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV2.TextGANV2/1489242196

2017:03:11 14:23:21	------ do the pretrain ------ 

2017:03:12 21:48:43	use DataLoaderChildrenStory to init data.
2017:03:12 21:49:11	use DataLoaderChildrenStory to init data.
2017:03:12 21:49:11	loading preprocessed files.
2017:03:12 21:49:19	the sample size is 97314, the vocab size is 18739
2017:03:12 21:50:02	use DataLoaderChildrenStory to init data.
2017:03:12 21:50:02	loading preprocessed files.
2017:03:12 21:50:09	the sample size is 97314, the vocab size is 18739
2017:03:12 21:50:16	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489355415

2017:03:12 21:50:19	------ do the pretrain ------ 

2017:03:12 21:53:24	use DataLoaderChildrenStory to init data.
2017:03:12 21:53:24	loading preprocessed files.
2017:03:12 21:53:32	the sample size is 97314, the vocab size is 18739
2017:03:12 21:53:38	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489355618

2017:03:12 21:53:42	------ do the pretrain ------ 

2017:03:12 21:53:43	save best model to: /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489355618/checkpoints/bestmodel-0.

2017:03:12 21:53:43	------ do the standard GAN training ------ 

2017:03:12 21:53:43	------ generate sentence from latent space / noice ------ 

2017:03:12 21:55:14	use DataLoaderChildrenStory to init data.
2017:03:12 21:55:14	loading preprocessed files.
2017:03:12 21:55:21	the sample size is 97314, the vocab size is 18739
2017:03:12 21:55:27	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489355727

2017:03:12 21:55:31	------ do the pretrain ------ 

2017:03:12 21:55:32	save best model to: /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489355727/checkpoints/bestmodel-0.

2017:03:12 21:55:32	------ do the standard GAN training ------ 

2017:03:12 21:55:32	------ generate sentence from latent space / noice ------ 

2017:03:12 21:56:51	use DataLoaderChildrenStory to init data.
2017:03:12 21:56:51	loading preprocessed files.
2017:03:12 21:56:59	the sample size is 97314, the vocab size is 18739
2017:03:12 21:57:05	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489355825

2017:03:12 21:57:08	------ do the pretrain ------ 

2017:03:12 21:57:09	save best model to: /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489355825/checkpoints/bestmodel-0.

2017:03:12 21:57:09	------ do the standard GAN training ------ 

2017:03:12 21:57:09	------ generate sentence from latent space / noice ------ 

2017:03:12 21:58:05	use DataLoaderChildrenStory to init data.
2017:03:12 21:58:05	loading preprocessed files.
2017:03:12 21:58:12	the sample size is 97314, the vocab size is 18739
2017:03:12 21:58:18	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489355898

2017:03:12 21:58:22	------ do the pretrain ------ 

2017:03:12 21:58:23	save best model to: /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489355898/checkpoints/bestmodel-0.

2017:03:12 21:58:23	------ do the standard GAN training ------ 

2017:03:12 21:58:23	------ generate sentence from latent space / noice ------ 

2017:03:12 21:58:55	use DataLoaderChildrenStory to init data.
2017:03:12 21:58:55	loading preprocessed files.
2017:03:12 21:59:03	the sample size is 97314, the vocab size is 18739
2017:03:12 21:59:10	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489355949

2017:03:12 22:01:03	use DataLoaderChildrenStory to init data.
2017:03:12 22:01:03	loading preprocessed files.
2017:03:12 22:01:11	the sample size is 97314, the vocab size is 18739
2017:03:12 22:01:18	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489356078

2017:03:12 22:01:22	------ do the pretrain ------ 

2017:03:12 22:01:22	save best model to: /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489356078/checkpoints/bestmodel-0.

2017:03:12 22:01:22	------ do the standard GAN training ------ 

2017:03:12 22:01:22	------ generate sentence from latent space / noice ------ 

2017:03:12 23:21:39	use DataLoaderChildrenStory to init data.
2017:03:12 23:21:39	loading preprocessed files.
2017:03:12 23:21:47	the sample size is 97314, the vocab size is 18739
2017:03:12 23:22:36	use DataLoaderChildrenStory to init data.
2017:03:12 23:22:36	loading preprocessed files.
2017:03:12 23:22:43	the sample size is 97314, the vocab size is 18739
2017:03:12 23:23:19	use DataLoaderChildrenStory to init data.
2017:03:12 23:23:19	loading preprocessed files.
2017:03:12 23:23:26	the sample size is 97314, the vocab size is 18739
2017:03:12 23:23:45	use DataLoaderChildrenStory to init data.
2017:03:12 23:23:45	loading preprocessed files.
2017:03:12 23:23:52	the sample size is 97314, the vocab size is 18739
2017:03:12 23:28:21	use DataLoaderChildrenStory to init data.
2017:03:12 23:28:21	loading preprocessed files.
2017:03:12 23:28:28	the sample size is 97314, the vocab size is 18739
2017:03:12 23:31:40	use DataLoaderChildrenStory to init data.
2017:03:12 23:31:40	loading preprocessed files.
2017:03:12 23:31:47	the sample size is 97314, the vocab size is 18739
2017:03:12 23:36:45	use DataLoaderChildrenStory to init data.
2017:03:12 23:36:45	loading preprocessed files.
2017:03:12 23:36:52	the sample size is 97314, the vocab size is 18739
2017:03:12 23:36:59	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489361819

2017:03:12 23:37:03	------ do the pretrain ------ 

2017:03:12 23:37:04	------ do the standard GAN training ------ 

2017:03:12 23:37:04	total execution time: 18
2017:03:12 23:37:40	use DataLoaderChildrenStory to init data.
2017:03:12 23:37:40	loading preprocessed files.
2017:03:12 23:37:47	the sample size is 97314, the vocab size is 18739
2017:03:12 23:39:15	use DataLoaderChildrenStory to init data.
2017:03:12 23:39:15	loading preprocessed files.
2017:03:12 23:39:22	the sample size is 97314, the vocab size is 18739
2017:03:12 23:39:29	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489361968

2017:03:12 23:39:33	------ do the pretrain ------ 

2017:03:12 23:39:33	save best model to: /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489361968/checkpoints/bestmodel-0.

2017:03:12 23:39:33	------ do the standard GAN training ------ 

2017:03:12 23:39:33	total execution time: 18
2017:03:12 23:39:52	use DataLoaderChildrenStory to init data.
2017:03:12 23:39:52	loading preprocessed files.
2017:03:12 23:39:58	the sample size is 97314, the vocab size is 18739
2017:03:12 23:43:09	use DataLoaderChildrenStory to init data.
2017:03:12 23:43:09	loading preprocessed files.
2017:03:12 23:43:17	the sample size is 97314, the vocab size is 18739
2017:03:12 23:45:19	use DataLoaderChildrenStory to init data.
2017:03:12 23:45:19	loading preprocessed files.
2017:03:12 23:45:27	the sample size is 97314, the vocab size is 18739
2017:03:13 09:17:42	use DataLoaderChildrenStory to init data.
2017:03:13 09:17:42	loading preprocessed files.
2017:03:13 09:17:50	the sample size is 97314, the vocab size is 18739
2017:03:13 09:18:17	use DataLoaderChildrenStory to init data.
2017:03:13 09:18:17	loading preprocessed files.
2017:03:13 09:18:25	the sample size is 97314, the vocab size is 18739
2017:03:13 09:19:57	use DataLoaderChildrenStory to init data.
2017:03:13 09:19:57	loading preprocessed files.
2017:03:13 09:20:05	the sample size is 97314, the vocab size is 18739
2017:03:13 09:21:10	use DataLoaderChildrenStory to init data.
2017:03:13 09:21:10	loading preprocessed files.
2017:03:13 09:21:16	the sample size is 97314, the vocab size is 18739
2017:03:13 09:29:20	use DataLoaderChildrenStory to init data.
2017:03:13 09:29:20	loading preprocessed files.
2017:03:13 09:29:28	the sample size is 97314, the vocab size is 18739
2017:03:13 09:33:55	use DataLoaderChildrenStory to init data.
2017:03:13 09:33:55	loading preprocessed files.
2017:03:13 09:34:02	the sample size is 97314, the vocab size is 18739
2017:03:13 09:35:25	use DataLoaderChildrenStory to init data.
2017:03:13 09:35:25	loading preprocessed files.
2017:03:13 09:35:32	the sample size is 97314, the vocab size is 18739
2017:03:13 09:37:10	use DataLoaderChildrenStory to init data.
2017:03:13 09:37:10	loading preprocessed files.
2017:03:13 09:37:18	the sample size is 97314, the vocab size is 18739
2017:03:13 09:38:06	use DataLoaderChildrenStory to init data.
2017:03:13 09:38:06	loading preprocessed files.
2017:03:13 09:38:13	the sample size is 97314, the vocab size is 18739
2017:03:13 09:53:11	use DataLoaderChildrenStory to init data.
2017:03:13 09:53:11	loading preprocessed files.
2017:03:13 09:53:19	the sample size is 97314, the vocab size is 18739
2017:03:13 09:54:12	use DataLoaderChildrenStory to init data.
2017:03:13 09:54:12	loading preprocessed files.
2017:03:13 09:54:20	the sample size is 97314, the vocab size is 18739
2017:03:13 09:55:47	use DataLoaderChildrenStory to init data.
2017:03:13 09:55:47	loading preprocessed files.
2017:03:13 09:55:54	the sample size is 97314, the vocab size is 18739
2017:03:13 09:56:07	use DataLoaderChildrenStory to init data.
2017:03:13 09:56:07	loading preprocessed files.
2017:03:13 09:56:13	the sample size is 97314, the vocab size is 18739
2017:03:13 10:31:59	use DataLoaderChildrenStory to init data.
2017:03:13 10:31:59	loading preprocessed files.
2017:03:13 10:32:08	the sample size is 97314, the vocab size is 18739
2017:03:13 10:33:09	use DataLoaderChildrenStory to init data.
2017:03:13 10:33:09	loading preprocessed files.
2017:03:13 10:33:17	the sample size is 97314, the vocab size is 18739
2017:03:13 10:40:03	use DataLoaderChildrenStory to init data.
2017:03:13 10:40:03	loading preprocessed files.
2017:03:13 10:40:12	the sample size is 97314, the vocab size is 18739
2017:03:13 10:43:03	use DataLoaderChildrenStory to init data.
2017:03:13 10:43:03	loading preprocessed files.
2017:03:13 10:43:10	the sample size is 97314, the vocab size is 18739
2017:03:13 11:24:55	use DataLoaderChildrenStory to init data.
2017:03:13 11:24:55	loading preprocessed files.
2017:03:13 11:25:10	the sample size is 97314, the vocab size is 18739
2017:03:13 11:25:49	use DataLoaderChildrenStory to init data.
2017:03:13 11:25:49	loading preprocessed files.
2017:03:13 11:25:57	the sample size is 97314, the vocab size is 18739
2017:03:13 11:26:27	use DataLoaderChildrenStory to init data.
2017:03:13 11:26:27	loading preprocessed files.
2017:03:13 11:26:36	the sample size is 97314, the vocab size is 18739
2017:03:13 11:27:02	use DataLoaderChildrenStory to init data.
2017:03:13 11:27:02	loading preprocessed files.
2017:03:13 11:27:10	the sample size is 97314, the vocab size is 18739
2017:03:13 14:06:27	use DataLoaderChildrenStory to init data.
2017:03:13 14:06:27	loading preprocessed files.
2017:03:13 14:06:35	the sample size is 97314, the vocab size is 18739
2017:03:13 14:06:42	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489414002

2017:03:13 14:06:46	------ do the pretrain ------ 

2017:03:13 14:06:46	save bestmodel:1.

2017:03:13 14:06:46	------ do the standard GAN training ------ 

2017:03:13 14:06:46	total execution time: 19
2017:03:13 14:07:05	use DataLoaderChildrenStory to init data.
2017:03:13 14:07:05	loading preprocessed files.
2017:03:13 14:07:12	the sample size is 97314, the vocab size is 18739
2017:03:13 14:07:19	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489414038

2017:03:13 14:07:22	------ do the pretrain ------ 

2017:03:13 14:07:22	pretrain epoch 0
2017:03:13 14:58:07	use DataLoaderChildrenStory to init data.
2017:03:13 14:58:07	loading preprocessed files.
2017:03:13 14:58:15	the sample size is 97314, the vocab size is 18739
2017:03:13 14:59:09	use DataLoaderChildrenStory to init data.
2017:03:13 14:59:09	loading preprocessed files.
2017:03:13 14:59:16	the sample size is 97314, the vocab size is 18739
2017:03:13 14:59:49	use DataLoaderChildrenStory to init data.
2017:03:13 14:59:49	loading preprocessed files.
2017:03:13 14:59:56	the sample size is 97314, the vocab size is 18739
2017:03:13 15:00:04	use DataLoaderChildrenStory to init data.
2017:03:13 15:00:04	loading preprocessed files.
2017:03:13 15:00:11	the sample size is 97314, the vocab size is 18739
2017:03:13 15:00:34	use DataLoaderChildrenStory to init data.
2017:03:13 15:00:34	loading preprocessed files.
2017:03:13 15:00:41	the sample size is 97314, the vocab size is 18739
2017:03:13 15:01:23	use DataLoaderChildrenStory to init data.
2017:03:13 15:01:23	loading preprocessed files.
2017:03:13 15:01:30	the sample size is 97314, the vocab size is 18739
2017:03:13 15:02:59	use DataLoaderChildrenStory to init data.
2017:03:13 15:02:59	loading preprocessed files.
2017:03:13 15:03:05	the sample size is 97314, the vocab size is 18739
2017:03:13 15:03:30	use DataLoaderChildrenStory to init data.
2017:03:13 15:03:30	loading preprocessed files.
2017:03:13 15:03:37	the sample size is 97314, the vocab size is 18739
2017:03:13 15:04:00	use DataLoaderChildrenStory to init data.
2017:03:13 15:04:00	loading preprocessed files.
2017:03:13 15:04:07	the sample size is 97314, the vocab size is 18739
2017:03:13 15:06:46	use DataLoaderChildrenStory to init data.
2017:03:13 15:06:46	loading preprocessed files.
2017:03:13 15:06:53	the sample size is 97314, the vocab size is 18739
2017:03:13 15:07:50	use DataLoaderChildrenStory to init data.
2017:03:13 15:07:50	loading preprocessed files.
2017:03:13 15:07:56	the sample size is 97314, the vocab size is 18739
2017:03:13 15:08:03	restore checkpoint_file from path: /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489361968/checkpoints/bestmodel-0
2017:03:13 15:09:05	use DataLoaderChildrenStory to init data.
2017:03:13 15:09:05	loading preprocessed files.
2017:03:13 15:09:12	the sample size is 97314, the vocab size is 18739
2017:03:13 15:09:18	restore checkpoint_file from path: /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489361968/checkpoints/bestmodel-0
2017:03:13 15:09:19	generate sentence and write to path: /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489361968/checkpoints
2017:03:13 15:09:23	use DataLoaderChildrenStory to init data.
2017:03:13 15:09:23	loading preprocessed files.
2017:03:13 15:09:30	the sample size is 97314, the vocab size is 18739
2017:03:13 15:09:36	restore checkpoint_file from path: /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489361968/checkpoints/bestmodel-0
2017:03:13 15:09:37	generate sentence and write to path: /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489361968/checkpoints
2017:03:13 15:53:32	use DataLoaderChildrenStory to init data.
2017:03:13 15:53:32	loading preprocessed files.
2017:03:13 15:53:40	the sample size is 97314, the vocab size is 18739
2017:03:13 15:53:46	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489420426

2017:03:13 15:54:33	use DataLoaderChildrenStory to init data.
2017:03:13 15:54:33	loading preprocessed files.
2017:03:13 15:54:40	the sample size is 97314, the vocab size is 18739
2017:03:13 15:54:47	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489420487

2017:03:13 15:54:50	------ do the pretrain ------ 

2017:03:13 15:54:50	pretrain epoch 0
2017:03:13 16:13:18	use DataLoaderChildrenStory to init data.
2017:03:13 16:13:18	loading preprocessed files.
2017:03:13 16:13:25	the sample size is 97314, the vocab size is 18739
2017:03:13 16:14:08	use DataLoaderChildrenStory to init data.
2017:03:13 16:14:08	loading preprocessed files.
2017:03:13 16:14:16	the sample size is 97314, the vocab size is 18739
2017:03:13 16:14:36	use DataLoaderChildrenStory to init data.
2017:03:13 16:14:36	loading preprocessed files.
2017:03:13 16:14:44	the sample size is 97314, the vocab size is 18739
2017:03:13 16:16:23	use DataLoaderChildrenStory to init data.
2017:03:13 16:16:23	loading preprocessed files.
2017:03:13 16:16:30	the sample size is 97314, the vocab size is 18739
2017:03:13 16:16:44	use DataLoaderChildrenStory to init data.
2017:03:13 16:16:44	loading preprocessed files.
2017:03:13 16:16:51	the sample size is 97314, the vocab size is 18739
2017:03:13 16:17:13	use DataLoaderChildrenStory to init data.
2017:03:13 16:17:13	loading preprocessed files.
2017:03:13 16:17:20	the sample size is 97314, the vocab size is 18739
2017:03:13 16:20:50	use DataLoaderChildrenStory to init data.
2017:03:13 16:20:50	loading preprocessed files.
2017:03:13 16:20:57	the sample size is 97314, the vocab size is 18739
2017:03:13 16:21:24	use DataLoaderChildrenStory to init data.
2017:03:13 16:21:24	loading preprocessed files.
2017:03:13 16:21:31	the sample size is 97314, the vocab size is 18739
2017:03:13 16:21:49	use DataLoaderChildrenStory to init data.
2017:03:13 16:21:49	loading preprocessed files.
2017:03:13 16:21:56	the sample size is 97314, the vocab size is 18739
2017:03:13 16:23:14	use DataLoaderChildrenStory to init data.
2017:03:13 16:23:14	loading preprocessed files.
2017:03:13 16:23:21	the sample size is 97314, the vocab size is 18739
2017:03:13 16:29:59	use DataLoaderChildrenStory to init data.
2017:03:13 16:29:59	loading preprocessed files.
2017:03:13 16:30:07	the sample size is 97314, the vocab size is 18739
2017:03:13 16:34:17	use DataLoaderChildrenStory to init data.
2017:03:13 16:34:17	loading preprocessed files.
2017:03:13 16:34:25	the sample size is 97314, the vocab size is 18739
2017:03:13 16:34:31	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489422871

2017:03:13 16:34:35	------ do the pretrain ------ 

2017:03:13 16:34:35	pretrain epoch 0
2017:03:13 16:35:53	use DataLoaderChildrenStory to init data.
2017:03:13 16:35:53	loading preprocessed files.
2017:03:13 16:36:01	the sample size is 97314, the vocab size is 18739
2017:03:13 16:38:09	use DataLoaderChildrenStory to init data.
2017:03:13 16:38:09	loading preprocessed files.
2017:03:13 16:38:16	the sample size is 97314, the vocab size is 18739
2017:03:13 16:38:30	use DataLoaderChildrenStory to init data.
2017:03:13 16:38:30	loading preprocessed files.
2017:03:13 16:38:37	the sample size is 97314, the vocab size is 18739
2017:03:13 16:38:43	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489423123

2017:03:13 16:38:46	------ do the pretrain ------ 

2017:03:13 16:38:46	pretrain epoch 0
2017:03:13 16:39:26	use DataLoaderChildrenStory to init data.
2017:03:13 16:39:26	loading preprocessed files.
2017:03:13 16:39:33	the sample size is 97314, the vocab size is 18739
2017:03:13 16:40:34	use DataLoaderChildrenStory to init data.
2017:03:13 16:40:34	loading preprocessed files.
2017:03:13 16:40:41	the sample size is 97314, the vocab size is 18739
2017:03:13 16:56:10	use DataLoaderChildrenStory to init data.
2017:03:13 16:56:10	loading preprocessed files.
2017:03:13 16:56:17	the sample size is 97314, the vocab size is 18739
2017:03:13 16:56:43	use DataLoaderChildrenStory to init data.
2017:03:13 16:56:43	loading preprocessed files.
2017:03:13 16:56:50	the sample size is 97314, the vocab size is 18739
2017:03:13 16:56:59	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV1.TextGANV1/1489424219

2017:03:13 16:58:58	use DataLoaderChildrenStory to init data.
2017:03:13 16:58:58	loading preprocessed files.
2017:03:13 16:59:06	the sample size is 97314, the vocab size is 18739
2017:03:13 16:59:31	use DataLoaderChildrenStory to init data.
2017:03:13 16:59:31	loading preprocessed files.
2017:03:13 16:59:38	the sample size is 97314, the vocab size is 18739
2017:03:13 16:59:56	use DataLoaderChildrenStory to init data.
2017:03:13 16:59:56	loading preprocessed files.
2017:03:13 17:00:03	the sample size is 97314, the vocab size is 18739
2017:03:13 17:01:12	use DataLoaderChildrenStory to init data.
2017:03:13 17:01:12	loading preprocessed files.
2017:03:13 17:01:19	the sample size is 97314, the vocab size is 18739
2017:03:13 17:02:25	use DataLoaderChildrenStory to init data.
2017:03:13 17:02:25	loading preprocessed files.
2017:03:13 17:02:32	the sample size is 97314, the vocab size is 18739
2017:03:13 17:04:50	use DataLoaderChildrenStory to init data.
2017:03:13 17:04:50	loading preprocessed files.
2017:03:13 17:04:57	the sample size is 97314, the vocab size is 18739
2017:03:13 17:05:08	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV1.TextGANV1/1489424707

2017:03:13 17:05:13	------ do the pretrain ------ 

2017:03:13 17:05:39	use DataLoaderChildrenStory to init data.
2017:03:13 17:05:39	loading preprocessed files.
2017:03:13 17:05:46	the sample size is 97314, the vocab size is 18739
2017:03:13 17:05:56	use DataLoaderChildrenStory to init data.
2017:03:13 17:05:56	loading preprocessed files.
2017:03:13 17:06:03	the sample size is 97314, the vocab size is 18739
2017:03:13 17:06:14	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV1.TextGANV1/1489424774

2017:03:13 17:07:25	use DataLoaderChildrenStory to init data.
2017:03:13 17:07:25	loading preprocessed files.
2017:03:13 17:07:32	the sample size is 97314, the vocab size is 18739
2017:03:13 17:07:44	use DataLoaderChildrenStory to init data.
2017:03:13 17:07:44	loading preprocessed files.
2017:03:13 17:07:51	the sample size is 97314, the vocab size is 18739
2017:03:13 17:08:01	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV1.TextGANV1/1489424881

2017:03:13 17:08:06	------ do the pretrain ------ 

2017:03:13 17:08:08	save bestmodel:1.

2017:03:13 17:08:08	------ do the standard GAN training ------ 

2017:03:13 17:08:08	total execution time: 23
2017:03:13 18:18:23	use DataLoaderChildrenStory to init data.
2017:03:13 18:18:23	loading preprocessed files.
2017:03:13 18:18:29	the sample size is 97314, the vocab size is 18739
2017:03:13 18:22:18	use DataLoaderChildrenStory to init data.
2017:03:13 18:22:18	loading preprocessed files.
2017:03:13 18:22:24	the sample size is 97314, the vocab size is 18739
2017:03:13 18:37:52	use DataLoaderChildrenStory to init data.
2017:03:13 18:37:52	loading preprocessed files.
2017:03:13 18:37:59	the sample size is 97314, the vocab size is 18739
2017:03:13 18:39:38	use DataLoaderChildrenStory to init data.
2017:03:13 18:39:38	loading preprocessed files.
2017:03:13 18:39:45	the sample size is 97314, the vocab size is 18739
2017:03:13 18:40:12	use DataLoaderChildrenStory to init data.
2017:03:13 18:40:12	loading preprocessed files.
2017:03:13 18:40:19	the sample size is 97314, the vocab size is 18739
2017:03:13 18:42:15	use DataLoaderChildrenStory to init data.
2017:03:13 18:42:15	loading preprocessed files.
2017:03:13 18:42:22	the sample size is 97314, the vocab size is 18739
2017:03:13 18:42:27	use DataLoaderChildrenStory to init data.
2017:03:13 18:42:27	loading preprocessed files.
2017:03:13 18:42:36	the sample size is 97314, the vocab size is 18739
2017:03:13 18:42:46	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV1.TextGANV1/1489430566

2017:03:13 18:42:51	------ do the pretrain ------ 

2017:03:13 18:42:51	pretrain epoch 0
2017:03:13 18:42:59	use DataLoaderChildrenStory to init data.
2017:03:13 18:42:59	loading preprocessed files.
2017:03:13 18:43:06	the sample size is 97314, the vocab size is 18739
2017:03:13 18:45:09	use DataLoaderChildrenStory to init data.
2017:03:13 18:45:09	loading preprocessed files.
2017:03:13 18:45:16	the sample size is 97314, the vocab size is 18739
2017:03:13 18:46:43	use DataLoaderChildrenStory to init data.
2017:03:13 18:46:43	loading preprocessed files.
2017:03:13 18:46:50	the sample size is 97314, the vocab size is 18739
2017:03:13 18:47:02	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV2.TextGANV2/1489430822

2017:03:13 18:47:07	------ do the pretrain ------ 

2017:03:13 18:47:07	pretrain epoch 0
2017:03:13 18:47:19	use DataLoaderChildrenStory to init data.
2017:03:13 18:47:19	loading preprocessed files.
2017:03:13 18:47:26	the sample size is 97314, the vocab size is 18739
2017:03:13 18:47:36	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV1.TextGANV1/1489430856

2017:03:13 18:47:41	------ do the pretrain ------ 

2017:03:13 18:47:41	pretrain epoch 0
2017:03:13 18:47:52	use DataLoaderChildrenStory to init data.
2017:03:13 18:47:52	loading preprocessed files.
2017:03:13 18:47:59	the sample size is 97314, the vocab size is 18739
2017:03:13 18:48:26	use DataLoaderChildrenStory to init data.
2017:03:13 18:48:26	loading preprocessed files.
2017:03:13 18:48:33	the sample size is 97314, the vocab size is 18739
2017:03:13 18:48:39	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489430919

2017:03:13 18:48:43	------ do the pretrain ------ 

2017:03:13 18:48:43	pretrain epoch 0
2017:03:13 22:46:06	use DataLoaderChildrenStory to init data.
2017:03:13 22:46:06	loading preprocessed files.
2017:03:13 22:46:14	the sample size is 97314, the vocab size is 18739
2017:03:13 22:47:29	use DataLoaderChildrenStory to init data.
2017:03:13 22:47:29	loading preprocessed files.
2017:03:13 22:47:36	the sample size is 97314, the vocab size is 18739
2017:03:13 22:48:15	use DataLoaderChildrenStory to init data.
2017:03:13 22:48:15	loading preprocessed files.
2017:03:13 22:48:22	the sample size is 97314, the vocab size is 18739
2017:03:13 22:48:36	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV3.TextGANV3/1489445316

2017:03:13 22:48:42	------ do the pretrain ------ 

2017:03:13 22:48:42	pretrain epoch 0
2017:03:13 22:57:20	use DataLoaderChildrenStory to init data.
2017:03:13 22:57:20	loading preprocessed files.
2017:03:13 22:57:27	the sample size is 97314, the vocab size is 18739
2017:03:13 22:57:33	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489445853

2017:03:13 22:57:37	------ do the pretrain ------ 

2017:03:13 22:57:37	pretrain epoch 0
2017:03:13 23:03:25	use DataLoaderChildrenStory to init data.
2017:03:13 23:03:25	loading preprocessed files.
2017:03:13 23:03:32	the sample size is 97314, the vocab size is 18739
2017:03:13 23:03:44	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV1.TextGANV1/1489446224

2017:03:13 23:03:49	------ do the pretrain ------ 

2017:03:13 23:03:49	pretrain epoch 0
2017:03:13 23:06:38	use DataLoaderChildrenStory to init data.
2017:03:13 23:06:38	loading preprocessed files.
2017:03:13 23:06:46	the sample size is 97314, the vocab size is 18739
2017:03:13 23:06:59	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV2.TextGANV2/1489446419

2017:03:13 23:07:04	------ do the pretrain ------ 

2017:03:13 23:07:04	pretrain epoch 0
2017:03:14 00:19:28	use DataLoaderChildrenStory to init data.
2017:03:14 00:19:28	loading preprocessed files.
2017:03:14 00:19:35	the sample size is 97314, the vocab size is 18739
2017:03:14 00:20:27	use DataLoaderChildrenStory to init data.
2017:03:14 00:20:27	loading preprocessed files.
2017:03:14 00:20:34	the sample size is 97314, the vocab size is 18739
2017:03:14 00:21:26	use DataLoaderChildrenStory to init data.
2017:03:14 00:21:26	loading preprocessed files.
2017:03:14 00:21:33	the sample size is 97314, the vocab size is 18739
2017:03:14 00:22:14	use DataLoaderChildrenStory to init data.
2017:03:14 00:22:14	loading preprocessed files.
2017:03:14 00:22:21	the sample size is 97314, the vocab size is 18739
2017:03:14 00:23:13	use DataLoaderChildrenStory to init data.
2017:03:14 00:23:13	loading preprocessed files.
2017:03:14 00:23:21	the sample size is 97314, the vocab size is 18739
2017:03:14 00:24:32	use DataLoaderChildrenStory to init data.
2017:03:14 00:24:32	loading preprocessed files.
2017:03:14 00:24:39	the sample size is 97314, the vocab size is 18739
2017:03:14 00:24:50	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV4.TextGANV4/1489451090

2017:03:14 00:24:56	------ do the pretrain ------ 

2017:03:14 00:24:57	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV4.TextGANV4/1489451090/checkpoints/bestmodel.

2017:03:14 00:24:57	------ do the standard GAN training ------ 

2017:03:14 00:24:57	train epoch 0
2017:03:14 00:25:58	use DataLoaderChildrenStory to init data.
2017:03:14 00:25:58	loading preprocessed files.
2017:03:14 00:26:05	the sample size is 97314, the vocab size is 18739
2017:03:14 00:26:17	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV4.TextGANV4/1489451177

2017:03:14 00:26:23	------ do the pretrain ------ 

2017:03:14 00:26:24	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV4.TextGANV4/1489451177/checkpoints/bestmodel.

2017:03:14 00:26:24	------ do the standard GAN training ------ 

2017:03:14 00:26:24	train epoch 0
2017:03:14 00:26:44	use DataLoaderChildrenStory to init data.
2017:03:14 00:26:44	loading preprocessed files.
2017:03:14 00:26:53	the sample size is 97314, the vocab size is 18739
2017:03:14 00:27:05	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV3.TextGANV3/1489451225

2017:03:14 00:27:11	------ do the pretrain ------ 

2017:03:14 00:27:13	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGANV3.TextGANV3/1489451225/checkpoints/bestmodel.

2017:03:14 00:27:13	------ do the standard GAN training ------ 

2017:03:14 00:27:13	train epoch 0
2017:03:14 19:17:14	use DataLoaderChildrenStory to init data.
2017:03:14 19:17:14	loading preprocessed files.
2017:03:14 19:17:22	the number of sentence is 97314, the vocab size is 18739
2017:03:14 19:20:35	use DataLoaderChildrenStory to init data.
2017:03:14 19:20:35	loading preprocessed files.
2017:03:14 19:20:43	the number of sentence is 97314, the vocab size is 18739
2017:03:14 19:20:49	writing to /home/tlin/notebooks/code/demo3/data/training/runs/code.model.textGAN.TextGAN/1489519249

2017:03:14 19:20:53	------ do the pretrain ------ 

2017:03:14 19:20:53	pretrain epoch 0
2017:03:14 19:31:59	use DataLoaderChildrenStory to init data.
2017:03:14 19:31:59	loading preprocessed files.
2017:03:14 19:32:06	the number of sentence is 97314, the vocab size is 18739
2017:03:14 19:32:52	use DataLoaderChildrenStory to init data.
2017:03:14 19:32:52	loading preprocessed files.
2017:03:14 19:32:59	the number of sentence is 97314, the vocab size is 18739
2017:03:14 19:33:06	writing to /home/tlin/notebooks/code/demo3/data/training/runs/DataLoaderChildrenStory/code.model.textGAN.TextGAN/1489519985

2017:03:14 19:33:10	------ do the pretrain ------ 

2017:03:14 19:33:10	pretrain epoch 0
2017:03:14 20:21:01	use DataLoaderChildrenStory to init data.
2017:03:14 20:21:01	loading preprocessed files.
2017:03:14 20:21:08	the number of sentence is 97314, the vocab size is 18739
2017:03:14 20:21:15	writing to /home/tlin/notebooks/code/demo3/data/training/runs/DataLoaderChildrenStory/code.model.textGAN.TextGAN/1489522875

2017:03:14 20:21:18	------ do the pretrain ------ 

2017:03:14 20:21:18	pretrain epoch 0
2017:03:15 10:13:19	use DataLoaderChildrenStory to init data.
2017:03:15 10:13:19	loading preprocessed files.
2017:03:15 10:13:26	the number of sentence is 97314, the vocab size is 18739
2017:03:15 10:14:24	use DataLoaderChildrenStory to init data.
2017:03:15 10:14:24	loading preprocessed files.
2017:03:15 10:14:31	the number of sentence is 97314, the vocab size is 18739
2017:03:15 10:16:07	use DataLoaderChildrenStory to init data.
2017:03:15 10:16:07	loading preprocessed files.
2017:03:15 10:16:14	the number of sentence is 97314, the vocab size is 18739
2017:03:15 10:16:41	use DataLoaderChildrenStory to init data.
2017:03:15 10:16:41	loading preprocessed files.
2017:03:15 10:16:48	the number of sentence is 97314, the vocab size is 18739
2017:03:15 10:16:59	use DataLoaderChildrenStory to init data.
2017:03:15 10:16:59	loading preprocessed files.
2017:03:15 10:17:05	the number of sentence is 97314, the vocab size is 18739
2017:03:15 14:28:04	use DataLoaderChildrenStory to init data.
2017:03:15 14:28:04	loading preprocessed files.
2017:03:15 14:28:11	the number of sentence is 97314, the vocab size is 18739
2017:03:15 14:28:23	writing to /home/tlin/notebooks/code/demo3/data/training/runs/DataLoaderChildrenStory/code.model.textGANV1.TextGANV1/1489588103

2017:03:15 14:28:29	------ do the pretrain ------ 

2017:03:15 14:28:29	pretrain epoch 0
2017:03:16 12:44:20	use DataLoaderChildrenStory to init data.
2017:03:16 12:44:20	loading preprocessed files.
2017:03:16 12:44:28	the number of sentence is 97314, the vocab size is 18739
2017:03:16 12:47:45	use DataLoaderChildrenStory to init data.
2017:03:16 12:47:45	loading preprocessed files.
2017:03:16 12:47:52	the number of sentence is 97314, the vocab size is 18739
2017:03:16 12:48:09	use DataLoaderChildrenStory to init data.
2017:03:16 12:48:09	loading preprocessed files.
2017:03:16 12:48:16	the number of sentence is 97314, the vocab size is 18739
2017:03:16 12:49:09	use DataLoaderChildrenStory to init data.
2017:03:16 12:49:09	loading preprocessed files.
2017:03:16 12:49:16	the number of sentence is 97314, the vocab size is 18739
2017:03:16 12:50:06	use DataLoaderChildrenStory to init data.
2017:03:16 12:50:06	loading preprocessed files.
2017:03:16 12:50:13	the number of sentence is 97314, the vocab size is 18739
2017:03:16 12:50:34	use DataLoaderChildrenStory to init data.
2017:03:16 12:50:34	reading and processing the text file.
2017:03:16 12:50:34	preprocess the dataset.
2017:03:16 12:50:38	...mask and pad the sentence.
2017:03:16 12:50:38	......max len:21, median len:10.0, min len:3
2017:03:16 12:50:39	build a vocabulary.
2017:03:16 12:50:39	...flatmap a list of sentence list to a list of sentence.
2017:03:16 12:50:40	...mapping from index to word.
2017:03:16 12:50:40	...mapping from word to index.
2017:03:16 12:50:40	...map word to index.
2017:03:16 12:50:41	...some data statistics.
2017:03:16 12:50:41	...save processed data to file.
2017:03:16 12:50:45	the number of sentence is 97241, the vocab size is 19396
2017:03:16 12:51:36	use DataLoaderChildrenStory to init data.
2017:03:16 12:51:36	loading preprocessed files.
2017:03:16 12:51:44	the number of sentence is 97241, the vocab size is 19396
2017:03:16 12:52:26	use DataLoaderChildrenStory to init data.
2017:03:16 12:52:26	loading preprocessed files.
2017:03:16 12:52:34	the number of sentence is 97241, the vocab size is 19396
2017:03:16 12:52:53	use DataLoaderChildrenStory to init data.
2017:03:16 12:52:53	loading preprocessed files.
2017:03:16 12:53:01	the number of sentence is 97241, the vocab size is 19396
2017:03:16 12:53:43	use DataLoaderChildrenStory to init data.
2017:03:16 12:53:43	loading preprocessed files.
2017:03:16 12:53:50	the number of sentence is 97241, the vocab size is 19396
2017:03:16 12:56:40	use DataLoaderChildrenStory to init data.
2017:03:16 12:56:41	loading preprocessed files.
2017:03:16 12:56:55	the number of sentence is 97241, the vocab size is 19396
2017:03:16 12:59:12	use DataLoaderChildrenStory to init data.
2017:03:16 12:59:12	loading preprocessed files.
2017:03:16 12:59:21	the number of sentence is 97241, the vocab size is 19396
2017:03:16 12:59:44	use DataLoaderChildrenStory to init data.
2017:03:16 12:59:44	loading preprocessed files.
2017:03:16 12:59:51	the number of sentence is 97241, the vocab size is 19396
2017:03:16 12:59:54	writing to /home/tlin/notebooks/code/demo3/data/training/runs/DataLoaderChildrenStory/code.model.textG.TextG/1489669194

2017:03:16 12:59:55	------ do the pretrain ------ 

2017:03:16 12:59:55	train epoch 0
2017:03:16 13:00:11	use DataLoaderChildrenStory to init data.
2017:03:16 13:00:11	loading preprocessed files.
2017:03:16 13:00:18	the number of sentence is 97241, the vocab size is 19396
2017:03:16 13:00:21	writing to /home/tlin/notebooks/code/demo3/data/training/runs/DataLoaderChildrenStory/code.model.textG.TextG/1489669220

2017:03:16 13:00:21	------ do the pretrain ------ 

2017:03:16 13:00:21	train epoch 0
2017:03:16 13:29:15	use DataLoaderChildrenStory to init data.
2017:03:16 13:29:15	reading and processing the text file.
2017:03:16 13:29:15	preprocess the dataset.
2017:03:16 13:29:24	...mask and pad the sentence.
2017:03:16 13:29:24	......max len:21, median len:10.0, min len:3
2017:03:16 13:29:25	build a vocabulary.
2017:03:16 13:29:25	...flatmap a list of sentence list to a list of sentence.
2017:03:16 13:29:27	...mapping from index to word.
2017:03:16 13:29:27	...mapping from word to index.
2017:03:16 13:29:27	...map word to index.
2017:03:16 13:29:29	...some data statistics.
2017:03:16 13:29:29	...save processed data to file.
2017:03:16 13:29:35	the number of sentence is 97241, the vocab size is 19396
2017:03:16 13:29:42	writing to /home/tlin/notebooks/code/demo3/data/training/runs/DataLoaderChildrenStory/code.model.textG.TextG/1489670982

2017:03:16 13:29:45	------ training ------ 

2017:03:16 13:29:45	train epoch 0
2017:03:16 13:32:20	use DataLoaderChildrenStory to init data.
2017:03:16 13:32:20	loading preprocessed files.
2017:03:16 13:32:30	the number of sentence is 97241, the vocab size is 19396
2017:03:16 13:32:35	writing to /home/tlin/notebooks/code/demo3/data/training/runs/DataLoaderChildrenStory/code.model.textG.TextG/1489671155

2017:03:16 13:32:36	------ training ------ 

2017:03:16 13:32:36	train epoch 0
2017:03:16 13:33:16	use DataLoaderChildrenStory to init data.
2017:03:16 13:33:17	loading preprocessed files.
2017:03:16 13:33:25	the number of sentence is 97241, the vocab size is 19396
2017:03:16 13:33:29	writing to /home/tlin/notebooks/code/demo3/data/training/runs/DataLoaderChildrenStory/code.model.textG.TextG/1489671209

2017:03:16 13:33:31	------ training ------ 

2017:03:16 13:33:31	train epoch 0
2017:03:16 13:33:50	train loss: 9.8720369339, execution speed: 4.75 seconds/batch

2017:03:16 13:34:28	use DataLoaderChildrenStory to init data.
2017:03:16 13:34:28	loading preprocessed files.
2017:03:16 13:34:35	the number of sentence is 97241, the vocab size is 19396
2017:03:16 13:34:39	writing to /home/tlin/notebooks/code/demo3/data/training/runs/DataLoaderChildrenStory/code.model.textG.TextG/1489671279

2017:03:16 13:34:41	------ training ------ 

2017:03:16 13:34:41	train epoch 0
2017:03:16 13:35:01	train loss: 9.87321853638, execution speed: 5.00 seconds/batch

2017:03:16 13:36:03	use DataLoaderChildrenStory to init data.
2017:03:16 13:36:03	loading preprocessed files.
2017:03:16 13:36:11	the number of sentence is 97241, the vocab size is 19396
2017:03:16 13:36:14	writing to /home/tlin/notebooks/code/demo3/data/training/runs/DataLoaderChildrenStory/code.model.textG.TextG/1489671374

2017:03:16 13:36:16	------ training ------ 

2017:03:16 13:36:16	train epoch 0
2017:03:16 13:36:34	train loss: 9.87308216095, execution speed: 4.50 seconds/batch

2017:03:16 13:36:40	val loss: 9.87268066406, execution speed: 1.50 seconds/batch

2017:03:16 16:32:36	use DataLoaderShakespeare to init data.
2017:03:16 16:32:36	reading and processing the text file.
2017:03:16 16:32:36	preprocess the dataset.
2017:03:16 16:33:24	use DataLoaderShakespeare to init data.
2017:03:16 16:33:24	reading and processing the text file.
2017:03:16 16:33:24	preprocess the dataset.
2017:03:16 16:38:51	use DataLoaderShakespeare to init data.
2017:03:16 16:38:51	reading and processing the text file.
2017:03:16 16:38:51	preprocess the dataset.
2017:03:16 16:39:06	build a vocabulary.
2017:03:16 16:39:06	...flatmap a list of sentence list to a list of sentence.
2017:03:16 16:39:06	...mapping from index to word.
2017:03:16 16:39:06	...mapping from word to index.
2017:03:16 16:39:06	...map word to index.
2017:03:16 16:39:06	...some data statistics.
2017:03:16 16:46:11	use DataLoaderShakespeare to init data.
2017:03:16 16:46:11	reading and processing the text file.
2017:03:16 16:46:11	preprocess the dataset.
2017:03:16 16:46:27	build a vocabulary.
2017:03:16 16:46:27	...flatmap a list of sentence list to a list of sentence.
2017:03:16 16:46:27	...mapping from index to word.
2017:03:16 16:46:27	...mapping from word to index.
2017:03:16 16:46:27	...map word to index.
2017:03:16 16:46:27	...some data statistics.
2017:03:16 16:46:27	...save processed data to file.
2017:03:16 16:48:27	use DataLoaderShakespeare to init data.
2017:03:16 16:48:27	reading and processing the text file.
2017:03:16 16:48:27	preprocess the dataset.
2017:03:16 16:48:41	build a vocabulary.
2017:03:16 16:48:41	...flatmap a list of sentence list to a list of sentence.
2017:03:16 16:48:42	...mapping from index to word.
2017:03:16 16:48:42	...mapping from word to index.
2017:03:16 16:48:42	...map word to index.
2017:03:16 16:48:42	...some data statistics.
2017:03:16 16:48:42	...save processed data to file.
2017:03:16 16:49:13	use DataLoaderShakespeare to init data.
2017:03:16 16:49:13	reading and processing the text file.
2017:03:16 16:49:13	preprocess the dataset.
2017:03:16 16:49:28	build a vocabulary.
2017:03:16 16:49:28	...flatmap a list of sentence list to a list of sentence.
2017:03:16 16:49:28	...mapping from index to word.
2017:03:16 16:49:28	...mapping from word to index.
2017:03:16 16:49:28	...map word to index.
2017:03:16 16:49:28	...some data statistics.
2017:03:16 16:49:28	...save processed data to file.
2017:03:16 16:50:43	use DataLoaderShakespeare to init data.
2017:03:16 16:50:43	reading and processing the text file.
2017:03:16 16:50:43	preprocess the dataset.
2017:03:16 16:50:57	build a vocabulary.
2017:03:16 16:50:57	...flatmap a list of sentence list to a list of sentence.
2017:03:16 16:50:58	...mapping from index to word.
2017:03:16 16:50:58	...mapping from word to index.
2017:03:16 16:50:58	...map word to index.
2017:03:16 16:50:58	...some data statistics.
2017:03:16 16:50:58	...save processed data to file.
2017:03:16 16:52:54	use DataLoaderShakespeare to init data.
2017:03:16 16:52:54	reading and processing the text file.
2017:03:16 16:52:54	preprocess the dataset.
2017:03:16 16:53:08	build a vocabulary.
2017:03:16 16:53:08	...flatmap a list of sentence list to a list of sentence.
2017:03:16 16:53:08	...mapping from index to word.
2017:03:16 16:53:08	...mapping from word to index.
2017:03:16 16:53:08	...map word to index.
2017:03:16 16:53:08	...some data statistics.
2017:03:16 16:53:08	...save processed data to file.
2017:03:16 16:53:08	the number of sentence is 243200, the vocab size is 38
2017:03:16 16:53:51	use DataLoaderShakespeare to init data.
2017:03:16 16:53:52	reading and processing the text file.
2017:03:16 16:53:52	preprocess the dataset.
2017:03:16 16:54:08	build a vocabulary.
2017:03:16 16:54:08	...flatmap a list of sentence list to a list of sentence.
2017:03:16 16:54:09	...mapping from index to word.
2017:03:16 16:54:09	...mapping from word to index.
2017:03:16 16:54:09	...map word to index.
2017:03:16 16:54:09	...some data statistics.
2017:03:16 16:54:09	...save processed data to file.
2017:03:16 16:54:09	the number of sentence is 243200, the vocab size is 38
2017:03:16 16:54:13	writing to /home/tlin/notebooks/code/demo2/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489683252

2017:03:16 16:54:13	------ training ------ 

2017:03:16 16:54:13	train epoch 0
2017:03:16 16:56:00	use DataLoaderShakespeare to init data.
2017:03:16 16:56:00	loading preprocessed files.
2017:03:16 16:56:00	the number of sentence is 243200, the vocab size is 38
2017:03:16 16:56:03	writing to /home/tlin/notebooks/code/demo2/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489683362

2017:03:16 16:56:03	------ training ------ 

2017:03:16 16:56:03	train epoch 0
2017:03:16 16:58:23	use DataLoaderShakespeare to init data.
2017:03:16 16:58:23	loading preprocessed files.
2017:03:16 16:58:23	the number of sentence is 243200, the vocab size is 38
2017:03:16 17:01:47	use DataLoaderShakespeare to init data.
2017:03:16 17:01:47	loading preprocessed files.
2017:03:16 17:01:48	the number of sentence is 243200, the vocab size is 38
2017:03:16 17:02:00	use DataLoaderShakespeare to init data.
2017:03:16 17:02:00	loading preprocessed files.
2017:03:16 17:02:01	the number of sentence is 243200, the vocab size is 38
2017:03:16 17:02:16	use DataLoaderShakespeare to init data.
2017:03:16 17:02:16	loading preprocessed files.
2017:03:16 17:02:16	the number of sentence is 243200, the vocab size is 38
2017:03:16 17:02:19	writing to /home/tlin/notebooks/code/demo2/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489683738

2017:03:16 17:02:20	------ training ------ 

2017:03:16 17:02:20	train epoch 0
2017:03:16 17:04:50	use DataLoaderShakespeare to init data.
2017:03:16 17:04:50	loading preprocessed files.
2017:03:16 17:04:51	the number of sentence is 243200, the vocab size is 38
2017:03:16 17:04:52	writing to /home/tlin/notebooks/code/demo2/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489683892

2017:03:16 17:04:53	------ training ------ 

2017:03:16 17:04:53	train epoch 0
2017:03:16 17:17:46	use DataLoaderShakespeare to init data.
2017:03:16 17:17:46	loading preprocessed files.
2017:03:16 17:17:46	the number of sentence is 243200, the vocab size is 38
2017:03:16 17:17:48	writing to /home/tlin/notebooks/code/demo2/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489684667

2017:03:16 17:17:48	------ training ------ 

2017:03:16 17:17:48	train epoch 0
2017:03:16 17:18:40	use DataLoaderShakespeare to init data.
2017:03:16 17:18:40	loading preprocessed files.
2017:03:16 17:18:40	the number of sentence is 243200, the vocab size is 38
2017:03:16 17:18:42	writing to /home/tlin/notebooks/code/demo2/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489684722

2017:03:16 17:18:43	------ training ------ 

2017:03:16 17:18:43	train epoch 0
2017:03:16 17:20:44	use DataLoaderShakespeare to init data.
2017:03:16 17:20:44	loading preprocessed files.
2017:03:16 17:20:45	the number of sentence is 243200, the vocab size is 38
2017:03:16 17:20:47	writing to /home/tlin/notebooks/code/demo2/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489684846

2017:03:16 17:20:48	------ training ------ 

2017:03:16 17:20:48	train epoch 0
2017:03:16 17:21:04	use DataLoaderShakespeare to init data.
2017:03:16 17:21:04	reading and processing the text file.
2017:03:16 17:21:04	preprocess the dataset.
2017:03:16 17:21:18	build a vocabulary.
2017:03:16 17:21:18	...flatmap a list of sentence list to a list of sentence.
2017:03:16 17:21:18	...mapping from index to word.
2017:03:16 17:21:18	...mapping from word to index.
2017:03:16 17:21:18	...map word to index.
2017:03:16 17:21:18	...some data statistics.
2017:03:16 17:21:18	...save processed data to file.
2017:03:16 17:21:18	the number of sentence is 243200, the vocab size is 38
2017:03:16 17:21:20	writing to /home/tlin/notebooks/code/demo2/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489684880

2017:03:16 17:21:21	------ training ------ 

2017:03:16 17:21:21	train epoch 0
2017:03:16 17:21:47	use DataLoaderShakespeare to init data.
2017:03:16 17:21:47	reading and processing the text file.
2017:03:16 17:21:47	preprocess the dataset.
2017:03:16 17:22:01	build a vocabulary.
2017:03:16 17:22:01	...flatmap a list of sentence list to a list of sentence.
2017:03:16 17:22:01	...mapping from index to word.
2017:03:16 17:22:01	...mapping from word to index.
2017:03:16 17:22:01	...map word to index.
2017:03:16 17:22:01	...some data statistics.
2017:03:16 17:22:01	...save processed data to file.
2017:03:16 17:22:01	the number of sentence is 243200, the vocab size is 38
2017:03:16 17:22:03	writing to /home/tlin/notebooks/code/demo2/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489684923

2017:03:16 17:22:04	------ training ------ 

2017:03:16 17:22:04	train epoch 0
2017:03:16 17:22:37	use DataLoaderShakespeare to init data.
2017:03:16 17:22:37	reading and processing the text file.
2017:03:16 17:22:37	preprocess the dataset.
2017:03:16 17:22:53	build a vocabulary.
2017:03:16 17:22:53	...flatmap a list of sentence list to a list of sentence.
2017:03:16 17:22:53	...mapping from index to word.
2017:03:16 17:22:53	...mapping from word to index.
2017:03:16 17:22:53	...map word to index.
2017:03:16 17:22:53	...some data statistics.
2017:03:16 17:22:53	...save processed data to file.
2017:03:16 17:22:53	the number of sentence is 243200, the vocab size is 14004
2017:03:16 17:22:55	writing to /home/tlin/notebooks/code/demo2/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489684975

2017:03:16 17:22:56	------ training ------ 

2017:03:16 17:22:56	train epoch 0
2017:03:16 17:23:54	use DataLoaderShakespeare to init data.
2017:03:16 17:23:54	reading and processing the text file.
2017:03:16 17:23:54	preprocess the dataset.
2017:03:16 17:24:08	build a vocabulary.
2017:03:16 17:24:08	...flatmap a list of sentence list to a list of sentence.
2017:03:16 17:24:08	...mapping from index to word.
2017:03:16 17:24:08	...mapping from word to index.
2017:03:16 17:24:08	...map word to index.
2017:03:16 17:24:08	...some data statistics.
2017:03:16 17:24:08	...save processed data to file.
2017:03:16 17:25:02	use DataLoaderShakespeare to init data.
2017:03:16 17:25:02	reading and processing the text file.
2017:03:16 17:25:02	preprocess the dataset.
2017:03:16 17:25:16	build a vocabulary.
2017:03:16 17:25:16	...flatmap a list of sentence list to a list of sentence.
2017:03:16 17:25:16	...mapping from index to word.
2017:03:16 17:25:16	...mapping from word to index.
2017:03:16 17:25:16	...map word to index.
2017:03:16 17:25:16	...some data statistics.
2017:03:16 17:25:16	...save processed data to file.
2017:03:16 17:25:30	use DataLoaderShakespeare to init data.
2017:03:16 17:25:30	loading preprocessed files.
2017:03:16 17:27:08	use DataLoaderShakespeare to init data.
2017:03:16 17:27:08	loading preprocessed files.
2017:03:16 17:27:31	use DataLoaderShakespeare to init data.
2017:03:16 17:27:31	loading preprocessed files.
2017:03:16 17:27:53	use DataLoaderShakespeare to init data.
2017:03:16 17:27:53	loading preprocessed files.
2017:03:16 17:28:02	use DataLoaderShakespeare to init data.
2017:03:16 17:28:02	reading and processing the text file.
2017:03:16 17:28:02	preprocess the dataset.
2017:03:16 17:28:15	build a vocabulary.
2017:03:16 17:28:15	...flatmap a list of sentence list to a list of sentence.
2017:03:16 17:28:15	...mapping from index to word.
2017:03:16 17:28:15	...mapping from word to index.
2017:03:16 17:28:15	...map word to index.
2017:03:16 17:28:15	...some data statistics.
2017:03:16 17:28:46	use DataLoaderShakespeare to init data.
2017:03:16 17:28:46	reading and processing the text file.
2017:03:16 17:28:46	preprocess the dataset.
2017:03:16 17:29:00	build a vocabulary.
2017:03:16 17:29:00	...flatmap a list of sentence list to a list of sentence.
2017:03:16 17:29:00	...mapping from index to word.
2017:03:16 17:29:00	...mapping from word to index.
2017:03:16 17:29:00	...map word to index.
2017:03:16 17:29:00	...some data statistics.
2017:03:16 17:29:00	......existing 243264 words, vocabulary size is 14004
2017:03:16 17:29:00	...save processed data to file.
2017:03:16 17:29:01	the number of sentence is 243200, the vocab size is 14004
2017:03:16 17:29:03	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489685342

2017:03:16 17:29:03	------ training ------ 

2017:03:16 17:29:03	train epoch 0
2017:03:16 17:30:28	use DataLoaderShakespeare to init data.
2017:03:16 17:30:28	reading and processing the text file.
2017:03:16 17:30:28	preprocess the dataset.
2017:03:16 17:30:41	build a vocabulary.
2017:03:16 17:30:41	...flatmap a list of sentence list to a list of sentence.
2017:03:16 17:30:41	...mapping from index to word.
2017:03:16 17:30:41	...mapping from word to index.
2017:03:16 17:30:41	...map word to index.
2017:03:16 17:30:41	...some data statistics.
2017:03:16 17:30:41	......existing 243264 words, vocabulary size is 14004
2017:03:16 17:30:41	...save processed data to file.
2017:03:16 17:30:41	...number of batches: 0
2017:03:16 17:31:12	use DataLoaderShakespeare to init data.
2017:03:16 17:31:12	reading and processing the text file.
2017:03:16 17:31:12	preprocess the dataset.
2017:03:16 17:31:25	build a vocabulary.
2017:03:16 17:31:25	...flatmap a list of sentence list to a list of sentence.
2017:03:16 17:31:25	...mapping from index to word.
2017:03:16 17:31:25	...mapping from word to index.
2017:03:16 17:31:25	...map word to index.
2017:03:16 17:31:25	...some data statistics.
2017:03:16 17:31:25	......existing 243264 words, vocabulary size is 14004
2017:03:16 17:31:25	...save processed data to file.
2017:03:16 17:31:26	...number of batches: 8
2017:03:16 17:31:26	the number of sentence is 10240, the vocab size is 14004
2017:03:16 17:31:27	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489685487

2017:03:16 17:31:28	------ training ------ 

2017:03:16 17:31:28	train epoch 0
2017:03:16 17:31:33	train loss: 9.5472984314, execution speed: 0.50 seconds/batch

2017:03:16 17:31:34	val loss: 9.54658508301, execution speed: 0.12 seconds/batch

2017:03:16 17:31:35	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489685487/checkpoints/bestmodel.

2017:03:16 17:31:35	train epoch 1
2017:03:16 17:33:03	use DataLoaderShakespeare to init data.
2017:03:16 17:33:03	reading and processing the text file.
2017:03:16 17:33:03	preprocess the dataset.
2017:03:16 17:33:17	build a vocabulary.
2017:03:16 17:33:17	...flatmap a list of sentence list to a list of sentence.
2017:03:16 17:33:17	...mapping from index to word.
2017:03:16 17:33:17	...mapping from word to index.
2017:03:16 17:33:17	...map word to index.
2017:03:16 17:33:17	...some data statistics.
2017:03:16 17:33:17	......existing 243264 words, vocabulary size is 14004
2017:03:16 17:33:17	...save processed data to file.
2017:03:16 17:33:17	...number of batches: 8
2017:03:16 17:33:17	the number of sentence is 10240, the vocab size is 14004
2017:03:16 17:33:19	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489685599

2017:03:16 17:33:20	------ training ------ 

2017:03:16 17:33:20	train epoch 0
2017:03:16 17:33:24	train loss: 9.54664516449, execution speed: 0.50 seconds/batch

2017:03:16 17:33:26	val loss: 9.54594230652, execution speed: 0.12 seconds/batch

2017:03:16 17:33:26	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489685599/checkpoints/bestmodel.

2017:03:16 17:33:26	train epoch 1
2017:03:16 17:33:30	train loss: 9.54544448853, execution speed: 0.50 seconds/batch

2017:03:16 17:33:32	val loss: 9.54474544525, execution speed: 0.12 seconds/batch

2017:03:16 17:33:32	train epoch 2
2017:03:16 19:35:34	use DataLoaderBBC to init data.
2017:03:16 19:36:07	use DataLoaderBBC to init data.
2017:03:16 19:36:07	reading and processing the text file.
2017:03:16 19:36:07	preprocess the dataset.
2017:03:16 19:36:07	...load data.
2017:03:16 19:36:07	load context for further preprocessing.
2017:03:16 19:36:28	use DataLoaderBBC to init data.
2017:03:16 19:36:28	reading and processing the text file.
2017:03:16 19:36:28	preprocess the dataset.
2017:03:16 19:36:28	...load data.
2017:03:16 19:36:28	load context for further preprocessing.
2017:03:16 19:37:39	build a vocabulary.
2017:03:16 19:37:39	...flatmap a list of sentence list to a list of sentence.
2017:03:16 19:37:39	...mapping from index to word.
2017:03:16 19:37:39	...mapping from word to index.
2017:03:16 19:37:39	...map word to index.
2017:03:16 19:37:40	...some data statistics.
2017:03:16 19:37:40	......existing 954949 words, vocabulary size is 34571
2017:03:16 19:37:40	...save processed data to file.
2017:03:16 19:37:41	...number of batches: 8
2017:03:16 19:37:41	the number of sentence is 10240, the vocab size is 34571
2017:03:16 19:37:43	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489693063

2017:03:16 19:37:44	------ training ------ 

2017:03:16 19:37:44	train epoch 0
2017:03:16 19:38:00	train loss: 10.4502439499, execution speed: 2.00 seconds/batch

2017:03:16 19:38:06	val loss: 10.4495401382, execution speed: 0.62 seconds/batch

2017:03:16 19:38:06	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489693063/checkpoints/bestmodel.

2017:03:16 19:38:06	train epoch 1
2017:03:16 19:38:20	train loss: 10.4490385056, execution speed: 1.75 seconds/batch

2017:03:16 19:39:10	use DataLoaderBBC to init data.
2017:03:16 19:39:10	reading and processing the text file.
2017:03:16 19:39:10	preprocess the dataset.
2017:03:16 19:39:10	...load data.
2017:03:16 19:39:10	load context for further preprocessing.
2017:03:16 19:40:13	build a vocabulary.
2017:03:16 19:40:13	...flatmap a list of sentence list to a list of sentence.
2017:03:16 19:40:14	...mapping from index to word.
2017:03:16 19:40:14	...mapping from word to index.
2017:03:16 19:40:14	...map word to index.
2017:03:16 19:40:14	...some data statistics.
2017:03:16 19:40:14	......existing 954949 words, vocabulary size is 34571
2017:03:16 19:40:14	...save processed data to file.
2017:03:16 19:40:15	...number of batches: 8
2017:03:16 19:40:15	the number of sentence is 10240, the vocab size is 34571
2017:03:16 19:40:18	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489693217

2017:03:16 19:40:19	------ training ------ 

2017:03:16 19:40:19	train epoch 0
2017:03:16 19:40:51	use DataLoaderBBC to init data.
2017:03:16 19:40:51	reading and processing the text file.
2017:03:16 19:40:51	preprocess the dataset.
2017:03:16 19:40:51	...load data.
2017:03:16 19:40:51	init content from raw.
2017:03:16 19:40:51	init data from the raw dataset.
2017:03:16 19:42:18	use DataLoaderBBC to init data.
2017:03:16 19:42:18	reading and processing the text file.
2017:03:16 19:42:18	preprocess the dataset.
2017:03:16 19:42:18	...load data.
2017:03:16 19:42:18	init content from raw.
2017:03:16 19:42:18	init data from the raw dataset.
2017:03:16 19:43:15	use DataLoaderBBC to init data.
2017:03:16 19:43:15	reading and processing the text file.
2017:03:16 19:43:15	preprocess the dataset.
2017:03:16 19:43:15	...load data.
2017:03:16 19:43:15	init content from raw.
2017:03:16 19:43:15	init data from the raw dataset.
2017:03:16 19:43:44	use DataLoaderBBC to init data.
2017:03:16 19:43:44	reading and processing the text file.
2017:03:16 19:43:44	preprocess the dataset.
2017:03:16 19:43:44	...load data.
2017:03:16 19:43:44	init content from raw.
2017:03:16 19:43:44	init data from the raw dataset.
2017:03:16 19:43:46	clean data.
2017:03:16 19:44:08	use DataLoaderBBC to init data.
2017:03:16 19:44:08	reading and processing the text file.
2017:03:16 19:44:08	preprocess the dataset.
2017:03:16 19:44:08	load data.
2017:03:16 19:44:08	init content from raw.
2017:03:16 19:44:08	init data from the raw dataset.
2017:03:16 19:44:11	clean data.
2017:03:16 19:44:50	use DataLoaderBBC to init data.
2017:03:16 19:44:50	reading and processing the text file.
2017:03:16 19:44:50	preprocess the dataset.
2017:03:16 19:44:50	load data.
2017:03:16 19:44:50	init content from raw.
2017:03:16 19:44:50	init data from the raw dataset.
2017:03:16 19:45:36	use DataLoaderBBC to init data.
2017:03:16 19:45:36	reading and processing the text file.
2017:03:16 19:45:36	preprocess the dataset.
2017:03:16 19:45:36	load data.
2017:03:16 19:45:36	init content from raw.
2017:03:16 19:45:36	init data from the raw dataset.
2017:03:16 19:46:20	use DataLoaderBBC to init data.
2017:03:16 19:46:20	reading and processing the text file.
2017:03:16 19:46:20	preprocess the dataset.
2017:03:16 19:46:20	load data.
2017:03:16 19:46:20	init content from raw.
2017:03:16 19:46:20	init data from the raw dataset.
2017:03:16 19:46:21	load context for further preprocessing.
2017:03:16 19:46:21	clean data.
2017:03:16 19:46:22	build a vocabulary.
2017:03:16 19:46:22	...flatmap a list of sentence list to a list of sentence.
2017:03:16 19:46:22	...mapping from index to word.
2017:03:16 19:46:22	...mapping from word to index.
2017:03:16 19:47:30	...map word to index.
2017:03:16 19:47:30	...some data statistics.
2017:03:16 19:47:30	......existing 954949 words, vocabulary size is 34571
2017:03:16 19:47:30	...save processed data to file.
2017:03:16 19:47:32	...number of batches: 8
2017:03:16 19:47:32	the number of sentence is 10240, the vocab size is 34571
2017:03:16 19:47:34	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489693654

2017:03:16 19:47:36	------ training ------ 

2017:03:16 19:47:36	train epoch 0
2017:03:16 19:47:52	train loss: 10.4507255554, execution speed: 2.00 seconds/batch

2017:03:16 19:47:57	val loss: 10.4500274658, execution speed: 0.50 seconds/batch

2017:03:16 19:47:57	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489693654/checkpoints/bestmodel.

2017:03:16 19:47:57	train epoch 1
2017:03:16 19:48:12	train loss: 10.4495229721, execution speed: 1.75 seconds/batch

2017:03:16 19:48:17	val loss: 10.4488182068, execution speed: 0.50 seconds/batch

2017:03:16 19:48:17	train epoch 2
2017:03:16 19:57:37	use DataLoaderBBC to init data.
2017:03:16 19:57:37	reading and processing the text file.
2017:03:16 19:57:37	preprocess the dataset.
2017:03:16 19:57:37	load data.
2017:03:16 19:57:37	init content from raw.
2017:03:16 19:57:37	init data from the raw dataset.
2017:03:16 19:57:38	load context for further preprocessing.
2017:03:16 19:57:38	clean data.
2017:03:16 19:57:39	build a vocabulary.
2017:03:16 19:57:39	...flatmap a list of sentence list to a list of sentence.
2017:03:16 19:57:39	...mapping from index to word.
2017:03:16 19:57:39	...mapping from word to index.
2017:03:16 19:58:45	...map word to index.
2017:03:16 19:58:45	...some data statistics.
2017:03:16 19:58:45	......existing 954949 words, vocabulary size is 34571
2017:03:16 19:58:45	...save processed data to file.
2017:03:16 19:58:46	...number of batches: 8
2017:03:16 19:58:46	the number of sentence is 10240, the vocab size is 34571
2017:03:16 19:58:48	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489694328

2017:03:16 19:58:49	------ training ------ 

2017:03:16 19:58:49	train epoch 0
2017:03:16 20:00:25	use DataLoaderBBC to init data.
2017:03:16 20:00:25	reading and processing the text file.
2017:03:16 20:00:25	preprocess the dataset.
2017:03:16 20:00:25	load data.
2017:03:16 20:00:25	init content from raw.
2017:03:16 20:00:25	init data from the raw dataset.
2017:03:16 20:00:26	load context for further preprocessing.
2017:03:16 20:00:26	clean data.
2017:03:16 20:00:27	build a vocabulary.
2017:03:16 20:00:27	...flatmap a list of sentence list to a list of sentence.
2017:03:16 20:00:27	...mapping from index to word.
2017:03:16 20:00:27	...mapping from word to index.
2017:03:16 20:01:32	...map word to index.
2017:03:16 20:01:33	...some data statistics.
2017:03:16 20:01:33	......existing 954949 words, vocabulary size is 34571
2017:03:16 20:01:33	...save processed data to file.
2017:03:16 20:01:34	...number of batches: 8
2017:03:16 20:01:34	the number of sentence is 10240, the vocab size is 34571
2017:03:16 20:01:36	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489694496

2017:03:16 20:01:37	------ training ------ 

2017:03:16 20:01:37	train epoch 0
2017:03:16 20:01:52	train loss: 10.4500589371, execution speed: 1.88 seconds/batch

2017:03:16 20:01:57	val loss: 10.4493560791, execution speed: 0.50 seconds/batch

2017:03:16 20:01:57	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489694496/checkpoints/bestmodel.

2017:03:16 20:01:57	train epoch 1
2017:03:16 20:02:22	use DataLoaderBBC to init data.
2017:03:16 20:02:22	loading preprocessed files.
2017:03:16 20:02:24	......existing 954949 words, vocabulary size is 34571
2017:03:16 20:02:24	...number of batches: 8
2017:03:16 20:02:24	the number of sentence is 10240, the vocab size is 34571
2017:03:16 20:02:26	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489694546

2017:03:16 20:02:27	------ training ------ 

2017:03:16 20:02:27	train epoch 0
2017:03:16 20:03:16	use DataLoaderBBC to init data.
2017:03:16 20:03:16	loading preprocessed files.
2017:03:16 20:03:18	......existing 954949 words, vocabulary size is 34571
2017:03:16 20:03:18	...number of batches: 8
2017:03:16 20:03:18	the number of sentence is 10240, the vocab size is 34571
2017:03:16 20:03:19	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489694599

2017:03:16 20:03:33	use DataLoaderBBC to init data.
2017:03:16 20:03:33	loading preprocessed files.
2017:03:16 20:03:35	......existing 954949 words, vocabulary size is 34571
2017:03:16 20:03:35	...number of batches: 8
2017:03:16 20:03:35	the number of sentence is 10240, the vocab size is 34571
2017:03:16 20:03:36	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489694616

2017:03:16 20:05:39	use DataLoaderBBC to init data.
2017:03:16 20:05:39	reading and processing the text file.
2017:03:16 20:05:40	preprocess the dataset.
2017:03:16 20:05:40	load data.
2017:03:16 20:05:40	init content from raw.
2017:03:16 20:05:40	init data from the raw dataset.
2017:03:16 20:05:41	load context for further preprocessing.
2017:03:16 20:05:41	clean data.
2017:03:16 20:05:42	build a vocabulary.
2017:03:16 20:05:42	...flatmap a list of sentence list to a list of sentence.
2017:03:16 20:05:42	...mapping from index to word.
2017:03:16 20:05:42	...mapping from word to index.
2017:03:16 20:06:31	use DataLoaderBBC to init data.
2017:03:16 20:06:31	reading and processing the text file.
2017:03:16 20:06:31	preprocess the dataset.
2017:03:16 20:06:31	load data.
2017:03:16 20:06:31	init content from raw.
2017:03:16 20:06:31	init data from the raw dataset.
2017:03:16 20:06:32	load context for further preprocessing.
2017:03:16 20:06:32	clean data.
2017:03:16 20:06:33	build a vocabulary.
2017:03:16 20:06:33	...flatmap a list of sentence list to a list of sentence.
2017:03:16 20:06:33	...mapping from index to word.
2017:03:16 20:06:33	...mapping from word to index.
2017:03:16 20:06:33	...map word to index.
2017:03:16 20:06:34	...some data statistics.
2017:03:16 20:06:34	......existing 954949 words, vocabulary size is 34571
2017:03:16 20:06:34	...save processed data to file.
2017:03:16 20:06:35	...number of batches: 8
2017:03:16 20:06:35	the number of sentence is 10240, the vocab size is 34571
2017:03:16 20:06:39	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489694799

2017:03:16 20:06:41	------ training ------ 

2017:03:16 20:06:41	train epoch 0
2017:03:16 20:08:55	use DataLoaderShakespeare to init data.
2017:03:16 20:08:55	reading and processing the text file.
2017:03:16 20:08:55	preprocess the dataset.
2017:03:16 20:09:10	build a vocabulary.
2017:03:16 20:09:10	...flatmap a list of sentence list to a list of sentence.
2017:03:16 20:09:10	...mapping from index to word.
2017:03:16 20:09:10	...mapping from word to index.
2017:03:16 20:09:10	...map word to index.
2017:03:16 20:09:10	...some data statistics.
2017:03:16 20:09:10	......existing 243264 words, vocabulary size is 14004
2017:03:16 20:09:10	...save processed data to file.
2017:03:16 20:09:10	...number of batches: 8
2017:03:16 20:09:10	the number of sentence is 10240, the vocab size is 14004
2017:03:16 20:09:12	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489694952

2017:03:16 20:09:13	------ training ------ 

2017:03:16 20:09:13	train epoch 0
2017:03:16 20:09:18	train loss: 9.54503154755, execution speed: 0.62 seconds/batch

2017:03:16 20:09:20	val loss: 9.54432678223, execution speed: 0.12 seconds/batch

2017:03:16 20:09:20	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489694952/checkpoints/bestmodel.

2017:03:16 20:09:20	train epoch 1
2017:03:16 20:09:25	train loss: 9.54382419586, execution speed: 0.50 seconds/batch

2017:03:16 20:09:26	val loss: 9.54311561584, execution speed: 0.12 seconds/batch

2017:03:16 20:09:26	train epoch 2
2017:03:16 20:09:31	train loss: 9.54260635376, execution speed: 0.50 seconds/batch

2017:03:16 20:09:33	val loss: 9.54187488556, execution speed: 0.12 seconds/batch

2017:03:16 20:09:33	train epoch 3
2017:03:16 20:09:38	train loss: 9.54135036469, execution speed: 0.50 seconds/batch

2017:03:16 20:09:39	val loss: 9.54060173035, execution speed: 0.12 seconds/batch

2017:03:16 20:09:39	train epoch 4
2017:03:16 20:09:44	train loss: 9.54005050659, execution speed: 0.50 seconds/batch

2017:03:16 20:09:45	val loss: 9.5392742157, execution speed: 0.12 seconds/batch

2017:03:16 20:09:45	train epoch 5
2017:03:16 20:09:50	train loss: 9.53869533539, execution speed: 0.50 seconds/batch

2017:03:16 20:09:52	val loss: 9.53787326813, execution speed: 0.12 seconds/batch

2017:03:16 20:09:52	save 2-th bestmodel to path: /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489694952/checkpoints/bestmodel.

2017:03:16 20:09:52	train epoch 6
2017:03:16 20:09:56	train loss: 9.53725242615, execution speed: 0.50 seconds/batch

2017:03:16 20:09:58	val loss: 9.53635597229, execution speed: 0.12 seconds/batch

2017:03:16 20:09:58	train epoch 7
2017:03:16 20:10:03	train loss: 9.53569316864, execution speed: 0.50 seconds/batch

2017:03:16 20:10:04	val loss: 9.53472518921, execution speed: 0.12 seconds/batch

2017:03:16 20:10:04	train epoch 8
2017:03:16 20:10:09	train loss: 9.53400802612, execution speed: 0.50 seconds/batch

2017:03:16 20:10:10	val loss: 9.53296089172, execution speed: 0.12 seconds/batch

2017:03:16 20:10:10	train epoch 9
2017:03:16 20:10:15	train loss: 9.53215312958, execution speed: 0.50 seconds/batch

2017:03:16 20:10:16	val loss: 9.5309753418, execution speed: 0.12 seconds/batch

2017:03:16 20:10:16	train epoch 10
2017:03:16 20:10:21	train loss: 9.53011798859, execution speed: 0.62 seconds/batch

2017:03:16 20:10:23	val loss: 9.52880477905, execution speed: 0.12 seconds/batch

2017:03:16 20:10:24	save 3-th bestmodel to path: /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489694952/checkpoints/bestmodel.

2017:03:16 20:10:24	train epoch 11
2017:03:16 21:17:40	use DataLoaderBBC to init data.
2017:03:16 21:17:40	reading and processing the text file.
2017:03:16 21:17:40	preprocess the dataset.
2017:03:16 21:17:40	load data.
2017:03:16 21:17:40	init content from raw.
2017:03:16 21:17:40	init data from the raw dataset.
2017:03:16 21:17:41	load context for further preprocessing.
2017:03:16 21:17:41	clean data.
2017:03:16 21:17:42	build a vocabulary.
2017:03:16 21:17:42	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:17:42	...mapping from index to word.
2017:03:16 21:17:42	...mapping from word to index.
2017:03:16 21:17:42	...map word to index.
2017:03:16 21:17:42	...some data statistics.
2017:03:16 21:17:42	......existing 954949 words, vocabulary size is 34571
2017:03:16 21:17:42	...save processed data to file.
2017:03:16 21:17:44	...number of batches: 8
2017:03:16 21:17:44	the number of sentence is 10240, the vocab size is 34571
2017:03:16 21:17:46	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489699065

2017:03:16 21:17:47	------ training ------ 

2017:03:16 21:17:47	train epoch 0
2017:03:16 21:18:11	use DataLoaderBBC to init data.
2017:03:16 21:18:11	reading and processing the text file.
2017:03:16 21:18:11	preprocess the dataset.
2017:03:16 21:18:11	load data.
2017:03:16 21:18:11	init content from raw.
2017:03:16 21:18:11	init data from the raw dataset.
2017:03:16 21:18:11	load context for further preprocessing.
2017:03:16 21:18:11	clean data.
2017:03:16 21:18:26	use DataLoaderBBC to init data.
2017:03:16 21:18:26	reading and processing the text file.
2017:03:16 21:18:26	preprocess the dataset.
2017:03:16 21:18:26	load data.
2017:03:16 21:18:26	init content from raw.
2017:03:16 21:18:26	init data from the raw dataset.
2017:03:16 21:18:26	load context for further preprocessing.
2017:03:16 21:18:26	clean data.
2017:03:16 21:18:26	build a vocabulary.
2017:03:16 21:18:26	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:18:27	...mapping from index to word.
2017:03:16 21:18:27	...mapping from word to index.
2017:03:16 21:18:27	...map word to index.
2017:03:16 21:18:27	...some data statistics.
2017:03:16 21:18:27	......existing 954949 words, vocabulary size is 34571
2017:03:16 21:18:27	...save processed data to file.
2017:03:16 21:18:38	use DataLoaderBBC to init data.
2017:03:16 21:18:38	reading and processing the text file.
2017:03:16 21:18:38	preprocess the dataset.
2017:03:16 21:18:38	load data.
2017:03:16 21:18:38	init content from raw.
2017:03:16 21:18:38	init data from the raw dataset.
2017:03:16 21:18:38	load context for further preprocessing.
2017:03:16 21:18:38	clean data.
2017:03:16 21:18:38	build a vocabulary.
2017:03:16 21:18:38	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:18:39	...mapping from index to word.
2017:03:16 21:18:39	...mapping from word to index.
2017:03:16 21:18:39	...map word to index.
2017:03:16 21:18:39	...some data statistics.
2017:03:16 21:18:39	......existing 954949 words, vocabulary size is 34571
2017:03:16 21:18:39	...save processed data to file.
2017:03:16 21:19:11	use DataLoaderBBC to init data.
2017:03:16 21:19:11	reading and processing the text file.
2017:03:16 21:19:11	preprocess the dataset.
2017:03:16 21:19:11	load data.
2017:03:16 21:19:11	init content from raw.
2017:03:16 21:19:11	init data from the raw dataset.
2017:03:16 21:19:11	load context for further preprocessing.
2017:03:16 21:19:11	clean data.
2017:03:16 21:19:12	build a vocabulary.
2017:03:16 21:19:12	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:19:12	...mapping from index to word.
2017:03:16 21:19:12	...mapping from word to index.
2017:03:16 21:19:12	...map word to index.
2017:03:16 21:19:12	...some data statistics.
2017:03:16 21:19:12	......existing 950501 words, vocabulary size is 35793
2017:03:16 21:19:12	...save processed data to file.
2017:03:16 21:20:06	use DataLoaderBBC to init data.
2017:03:16 21:20:06	reading and processing the text file.
2017:03:16 21:20:06	preprocess the dataset.
2017:03:16 21:20:06	load data.
2017:03:16 21:20:06	init content from raw.
2017:03:16 21:20:06	init data from the raw dataset.
2017:03:16 21:20:06	load context for further preprocessing.
2017:03:16 21:20:06	clean data.
2017:03:16 21:20:07	build a vocabulary.
2017:03:16 21:20:07	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:20:08	...mapping from index to word.
2017:03:16 21:20:08	...mapping from word to index.
2017:03:16 21:20:08	...map word to index.
2017:03:16 21:20:08	...some data statistics.
2017:03:16 21:20:08	......existing 950501 words, vocabulary size is 35793
2017:03:16 21:20:08	...save processed data to file.
2017:03:16 21:20:09	...number of batches: 8
2017:03:16 21:20:09	the number of sentence is 10240, the vocab size is 35793
2017:03:16 21:21:07	use DataLoaderBBC to init data.
2017:03:16 21:21:07	reading and processing the text file.
2017:03:16 21:21:07	preprocess the dataset.
2017:03:16 21:21:07	load data.
2017:03:16 21:21:07	init content from raw.
2017:03:16 21:21:07	init data from the raw dataset.
2017:03:16 21:21:07	load context for further preprocessing.
2017:03:16 21:21:07	clean data.
2017:03:16 21:21:07	build a vocabulary.
2017:03:16 21:21:07	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:21:08	...mapping from index to word.
2017:03:16 21:21:08	...mapping from word to index.
2017:03:16 21:21:08	...map word to index.
2017:03:16 21:21:08	...some data statistics.
2017:03:16 21:21:08	......existing 945920 words, vocabulary size is 34165
2017:03:16 21:21:08	...save processed data to file.
2017:03:16 21:23:51	use DataLoaderBBC to init data.
2017:03:16 21:23:51	reading and processing the text file.
2017:03:16 21:23:51	preprocess the dataset.
2017:03:16 21:23:51	load data.
2017:03:16 21:23:51	init content from raw.
2017:03:16 21:23:51	init data from the raw dataset.
2017:03:16 21:23:51	load context for further preprocessing.
2017:03:16 21:23:51	clean data.
2017:03:16 21:23:51	build a vocabulary.
2017:03:16 21:23:51	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:23:51	...mapping from index to word.
2017:03:16 21:23:51	...mapping from word to index.
2017:03:16 21:23:51	...map word to index.
2017:03:16 21:23:52	...some data statistics.
2017:03:16 21:23:52	......existing 943569 words, vocabulary size is 34007
2017:03:16 21:23:52	...save processed data to file.
2017:03:16 21:30:33	use DataLoaderBBC to init data.
2017:03:16 21:30:33	reading and processing the text file.
2017:03:16 21:30:33	preprocess the dataset.
2017:03:16 21:30:33	load data.
2017:03:16 21:30:33	init content from raw.
2017:03:16 21:30:33	init data from the raw dataset.
2017:03:16 21:30:33	load context for further preprocessing.
2017:03:16 21:30:33	clean data.
2017:03:16 21:30:34	build a vocabulary.
2017:03:16 21:30:34	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:30:34	...mapping from index to word.
2017:03:16 21:30:35	...mapping from word to index.
2017:03:16 21:30:35	...map word to index.
2017:03:16 21:30:35	...some data statistics.
2017:03:16 21:30:35	......existing 943569 words, vocabulary size is 34007
2017:03:16 21:30:35	...save processed data to file.
2017:03:16 21:30:36	...number of batches: 8
2017:03:16 21:30:36	the number of sentence is 10240, the vocab size is 34007
2017:03:16 21:33:09	use DataLoaderBBC to init data.
2017:03:16 21:33:09	reading and processing the text file.
2017:03:16 21:33:09	preprocess the dataset.
2017:03:16 21:33:09	load data.
2017:03:16 21:33:09	init content from raw.
2017:03:16 21:33:09	init data from the raw dataset.
2017:03:16 21:33:09	load context for further preprocessing.
2017:03:16 21:33:09	clean data.
2017:03:16 21:33:10	build a vocabulary.
2017:03:16 21:33:10	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:33:10	...mapping from index to word.
2017:03:16 21:33:10	...mapping from word to index.
2017:03:16 21:33:10	...map word to index.
2017:03:16 21:33:11	...some data statistics.
2017:03:16 21:33:11	......existing 943569 words, vocabulary size is 34007
2017:03:16 21:33:11	...save processed data to file.
2017:03:16 21:35:30	use DataLoaderBBC to init data.
2017:03:16 21:35:30	reading and processing the text file.
2017:03:16 21:35:30	preprocess the dataset.
2017:03:16 21:35:30	load data.
2017:03:16 21:35:30	init content from raw.
2017:03:16 21:35:30	init data from the raw dataset.
2017:03:16 21:35:44	use DataLoaderBBC to init data.
2017:03:16 21:35:44	reading and processing the text file.
2017:03:16 21:35:44	preprocess the dataset.
2017:03:16 21:35:44	load data.
2017:03:16 21:35:44	init content from raw.
2017:03:16 21:35:44	init data from the raw dataset.
2017:03:16 21:35:44	load context for further preprocessing.
2017:03:16 21:37:15	use DataLoaderBBC to init data.
2017:03:16 21:37:15	reading and processing the text file.
2017:03:16 21:37:15	preprocess the dataset.
2017:03:16 21:37:15	load data.
2017:03:16 21:37:15	init content from raw.
2017:03:16 21:37:15	init data from the raw dataset.
2017:03:16 21:37:15	load context for further preprocessing.
2017:03:16 21:38:02	use DataLoaderBBC to init data.
2017:03:16 21:38:02	reading and processing the text file.
2017:03:16 21:38:02	preprocess the dataset.
2017:03:16 21:38:02	load data.
2017:03:16 21:38:02	init content from raw.
2017:03:16 21:38:02	init data from the raw dataset.
2017:03:16 21:38:02	load context for further preprocessing.
2017:03:16 21:38:33	use DataLoaderBBC to init data.
2017:03:16 21:38:33	reading and processing the text file.
2017:03:16 21:38:33	preprocess the dataset.
2017:03:16 21:38:33	load data.
2017:03:16 21:38:33	init content from raw.
2017:03:16 21:38:33	init data from the raw dataset.
2017:03:16 21:38:33	load context for further preprocessing.
2017:03:16 21:38:33	clean data.
2017:03:16 21:38:33	build a vocabulary.
2017:03:16 21:38:33	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:38:33	...mapping from index to word.
2017:03:16 21:38:33	...mapping from word to index.
2017:03:16 21:38:33	...map word to index.
2017:03:16 21:38:33	...some data statistics.
2017:03:16 21:38:33	......existing 199 words, vocabulary size is 121
2017:03:16 21:38:33	...save processed data to file.
2017:03:16 21:38:33	...number of batches: 8
2017:03:16 21:38:49	use DataLoaderBBC to init data.
2017:03:16 21:38:49	reading and processing the text file.
2017:03:16 21:38:49	preprocess the dataset.
2017:03:16 21:38:49	load data.
2017:03:16 21:38:49	init content from raw.
2017:03:16 21:38:49	init data from the raw dataset.
2017:03:16 21:38:49	load context for further preprocessing.
2017:03:16 21:38:49	clean data.
2017:03:16 21:38:50	build a vocabulary.
2017:03:16 21:38:50	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:38:50	...mapping from index to word.
2017:03:16 21:38:50	...mapping from word to index.
2017:03:16 21:38:50	...map word to index.
2017:03:16 21:38:51	...some data statistics.
2017:03:16 21:38:51	......existing 931810 words, vocabulary size is 33444
2017:03:16 21:38:51	...save processed data to file.
2017:03:16 21:38:52	...number of batches: 8
2017:03:16 21:38:52	the number of sentence is 10240, the vocab size is 33444
2017:03:16 21:38:54	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489700334

2017:03:16 21:38:55	------ training ------ 

2017:03:16 21:38:55	train epoch 0
2017:03:16 21:40:20	use DataLoaderBBC to init data.
2017:03:16 21:40:20	reading and processing the text file.
2017:03:16 21:40:20	preprocess the dataset.
2017:03:16 21:40:20	load data.
2017:03:16 21:40:20	init content from raw.
2017:03:16 21:40:20	init data from the raw dataset.
2017:03:16 21:40:20	load context for further preprocessing.
2017:03:16 21:40:20	clean data.
2017:03:16 21:40:21	build a vocabulary.
2017:03:16 21:40:21	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:40:21	...mapping from index to word.
2017:03:16 21:40:21	...mapping from word to index.
2017:03:16 21:40:21	...map word to index.
2017:03:16 21:40:21	...some data statistics.
2017:03:16 21:40:21	......existing 931344 words, vocabulary size is 33434
2017:03:16 21:40:21	...save processed data to file.
2017:03:16 21:40:22	...number of batches: 8
2017:03:16 21:40:22	the number of sentence is 10240, the vocab size is 33434
2017:03:16 21:40:24	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489700424

2017:03:16 21:40:25	------ training ------ 

2017:03:16 21:40:25	train epoch 0
2017:03:16 21:41:54	use DataLoaderBBC to init data.
2017:03:16 21:41:54	reading and processing the text file.
2017:03:16 21:41:54	preprocess the dataset.
2017:03:16 21:41:54	load data.
2017:03:16 21:41:54	init content from raw.
2017:03:16 21:41:54	init data from the raw dataset.
2017:03:16 21:41:54	load context for further preprocessing.
2017:03:16 21:41:54	clean data.
2017:03:16 21:41:54	build a vocabulary.
2017:03:16 21:41:54	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:41:55	...mapping from index to word.
2017:03:16 21:41:55	...mapping from word to index.
2017:03:16 21:41:55	...map word to index.
2017:03:16 21:41:55	...some data statistics.
2017:03:16 21:41:55	......existing 931344 words, vocabulary size is 33434
2017:03:16 21:41:55	...save processed data to file.
2017:03:16 21:41:56	...number of batches: 8
2017:03:16 21:41:56	the number of sentence is 10240, the vocab size is 33434
2017:03:16 21:42:36	use DataLoaderBBC to init data.
2017:03:16 21:42:36	reading and processing the text file.
2017:03:16 21:42:36	preprocess the dataset.
2017:03:16 21:42:36	load data.
2017:03:16 21:42:36	init content from raw.
2017:03:16 21:42:36	init data from the raw dataset.
2017:03:16 21:42:36	load context for further preprocessing.
2017:03:16 21:42:36	clean data.
2017:03:16 21:42:37	build a vocabulary.
2017:03:16 21:42:37	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:42:37	...mapping from index to word.
2017:03:16 21:42:37	...mapping from word to index.
2017:03:16 21:42:37	...map word to index.
2017:03:16 21:42:37	...some data statistics.
2017:03:16 21:42:37	......existing 931344 words, vocabulary size is 33434
2017:03:16 21:42:37	...save processed data to file.
2017:03:16 21:44:11	use DataLoaderBBC to init data.
2017:03:16 21:44:11	reading and processing the text file.
2017:03:16 21:44:11	preprocess the dataset.
2017:03:16 21:44:11	load data.
2017:03:16 21:44:11	init content from raw.
2017:03:16 21:44:11	init data from the raw dataset.
2017:03:16 21:44:12	load context for further preprocessing.
2017:03:16 21:44:12	clean data.
2017:03:16 21:44:12	build a vocabulary.
2017:03:16 21:44:12	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:44:12	...mapping from index to word.
2017:03:16 21:44:12	...mapping from word to index.
2017:03:16 21:44:12	...map word to index.
2017:03:16 21:44:13	...some data statistics.
2017:03:16 21:44:13	......existing 931344 words, vocabulary size is 33434
2017:03:16 21:44:13	...save processed data to file.
2017:03:16 21:44:14	...number of batches: 8
2017:03:16 21:44:14	the number of sentence is 10240, the vocab size is 33434
2017:03:16 21:45:38	use DataLoaderBBC to init data.
2017:03:16 21:45:38	reading and processing the text file.
2017:03:16 21:45:38	preprocess the dataset.
2017:03:16 21:45:38	load data.
2017:03:16 21:45:38	init content from raw.
2017:03:16 21:45:38	init data from the raw dataset.
2017:03:16 21:45:38	load context for further preprocessing.
2017:03:16 21:45:38	clean data.
2017:03:16 21:45:38	build a vocabulary.
2017:03:16 21:45:38	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:45:39	...mapping from index to word.
2017:03:16 21:45:39	...mapping from word to index.
2017:03:16 21:45:39	...map word to index.
2017:03:16 21:45:39	...some data statistics.
2017:03:16 21:45:39	......existing 931344 words, vocabulary size is 33434
2017:03:16 21:45:39	...save processed data to file.
2017:03:16 21:45:40	...number of batches: 8
2017:03:16 21:45:40	the number of sentence is 10240, the vocab size is 33434
2017:03:16 21:48:26	use DataLoaderBBC to init data.
2017:03:16 21:48:26	reading and processing the text file.
2017:03:16 21:48:26	preprocess the dataset.
2017:03:16 21:48:26	load data.
2017:03:16 21:48:26	init content from raw.
2017:03:16 21:48:26	init data from the raw dataset.
2017:03:16 21:48:26	load context for further preprocessing.
2017:03:16 21:48:26	clean data.
2017:03:16 21:48:27	build a vocabulary.
2017:03:16 21:48:27	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:48:27	...mapping from index to word.
2017:03:16 21:48:27	...mapping from word to index.
2017:03:16 21:48:27	...map word to index.
2017:03:16 21:48:27	...some data statistics.
2017:03:16 21:48:27	......existing 931344 words, vocabulary size is 33434
2017:03:16 21:48:27	...save processed data to file.
2017:03:16 21:49:20	use DataLoaderBBC to init data.
2017:03:16 21:49:20	reading and processing the text file.
2017:03:16 21:49:20	preprocess the dataset.
2017:03:16 21:49:20	load data.
2017:03:16 21:49:20	init content from raw.
2017:03:16 21:49:20	init data from the raw dataset.
2017:03:16 21:49:20	load context for further preprocessing.
2017:03:16 21:49:20	clean data.
2017:03:16 21:49:21	build a vocabulary.
2017:03:16 21:49:21	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:49:21	...mapping from index to word.
2017:03:16 21:49:21	...mapping from word to index.
2017:03:16 21:49:21	...map word to index.
2017:03:16 21:49:22	...some data statistics.
2017:03:16 21:49:22	......existing 931344 words, vocabulary size is 33434
2017:03:16 21:49:22	...save processed data to file.
2017:03:16 21:50:56	use DataLoaderBBC to init data.
2017:03:16 21:50:56	reading and processing the text file.
2017:03:16 21:50:56	preprocess the dataset.
2017:03:16 21:50:56	load data.
2017:03:16 21:50:56	init content from raw.
2017:03:16 21:50:56	init data from the raw dataset.
2017:03:16 21:50:56	load context for further preprocessing.
2017:03:16 21:50:56	clean data.
2017:03:16 21:50:58	build a vocabulary.
2017:03:16 21:50:58	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:50:58	...mapping from index to word.
2017:03:16 21:50:58	...mapping from word to index.
2017:03:16 21:50:58	...map word to index.
2017:03:16 21:52:04	use DataLoaderBBC to init data.
2017:03:16 21:52:04	reading and processing the text file.
2017:03:16 21:52:04	preprocess the dataset.
2017:03:16 21:52:04	load data.
2017:03:16 21:52:04	init content from raw.
2017:03:16 21:52:04	init data from the raw dataset.
2017:03:16 21:52:05	load context for further preprocessing.
2017:03:16 21:52:05	clean data.
2017:03:16 21:52:05	build a vocabulary.
2017:03:16 21:52:05	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:52:06	...mapping from index to word.
2017:03:16 21:52:06	...mapping from word to index.
2017:03:16 21:52:06	...map word to index.
2017:03:16 21:52:06	...some data statistics.
2017:03:16 21:52:06	......existing 888042 words, vocabulary size is 33433
2017:03:16 21:52:06	...save processed data to file.
2017:03:16 21:52:07	...number of batches: 8
2017:03:16 21:52:07	the number of sentence is 10240, the vocab size is 33433
2017:03:16 21:52:19	use DataLoaderBBC to init data.
2017:03:16 21:52:19	reading and processing the text file.
2017:03:16 21:52:19	preprocess the dataset.
2017:03:16 21:52:19	load data.
2017:03:16 21:52:19	init content from raw.
2017:03:16 21:52:19	init data from the raw dataset.
2017:03:16 21:52:19	load context for further preprocessing.
2017:03:16 21:52:19	clean data.
2017:03:16 21:52:20	build a vocabulary.
2017:03:16 21:52:20	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:52:21	...mapping from index to word.
2017:03:16 21:52:21	...mapping from word to index.
2017:03:16 21:52:21	...map word to index.
2017:03:16 21:52:21	...some data statistics.
2017:03:16 21:52:21	......existing 888042 words, vocabulary size is 33433
2017:03:16 21:52:21	...save processed data to file.
2017:03:16 21:55:33	use DataLoaderBBC to init data.
2017:03:16 21:55:33	reading and processing the text file.
2017:03:16 21:55:33	preprocess the dataset.
2017:03:16 21:55:33	load data.
2017:03:16 21:55:33	init content from raw.
2017:03:16 21:55:33	init data from the raw dataset.
2017:03:16 21:55:33	load context for further preprocessing.
2017:03:16 21:55:33	clean data.
2017:03:16 21:55:33	build a vocabulary.
2017:03:16 21:55:33	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:55:34	...mapping from index to word.
2017:03:16 21:55:34	...mapping from word to index.
2017:03:16 21:55:34	...map word to index.
2017:03:16 21:59:12	use DataLoaderBBC to init data.
2017:03:16 21:59:12	reading and processing the text file.
2017:03:16 21:59:12	preprocess the dataset.
2017:03:16 21:59:12	load data.
2017:03:16 21:59:12	init content from raw.
2017:03:16 21:59:12	init data from the raw dataset.
2017:03:16 21:59:12	load context for further preprocessing.
2017:03:16 21:59:12	clean data.
2017:03:16 21:59:13	...mask and pad the sentence.
2017:03:16 21:59:13	......max len:361, median len:20.0, min len:0
2017:03:16 21:59:14	build a vocabulary.
2017:03:16 21:59:14	...flatmap a list of sentence list to a list of sentence.
2017:03:16 21:59:19	...mapping from index to word.
2017:03:16 21:59:19	...mapping from word to index.
2017:03:16 21:59:19	...map word to index.
2017:03:16 21:59:20	...some data statistics.
2017:03:16 21:59:20	......existing 43303 words, vocabulary size is 33317
2017:03:16 21:59:20	...save processed data to file.
2017:03:16 21:59:20	...number of batches: 8
2017:03:16 21:59:20	the number of sentence is 10240, the vocab size is 33317
2017:03:16 22:00:11	use DataLoaderBBC to init data.
2017:03:16 22:00:11	reading and processing the text file.
2017:03:16 22:00:11	preprocess the dataset.
2017:03:16 22:00:11	load data.
2017:03:16 22:00:11	init content from raw.
2017:03:16 22:00:11	init data from the raw dataset.
2017:03:16 22:00:11	load context for further preprocessing.
2017:03:16 22:00:11	clean data.
2017:03:16 22:00:12	...mask and pad the sentence.
2017:03:16 22:00:12	......max len:361, median len:20.0, min len:0
2017:03:16 22:00:12	build a vocabulary.
2017:03:16 22:00:12	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:00:13	...mapping from index to word.
2017:03:16 22:00:13	...mapping from word to index.
2017:03:16 22:00:13	...map word to index.
2017:03:16 22:00:13	...some data statistics.
2017:03:16 22:00:13	......existing 43303 words, vocabulary size is 18249
2017:03:16 22:00:13	...save processed data to file.
2017:03:16 22:00:14	...number of batches: 8
2017:03:16 22:00:14	the number of sentence is 10240, the vocab size is 18249
2017:03:16 22:00:31	use DataLoaderBBC to init data.
2017:03:16 22:00:31	reading and processing the text file.
2017:03:16 22:00:31	preprocess the dataset.
2017:03:16 22:00:31	load data.
2017:03:16 22:00:31	init content from raw.
2017:03:16 22:00:31	init data from the raw dataset.
2017:03:16 22:00:31	load context for further preprocessing.
2017:03:16 22:00:31	clean data.
2017:03:16 22:00:32	...mask and pad the sentence.
2017:03:16 22:00:32	......max len:361, median len:20.0, min len:0
2017:03:16 22:00:32	build a vocabulary.
2017:03:16 22:00:32	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:01:23	use DataLoaderBBC to init data.
2017:03:16 22:01:23	reading and processing the text file.
2017:03:16 22:01:23	preprocess the dataset.
2017:03:16 22:01:23	load data.
2017:03:16 22:01:23	init content from raw.
2017:03:16 22:01:23	init data from the raw dataset.
2017:03:16 22:01:23	load context for further preprocessing.
2017:03:16 22:01:23	clean data.
2017:03:16 22:01:24	...mask and pad the sentence.
2017:03:16 22:01:24	......max len:361, median len:20.0, min len:0
2017:03:16 22:01:24	build a vocabulary.
2017:03:16 22:01:24	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:01:57	use DataLoaderBBC to init data.
2017:03:16 22:01:57	reading and processing the text file.
2017:03:16 22:01:57	preprocess the dataset.
2017:03:16 22:01:57	load data.
2017:03:16 22:01:57	init content from raw.
2017:03:16 22:01:57	init data from the raw dataset.
2017:03:16 22:01:58	load context for further preprocessing.
2017:03:16 22:01:58	clean data.
2017:03:16 22:01:58	...mask and pad the sentence.
2017:03:16 22:01:58	......max len:361, median len:20.0, min len:0
2017:03:16 22:01:59	build a vocabulary.
2017:03:16 22:01:59	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:01:59	...mapping from index to word.
2017:03:16 22:01:59	...mapping from word to index.
2017:03:16 22:01:59	...map word to index.
2017:03:16 22:01:59	...some data statistics.
2017:03:16 22:01:59	......existing 43303 words, vocabulary size is 29011
2017:03:16 22:01:59	...save processed data to file.
2017:03:16 22:03:02	use DataLoaderBBC to init data.
2017:03:16 22:03:02	reading and processing the text file.
2017:03:16 22:03:02	preprocess the dataset.
2017:03:16 22:03:02	load data.
2017:03:16 22:03:02	init content from raw.
2017:03:16 22:03:02	init data from the raw dataset.
2017:03:16 22:03:02	load context for further preprocessing.
2017:03:16 22:03:02	clean data.
2017:03:16 22:03:03	...mask and pad the sentence.
2017:03:16 22:03:03	......max len:363, median len:22.0, min len:2
2017:03:16 22:03:03	build a vocabulary.
2017:03:16 22:03:03	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:03:04	...mapping from index to word.
2017:03:16 22:03:04	...mapping from word to index.
2017:03:16 22:03:04	...map word to index.
2017:03:16 22:03:04	...some data statistics.
2017:03:16 22:03:04	......existing 43303 words, vocabulary size is 28074
2017:03:16 22:03:04	...save processed data to file.
2017:03:16 22:03:05	...number of batches: 8
2017:03:16 22:03:05	the number of sentence is 10240, the vocab size is 28074
2017:03:16 22:03:07	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489701787

2017:03:16 22:03:09	------ training ------ 

2017:03:16 22:03:09	train epoch 0
2017:03:16 22:03:36	use DataLoaderBBC to init data.
2017:03:16 22:03:36	reading and processing the text file.
2017:03:16 22:03:36	preprocess the dataset.
2017:03:16 22:03:36	load data.
2017:03:16 22:03:36	init content from raw.
2017:03:16 22:03:36	init data from the raw dataset.
2017:03:16 22:03:36	load context for further preprocessing.
2017:03:16 22:03:36	clean data.
2017:03:16 22:03:37	...mask and pad the sentence.
2017:03:16 22:03:37	......max len:363, median len:22.0, min len:2
2017:03:16 22:03:37	build a vocabulary.
2017:03:16 22:03:37	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:03:38	...mapping from index to word.
2017:03:16 22:03:38	...mapping from word to index.
2017:03:16 22:03:38	...map word to index.
2017:03:16 22:03:38	...some data statistics.
2017:03:16 22:03:38	......existing 43303 words, vocabulary size is 28075
2017:03:16 22:03:38	...save processed data to file.
2017:03:16 22:04:20	use DataLoaderBBC to init data.
2017:03:16 22:04:20	reading and processing the text file.
2017:03:16 22:04:20	preprocess the dataset.
2017:03:16 22:04:20	load data.
2017:03:16 22:04:20	init content from raw.
2017:03:16 22:04:20	init data from the raw dataset.
2017:03:16 22:04:20	load context for further preprocessing.
2017:03:16 22:04:20	clean data.
2017:03:16 22:04:21	...mask and pad the sentence.
2017:03:16 22:04:21	......max len:363, median len:22.0, min len:2
2017:03:16 22:04:21	build a vocabulary.
2017:03:16 22:04:21	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:04:21	...mapping from index to word.
2017:03:16 22:04:21	...mapping from word to index.
2017:03:16 22:04:21	...map word to index.
2017:03:16 22:04:22	...some data statistics.
2017:03:16 22:04:22	......existing 43303 words, vocabulary size is 28075
2017:03:16 22:04:22	...save processed data to file.
2017:03:16 22:05:58	use DataLoaderBBC to init data.
2017:03:16 22:05:58	reading and processing the text file.
2017:03:16 22:05:58	preprocess the dataset.
2017:03:16 22:05:58	load data.
2017:03:16 22:05:58	init content from raw.
2017:03:16 22:05:58	init data from the raw dataset.
2017:03:16 22:05:58	load context for further preprocessing.
2017:03:16 22:05:58	clean data.
2017:03:16 22:05:59	...mask and pad the sentence.
2017:03:16 22:05:59	......max len:363, median len:22.0, min len:2
2017:03:16 22:05:59	build a vocabulary.
2017:03:16 22:05:59	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:05:59	...mapping from index to word.
2017:03:16 22:05:59	...mapping from word to index.
2017:03:16 22:05:59	...map word to index.
2017:03:16 22:06:00	...some data statistics.
2017:03:16 22:06:24	use DataLoaderBBC to init data.
2017:03:16 22:06:24	reading and processing the text file.
2017:03:16 22:06:24	preprocess the dataset.
2017:03:16 22:06:24	load data.
2017:03:16 22:06:24	init content from raw.
2017:03:16 22:06:24	init data from the raw dataset.
2017:03:16 22:06:24	load context for further preprocessing.
2017:03:16 22:06:24	clean data.
2017:03:16 22:06:25	...mask and pad the sentence.
2017:03:16 22:06:25	......max len:363, median len:22.0, min len:2
2017:03:16 22:06:25	build a vocabulary.
2017:03:16 22:06:25	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:06:25	...mapping from index to word.
2017:03:16 22:06:25	...mapping from word to index.
2017:03:16 22:06:25	...map word to index.
2017:03:16 22:06:26	...some data statistics.
2017:03:16 22:06:48	use DataLoaderBBC to init data.
2017:03:16 22:06:48	reading and processing the text file.
2017:03:16 22:06:48	preprocess the dataset.
2017:03:16 22:06:48	load data.
2017:03:16 22:06:48	init content from raw.
2017:03:16 22:06:48	init data from the raw dataset.
2017:03:16 22:06:48	load context for further preprocessing.
2017:03:16 22:06:48	clean data.
2017:03:16 22:06:49	...mask and pad the sentence.
2017:03:16 22:06:49	......max len:363, median len:22.0, min len:2
2017:03:16 22:06:49	build a vocabulary.
2017:03:16 22:06:49	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:06:50	...mapping from index to word.
2017:03:16 22:06:50	...mapping from word to index.
2017:03:16 22:06:50	...map word to index.
2017:03:16 22:06:50	...some data statistics.
2017:03:16 22:07:11	use DataLoaderBBC to init data.
2017:03:16 22:07:11	reading and processing the text file.
2017:03:16 22:07:11	preprocess the dataset.
2017:03:16 22:07:11	load data.
2017:03:16 22:07:11	init content from raw.
2017:03:16 22:07:11	init data from the raw dataset.
2017:03:16 22:07:11	load context for further preprocessing.
2017:03:16 22:07:11	clean data.
2017:03:16 22:07:12	...mask and pad the sentence.
2017:03:16 22:07:12	......max len:363, median len:22.0, min len:2
2017:03:16 22:07:12	build a vocabulary.
2017:03:16 22:07:12	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:07:12	...mapping from index to word.
2017:03:16 22:07:12	...mapping from word to index.
2017:03:16 22:07:12	...map word to index.
2017:03:16 22:07:33	use DataLoaderBBC to init data.
2017:03:16 22:07:33	reading and processing the text file.
2017:03:16 22:07:33	preprocess the dataset.
2017:03:16 22:07:33	load data.
2017:03:16 22:07:33	init content from raw.
2017:03:16 22:07:33	init data from the raw dataset.
2017:03:16 22:07:33	load context for further preprocessing.
2017:03:16 22:07:33	clean data.
2017:03:16 22:07:34	...mask and pad the sentence.
2017:03:16 22:07:34	......max len:363, median len:22.0, min len:2
2017:03:16 22:07:34	build a vocabulary.
2017:03:16 22:07:34	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:07:35	...mapping from index to word.
2017:03:16 22:07:35	...mapping from word to index.
2017:03:16 22:07:35	...map word to index.
2017:03:16 22:07:35	...some data statistics.
2017:03:16 22:09:27	use DataLoaderBBC to init data.
2017:03:16 22:09:27	reading and processing the text file.
2017:03:16 22:09:27	preprocess the dataset.
2017:03:16 22:09:27	load data.
2017:03:16 22:09:27	init content from raw.
2017:03:16 22:09:27	init data from the raw dataset.
2017:03:16 22:09:27	load context for further preprocessing.
2017:03:16 22:09:27	clean data.
2017:03:16 22:09:28	...mask and pad the sentence.
2017:03:16 22:09:28	......max len:363, median len:22.0, min len:2
2017:03:16 22:09:28	build a vocabulary.
2017:03:16 22:09:28	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:09:28	...mapping from index to word.
2017:03:16 22:09:29	...mapping from word to index.
2017:03:16 22:09:29	...map word to index.
2017:03:16 22:09:29	...some data statistics.
2017:03:16 22:09:29	......existing 32626 sentences, vocabulary size is 28075
2017:03:16 22:09:29	...save processed data to file.
2017:03:16 22:09:31	...number of batches: 8
2017:03:16 22:09:31	the number of sentence is 10240, the vocab size is 28075
2017:03:16 22:10:08	use DataLoaderBBC to init data.
2017:03:16 22:10:08	reading and processing the text file.
2017:03:16 22:10:08	preprocess the dataset.
2017:03:16 22:10:08	load data.
2017:03:16 22:10:08	init content from raw.
2017:03:16 22:10:08	init data from the raw dataset.
2017:03:16 22:10:08	load context for further preprocessing.
2017:03:16 22:10:08	clean data.
2017:03:16 22:10:08	...mask and pad the sentence.
2017:03:16 22:10:08	......max len:363, median len:22.0, min len:2
2017:03:16 22:10:09	build a vocabulary.
2017:03:16 22:10:09	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:10:09	...mapping from index to word.
2017:03:16 22:10:09	...mapping from word to index.
2017:03:16 22:10:09	...map word to index.
2017:03:16 22:10:09	...some data statistics.
2017:03:16 22:10:09	......existing 32626 sentences, vocabulary size is 28075
2017:03:16 22:10:09	...save processed data to file.
2017:03:16 22:10:12	...number of batches: 8
2017:03:16 22:10:12	the number of sentence is 10240, the vocab size is 28075
2017:03:16 22:10:13	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489702213

2017:03:16 22:10:15	------ training ------ 

2017:03:16 22:10:15	train epoch 0
2017:03:16 22:22:05	use DataLoaderBBC to init data.
2017:03:16 22:22:05	reading and processing the text file.
2017:03:16 22:22:05	preprocess the dataset.
2017:03:16 22:22:05	load data.
2017:03:16 22:22:05	init content from raw.
2017:03:16 22:22:05	init data from the raw dataset.
2017:03:16 22:22:07	load context for further preprocessing.
2017:03:16 22:22:07	clean data.
2017:03:16 22:22:08	...mask and pad the sentence.
2017:03:16 22:22:08	......max len:363, median len:22.0, min len:2
2017:03:16 22:22:09	build a vocabulary.
2017:03:16 22:22:09	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:22:09	...mapping from index to word.
2017:03:16 22:22:09	...mapping from word to index.
2017:03:16 22:22:09	...map word to index.
2017:03:16 22:22:10	...some data statistics.
2017:03:16 22:22:10	......existing 32626 sentences, vocabulary size is 28075
2017:03:16 22:22:10	...save processed data to file.
2017:03:16 22:22:12	...number of batches: 80
2017:03:16 22:24:04	use DataLoaderBBC to init data.
2017:03:16 22:24:04	reading and processing the text file.
2017:03:16 22:24:04	preprocess the dataset.
2017:03:16 22:24:04	load data.
2017:03:16 22:24:04	init content from raw.
2017:03:16 22:24:04	init data from the raw dataset.
2017:03:16 22:24:04	load context for further preprocessing.
2017:03:16 22:24:04	clean data.
2017:03:16 22:24:05	...mask and pad the sentence.
2017:03:16 22:24:05	......max len:363, median len:22.0, min len:2
2017:03:16 22:24:05	build a vocabulary.
2017:03:16 22:24:05	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:24:06	...mapping from index to word.
2017:03:16 22:24:06	...mapping from word to index.
2017:03:16 22:24:06	...map word to index.
2017:03:16 22:24:07	...some data statistics.
2017:03:16 22:24:07	......existing 32626 sentences, vocabulary size is 28075
2017:03:16 22:24:07	...save processed data to file.
2017:03:16 22:24:09	...number of batches: 80
2017:03:16 22:24:09	the number of sentence is 32626, the vocab size is 28075
2017:03:16 22:24:13	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489703053

2017:03:16 22:24:15	------ training ------ 

2017:03:16 22:24:15	train epoch 0
2017:03:16 22:24:53	use DataLoaderBBC to init data.
2017:03:16 22:24:53	reading and processing the text file.
2017:03:16 22:24:53	preprocess the dataset.
2017:03:16 22:24:53	load data.
2017:03:16 22:24:53	init content from raw.
2017:03:16 22:24:53	init data from the raw dataset.
2017:03:16 22:24:54	load context for further preprocessing.
2017:03:16 22:24:54	clean data.
2017:03:16 22:24:55	...mask and pad the sentence.
2017:03:16 22:24:55	......max len:363, median len:22.0, min len:2
2017:03:16 22:24:55	build a vocabulary.
2017:03:16 22:24:55	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:24:56	...mapping from index to word.
2017:03:16 22:24:56	...mapping from word to index.
2017:03:16 22:24:56	...map word to index.
2017:03:16 22:24:56	...some data statistics.
2017:03:16 22:24:56	......existing 32626 sentences, vocabulary size is 28075
2017:03:16 22:24:56	...save processed data to file.
2017:03:16 22:24:58	...number of batches: 80
2017:03:16 22:24:58	the number of sentence is 32626, the vocab size is 28075
2017:03:16 22:25:02	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489703102

2017:03:16 22:25:04	------ training ------ 

2017:03:16 22:25:04	train epoch 0
2017:03:16 22:25:18	use DataLoaderBBC to init data.
2017:03:16 22:25:18	reading and processing the text file.
2017:03:16 22:25:18	preprocess the dataset.
2017:03:16 22:25:18	load data.
2017:03:16 22:25:18	init content from raw.
2017:03:16 22:25:18	init data from the raw dataset.
2017:03:16 22:25:18	load context for further preprocessing.
2017:03:16 22:25:18	clean data.
2017:03:16 22:25:19	...mask and pad the sentence.
2017:03:16 22:25:19	......max len:363, median len:22.0, min len:2
2017:03:16 22:25:19	build a vocabulary.
2017:03:16 22:25:19	...flatmap a list of sentence list to a list of sentence.
2017:03:16 22:25:20	...mapping from index to word.
2017:03:16 22:25:20	...mapping from word to index.
2017:03:16 22:25:20	...map word to index.
2017:03:16 22:25:20	...some data statistics.
2017:03:16 22:25:20	......existing 32626 sentences, vocabulary size is 28075
2017:03:16 22:25:20	...save processed data to file.
2017:03:16 22:25:22	...number of batches: 80
2017:03:16 22:25:22	the number of sentence is 32626, the vocab size is 28075
2017:03:16 22:25:25	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489703125

2017:03:16 22:25:27	------ training ------ 

2017:03:16 22:25:27	train epoch 0
2017:03:16 22:30:52	use DataLoaderBBC to init data.
2017:03:16 22:30:52	reading and processing the text file.
2017:03:16 22:30:52	preprocess the dataset.
2017:03:16 22:30:52	load data.
2017:03:16 22:30:52	init content from raw.
2017:03:16 22:30:52	init data from the raw dataset.
2017:03:16 23:24:08	use DataLoaderBBC to init data.
2017:03:16 23:24:08	reading and processing the text file.
2017:03:16 23:24:08	preprocess the dataset.
2017:03:16 23:24:08	load data.
2017:03:16 23:24:08	init content from raw.
2017:03:16 23:24:08	init data from the raw dataset.
2017:03:16 23:24:08	load context for further preprocessing.
2017:03:16 23:24:08	clean data.
2017:03:16 23:24:09	...mask and pad the sentence.
2017:03:16 23:24:09	......max len:363, median len:22.0, min len:2
2017:03:16 23:24:10	build a vocabulary.
2017:03:16 23:24:10	...flatmap a list of sentence list to a list of sentence.
2017:03:16 23:24:10	...mapping from index to word.
2017:03:16 23:24:10	...mapping from word to index.
2017:03:16 23:24:10	...map word to index.
2017:03:16 23:24:10	...some data statistics.
2017:03:16 23:24:10	......existing 32626 sentences, vocabulary size is 28075
2017:03:16 23:24:10	...save processed data to file.
2017:03:16 23:24:12	...number of batches: 80
2017:03:16 23:24:12	the number of sentence is 32626, the vocab size is 28075
2017:03:16 23:24:16	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489706656

2017:03:16 23:24:18	------ training ------ 

2017:03:16 23:24:18	train epoch 0
2017:03:17 09:43:36	use DataLoaderBBC to init data.
2017:03:17 09:43:36	reading and processing the text file.
2017:03:17 09:43:36	preprocess the dataset.
2017:03:17 09:43:36	load data.
2017:03:17 09:43:36	init content from raw.
2017:03:17 09:43:36	init data from the raw dataset.
2017:03:17 09:43:37	load context for further preprocessing.
2017:03:17 09:43:37	clean data.
2017:03:17 09:43:38	...mask and pad the sentence.
2017:03:17 09:43:38	......max len:363, median len:22.0, min len:2
2017:03:17 09:43:39	build a vocabulary.
2017:03:17 09:43:39	...flatmap a list of sentence list to a list of sentence.
2017:03:17 09:43:39	...mapping from index to word.
2017:03:17 09:43:39	...mapping from word to index.
2017:03:17 09:43:39	...map word to index.
2017:03:17 09:43:39	...some data statistics.
2017:03:17 09:43:39	......existing 32626 sentences, vocabulary size is 28075
2017:03:17 09:43:39	...save processed data to file.
2017:03:17 09:43:41	...number of batches: 254
2017:03:17 09:43:41	the number of sentence is 32626, the vocab size is 28075
2017:03:17 09:50:28	use DataLoaderBBC to init data.
2017:03:17 09:50:28	reading and processing the text file.
2017:03:17 09:50:28	preprocess the dataset.
2017:03:17 09:50:28	load data.
2017:03:17 09:50:28	init content from raw.
2017:03:17 09:50:28	init data from the raw dataset.
2017:03:17 09:50:28	load context for further preprocessing.
2017:03:17 09:50:28	clean data.
2017:03:17 09:50:29	...mask and pad the sentence.
2017:03:17 09:50:29	......max len:363, median len:22.0, min len:2
2017:03:17 09:50:29	build a vocabulary.
2017:03:17 09:50:29	...flatmap a list of sentence list to a list of sentence.
2017:03:17 09:50:30	...mapping from index to word.
2017:03:17 09:50:30	...mapping from word to index.
2017:03:17 09:50:30	...map word to index.
2017:03:17 09:50:30	...some data statistics.
2017:03:17 09:50:30	......existing 32626 sentences, vocabulary size is 28075
2017:03:17 09:50:30	...save processed data to file.
2017:03:17 09:50:32	...number of batches: 254
2017:03:17 09:50:32	the number of sentence is 32626, the vocab size is 28075
2017:03:17 09:50:35	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489744235

2017:03:18 19:39:17	use DataLoaderBBC to init data.
2017:03:18 19:39:17	reading and processing the text file.
2017:03:18 19:39:17	preprocess the dataset.
2017:03:18 19:39:17	load data.
2017:03:18 19:39:17	init content from raw.
2017:03:18 19:39:17	init data from the raw dataset.
2017:03:18 19:40:31	use DataLoaderBBC to init data.
2017:03:18 19:40:31	reading and processing the text file.
2017:03:18 19:40:31	preprocess the dataset.
2017:03:18 19:40:31	load data.
2017:03:18 19:40:31	load context for further preprocessing.
2017:03:18 19:40:31	clean data.
2017:03:18 19:40:32	...mask and pad the sentence.
2017:03:18 19:40:32	......max len:363, median len:22.0, min len:2
2017:03:18 19:40:32	build a vocabulary.
2017:03:18 19:40:32	...flatmap a list of sentence list to a list of sentence.
2017:03:18 19:40:33	...mapping from index to word.
2017:03:18 19:40:33	...mapping from word to index.
2017:03:18 19:40:33	...map word to index.
2017:03:18 19:40:33	...some data statistics.
2017:03:18 19:40:33	......existing 32626 sentences, vocabulary size is 28075
2017:03:18 19:40:33	...save processed data to file.
2017:03:18 19:40:36	...number of batches: 509
2017:03:18 19:41:23	use DataLoaderBBC to init data.
2017:03:18 19:41:23	loading preprocessed files.
2017:03:18 19:41:26	...some data statistics.
2017:03:18 19:41:26	......existing 32626 sentences, vocabulary size is 28075
2017:03:18 19:41:26	...number of batches: 509
2017:03:18 19:41:26	the number of sentence is 32626, the vocab size is 28075
2017:03:18 19:42:08	use DataLoaderBBC to init data.
2017:03:18 19:42:08	loading preprocessed files.
2017:03:18 19:42:12	...some data statistics.
2017:03:18 19:42:12	......existing 32626 sentences, vocabulary size is 28075
2017:03:18 19:42:12	...number of batches: 509
2017:03:18 19:42:12	the number of sentence is 32626, the vocab size is 28075
2017:03:18 19:43:04	use DataLoaderBBC to init data.
2017:03:18 19:43:04	loading preprocessed files.
2017:03:18 19:43:08	...some data statistics.
2017:03:18 19:43:08	......existing 32626 sentences, vocabulary size is 28075
2017:03:18 19:43:08	...number of batches: 509
2017:03:18 19:43:08	the number of sentence is 32626, the vocab size is 28075
2017:03:18 19:43:47	use DataLoaderBBC to init data.
2017:03:18 19:43:47	loading preprocessed files.
2017:03:18 19:43:51	...some data statistics.
2017:03:18 19:43:51	......existing 32626 sentences, vocabulary size is 28075
2017:03:18 19:43:51	...number of batches: 509
2017:03:18 19:43:51	the number of sentence is 32626, the vocab size is 28075
2017:03:18 19:44:28	use DataLoaderBBC to init data.
2017:03:18 19:44:28	loading preprocessed files.
2017:03:18 19:44:31	...some data statistics.
2017:03:18 19:44:31	......existing 32626 sentences, vocabulary size is 28075
2017:03:18 19:44:31	...number of batches: 509
2017:03:18 19:44:31	the number of sentence is 32626, the vocab size is 28075
2017:03:18 19:44:37	writing to /home/tlin/notebooks/code/demo2_v/data/tinyshakespeare/training/runs/DataLoaderBBC/code.model.textG.TextG/1489866276

2017:03:18 19:44:40	------ training ------ 

2017:03:18 19:45:04	use DataLoaderBBC to init data.
2017:03:18 19:45:04	loading preprocessed files.
2017:03:18 19:45:07	...some data statistics.
2017:03:18 19:45:07	......existing 32626 sentences, vocabulary size is 28075
2017:03:18 19:45:07	...number of batches: 509
2017:03:18 19:45:07	the number of sentence is 32626, the vocab size is 28075
2017:03:18 19:45:12	writing to /home/tlin/notebooks/code/demo2_v/data/tinyshakespeare/training/runs/DataLoaderBBC/code.model.textG.TextG/1489866312

2017:03:18 19:45:15	------ training ------ 

2017:03:18 19:45:15	train epoch 0
2017:03:18 19:46:04	use DataLoaderShakespeare to init data.
2017:03:18 19:46:29	use DataLoaderShakespeare to init data.
2017:03:18 19:46:29	reading and processing the text file.
2017:03:18 19:46:29	preprocess the dataset.
2017:03:18 19:46:58	use DataLoaderShakespeare to init data.
2017:03:18 19:46:58	reading and processing the text file.
2017:03:18 19:46:58	preprocess the dataset.
2017:03:18 19:47:16	build a vocabulary.
2017:03:18 19:47:16	...flatmap a list of sentence list to a list of sentence.
2017:03:18 19:47:16	...mapping from index to word.
2017:03:18 19:47:16	...mapping from word to index.
2017:03:18 19:47:16	...map word to index.
2017:03:18 19:47:16	...some data statistics.
2017:03:18 19:47:16	......existing 243264 words, vocabulary size is 14004
2017:03:18 19:47:16	...save processed data to file.
2017:03:18 19:47:17	...number of batches: 152
2017:03:18 19:47:27	use DataLoaderShakespeare to init data.
2017:03:18 19:47:27	loading preprocessed files.
2017:03:18 19:47:27	......existing 243264 words, vocabulary size is 14004
2017:03:18 19:47:27	...number of batches: 152
2017:03:18 19:47:27	the number of sentence is 243200, the vocab size is 14004
2017:03:18 19:47:32	writing to /home/tlin/notebooks/code/demo2_v/data/tinyshakespeare/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489866452

2017:03:18 19:47:35	------ training ------ 

2017:03:18 19:47:35	train epoch 0
2017:03:18 19:49:58	use DataLoaderShakespeare to init data.
2017:03:18 19:49:58	reading and processing the text file.
2017:03:18 19:49:58	preprocess the dataset.
2017:03:18 19:50:15	build a vocabulary.
2017:03:18 19:50:15	...flatmap a list of sentence list to a list of sentence.
2017:03:18 19:50:15	...mapping from index to word.
2017:03:18 19:50:15	...mapping from word to index.
2017:03:18 19:50:15	...map word to index.
2017:03:18 19:50:16	...some data statistics.
2017:03:18 19:50:16	......existing 243264 words, vocabulary size is 14004
2017:03:18 19:50:16	...save processed data to file.
2017:03:18 19:50:16	...number of batches: 152
2017:03:18 19:50:16	the number of sentence is 243200, the vocab size is 14004
2017:03:18 19:50:21	writing to /home/tlin/notebooks/code/demo2_v/data/tinyshakespeare/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489866621

2017:03:18 19:50:24	------ training ------ 

2017:03:18 19:50:24	train epoch 0
2017:03:18 19:51:56	use DataLoaderShakespeare to init data.
2017:03:18 19:51:56	reading and processing the text file.
2017:03:18 19:51:56	preprocess the dataset.
2017:03:18 19:52:13	build a vocabulary.
2017:03:18 19:52:13	...flatmap a list of sentence list to a list of sentence.
2017:03:18 19:52:13	...mapping from index to word.
2017:03:18 19:52:13	...mapping from word to index.
2017:03:18 19:52:13	...map word to index.
2017:03:18 19:52:13	...some data statistics.
2017:03:18 19:52:13	......existing 243264 words, vocabulary size is 14004
2017:03:18 19:52:13	...save processed data to file.
2017:03:18 19:52:13	...number of batches: 152
2017:03:18 19:52:13	the number of sentence is 243200, the vocab size is 14004
2017:03:18 19:52:18	writing to /home/tlin/notebooks/code/demo2_v/data/tinyshakespeare/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489866738

2017:03:18 19:52:21	------ training ------ 

2017:03:18 19:52:21	train epoch 0
2017:03:18 19:53:08	use DataLoaderShakespeare to init data.
2017:03:18 19:53:08	loading preprocessed files.
2017:03:18 19:53:09	......existing 243264 words, vocabulary size is 14004
2017:03:18 19:53:09	...number of batches: 152
2017:03:18 19:53:09	the number of sentence is 243200, the vocab size is 14004
2017:03:18 19:53:15	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489866795

2017:03:18 19:53:17	------ training ------ 

2017:03:18 19:53:17	train epoch 0
2017:03:18 19:58:17	use DataLoaderShakespeare to init data.
2017:03:18 19:58:17	loading preprocessed files.
2017:03:18 19:58:18	......existing 243264 words, vocabulary size is 14004
2017:03:18 19:58:18	...number of batches: 152
2017:03:18 19:58:18	the number of sentence is 243200, the vocab size is 14004
2017:03:18 19:58:22	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489867102

2017:03:18 19:58:25	------ training ------ 

2017:03:18 19:58:25	train epoch 0
2017:03:18 20:06:47	use DataLoaderShakespeare to init data.
2017:03:18 20:06:47	loading preprocessed files.
2017:03:18 20:06:48	......existing 243264 words, vocabulary size is 14004
2017:03:18 20:06:48	...number of batches: 152
2017:03:18 20:06:48	the number of sentence is 243200, the vocab size is 14004
2017:03:18 20:06:52	------ training ------ 

2017:03:18 20:06:52	train epoch 0
2017:03:18 20:15:22	use DataLoaderShakespeare to init data.
2017:03:18 20:15:22	loading preprocessed files.
2017:03:18 20:15:23	......existing 243264 words, vocabulary size is 14004
2017:03:18 20:15:23	...number of batches: 152
2017:03:18 20:15:23	the number of sentence is 243200, the vocab size is 14004
2017:03:18 20:15:27	------ training ------ 

2017:03:18 20:15:27	train epoch 0
2017:03:18 20:17:29	use DataLoaderShakespeare to init data.
2017:03:18 20:17:29	loading preprocessed files.
2017:03:18 20:17:29	......existing 243264 words, vocabulary size is 14004
2017:03:18 20:17:29	...number of batches: 152
2017:03:18 20:17:29	the number of sentence is 243200, the vocab size is 14004
2017:03:18 20:17:33	------ training ------ 

2017:03:18 20:17:33	train epoch 0
2017:03:18 20:17:50	use DataLoaderShakespeare to init data.
2017:03:18 20:17:50	loading preprocessed files.
2017:03:18 20:17:51	......existing 243264 words, vocabulary size is 14004
2017:03:18 20:17:51	...number of batches: 152
2017:03:18 20:17:51	the number of sentence is 243200, the vocab size is 14004
2017:03:18 20:17:54	------ training ------ 

2017:03:18 20:17:54	train epoch 0
2017:03:18 21:49:09	use DataLoaderShakespeare to init data.
2017:03:18 21:49:09	reading and processing the text file.
2017:03:18 21:49:09	preprocess the dataset.
2017:03:18 21:49:33	use DataLoaderShakespeare to init data.
2017:03:18 21:49:33	loading preprocessed files.
2017:03:18 21:50:09	use DataLoaderShakespeare to init data.
2017:03:18 21:50:09	reading and processing the text file.
2017:03:18 21:50:09	preprocess the dataset.
2017:03:18 21:50:26	build a vocabulary.
2017:03:18 21:50:26	...flatmap a list of sentence list to a list of sentence.
2017:03:18 21:50:26	...mapping from index to word.
2017:03:18 21:50:26	...mapping from word to index.
2017:03:18 21:50:26	...map word to index.
2017:03:18 21:50:26	...some data statistics.
2017:03:18 21:50:26	......existing 243264 words, vocabulary size is 14004
2017:03:18 21:50:26	...save processed data to file.
2017:03:18 21:50:27	...number of batches: 152
2017:03:18 21:51:16	use DataLoaderShakespeare to init data.
2017:03:18 21:51:16	reading and processing the text file.
2017:03:18 21:51:16	preprocess the dataset.
2017:03:18 21:51:32	build a vocabulary.
2017:03:18 21:51:32	...flatmap a list of sentence list to a list of sentence.
2017:03:18 21:51:32	...mapping from index to word.
2017:03:18 21:51:32	...mapping from word to index.
2017:03:18 21:51:32	...map word to index.
2017:03:18 21:51:32	...some data statistics.
2017:03:18 21:51:32	......existing 243264 words, vocabulary size is 14004
2017:03:18 21:51:32	...save processed data to file.
2017:03:18 21:51:32	...number of batches: 152
2017:03:18 21:51:53	use DataLoaderShakespeare to init data.
2017:03:18 21:51:53	loading preprocessed files.
2017:03:18 21:51:54	......existing 243264 words, vocabulary size is 14004
2017:03:18 21:51:54	...number of batches: 152
2017:03:18 21:53:36	use DataLoaderShakespeare to init data.
2017:03:18 21:53:36	loading preprocessed files.
2017:03:18 21:53:37	......existing 243264 words, vocabulary size is 14004
2017:03:18 21:53:37	...number of batches: 152
2017:03:18 21:53:37	the number of sentence is 9728, the vocab size is 14004
2017:03:18 21:54:03	use DataLoaderShakespeare to init data.
2017:03:18 21:54:03	loading preprocessed files.
2017:03:18 21:54:04	......existing 243264 words, vocabulary size is 14004
2017:03:18 21:54:04	...number of batches: 152
2017:03:18 21:54:04	the number of sentence is 9728, the vocab size is 14004
2017:03:18 21:54:30	use DataLoaderShakespeare to init data.
2017:03:18 21:54:30	loading preprocessed files.
2017:03:18 21:54:31	......existing 243264 words, vocabulary size is 14004
2017:03:18 21:54:31	...number of batches: 152
2017:03:18 21:54:31	the number of sentence is 9728, the vocab size is 14004
2017:03:18 21:54:37	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489874076

2017:03:18 21:54:40	------ training ------ 

2017:03:18 21:54:40	train epoch 0
2017:03:18 21:55:22	use DataLoaderShakespeare to init data.
2017:03:18 21:55:22	loading preprocessed files.
2017:03:18 21:55:23	......existing 243264 words, vocabulary size is 14004
2017:03:18 21:55:23	...number of batches: 152
2017:03:18 21:55:23	the number of sentence is 9728, the vocab size is 14004
2017:03:18 21:55:28	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489874128

2017:03:18 21:55:31	------ training ------ 

2017:03:18 21:55:31	train epoch 0
2017:03:18 21:56:00	use DataLoaderShakespeare to init data.
2017:03:18 21:56:00	loading preprocessed files.
2017:03:18 21:56:02	......existing 243264 words, vocabulary size is 14004
2017:03:18 21:56:02	...number of batches: 152
2017:03:18 21:56:02	the number of sentence is 9728, the vocab size is 14004
2017:03:18 21:56:08	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489874168

2017:03:18 21:56:10	------ training ------ 

2017:03:18 21:56:10	train epoch 0
2017:03:19 11:02:54	use DataLoaderBBC to init data.
2017:03:19 11:02:54	loading preprocessed files.
2017:03:19 11:02:58	...number of batches: 509
2017:03:19 11:08:08	use DataLoaderBBC to init data.
2017:03:19 11:08:08	loading preprocessed files.
2017:03:19 11:08:11	...get data info.
2017:03:19 11:08:11	...number of batches: 509
2017:03:19 11:08:11	...init batch data.
2017:03:19 11:08:11	num of sentence: 32576, sentence length: 30, vocab size: 28075
2017:03:19 11:08:35	use DataLoaderShakespeare to init data.
2017:03:19 11:08:35	loading preprocessed files.
2017:03:19 11:08:36	...get data info.
2017:03:19 11:08:36	...number of batches: 152
2017:03:19 11:08:36	...init batch data.
2017:03:19 11:08:36	the number of sentence is 9728, the vocab size is 14004
2017:03:19 11:08:40	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489921720

2017:03:19 11:08:43	------ training ------ 

2017:03:19 11:08:43	train epoch 0
2017:03:19 11:09:03	use DataLoaderBBC to init data.
2017:03:19 11:09:03	loading preprocessed files.
2017:03:19 11:09:06	...get data info.
2017:03:19 11:09:06	...number of batches: 509
2017:03:19 11:09:06	...init batch data.
2017:03:19 11:09:06	num of sentence: 32576, sentence length: 30, vocab size: 28075
2017:03:19 11:09:39	use DataLoaderBBC to init data.
2017:03:19 11:09:39	loading preprocessed files.
2017:03:19 11:09:43	...get data info.
2017:03:19 11:09:43	...number of batches: 509
2017:03:19 11:09:43	...init batch data.
2017:03:19 11:09:43	num of sentence: 32576, sentence length: 30, vocab size: 28075
2017:03:19 11:10:40	use DataLoaderBBC to init data.
2017:03:19 11:10:40	reading and processing the text file.
2017:03:19 11:10:40	preprocess the dataset.
2017:03:19 11:10:40	load data.
2017:03:19 11:10:40	init content from raw.
2017:03:19 11:10:40	init data from the raw dataset.
2017:03:19 11:10:41	load context for further preprocessing.
2017:03:19 11:10:41	clean data.
2017:03:19 11:10:42	...mask and pad the sentence.
2017:03:19 11:10:42	......max len:363, median len:22.0, min len:2
2017:03:19 11:10:43	build a vocabulary.
2017:03:19 11:10:43	...flatmap a list of sentence list to a list of sentence.
2017:03:19 11:10:43	...mapping from index to word.
2017:03:19 11:10:43	...mapping from word to index.
2017:03:19 11:10:43	...map word to index.
2017:03:19 11:10:43	...save processed data to file.
2017:03:19 11:10:44	...get data info.
2017:03:19 11:10:44	...number of batches: 390
2017:03:19 11:10:44	...init batch data.
2017:03:19 11:10:44	num of sentence: 24960, sentence length: 25, vocab size: 23702
2017:03:19 11:10:49	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489921848

2017:03:19 11:10:51	------ training ------ 

2017:03:19 11:10:51	train epoch 0
2017:03:19 11:12:18	use DataLoaderBBC to init data.
2017:03:19 11:12:18	reading and processing the text file.
2017:03:19 11:12:18	preprocess the dataset.
2017:03:19 11:12:18	load data.
2017:03:19 11:12:18	init content from raw.
2017:03:19 11:12:18	init data from the raw dataset.
2017:03:19 11:12:19	load context for further preprocessing.
2017:03:19 11:12:19	clean data.
2017:03:19 11:12:20	...mask and pad the sentence.
2017:03:19 11:12:20	......max len:363, median len:22.0, min len:2
2017:03:19 11:12:20	build a vocabulary.
2017:03:19 11:12:20	...flatmap a list of sentence list to a list of sentence.
2017:03:19 11:12:20	...mapping from index to word.
2017:03:19 11:12:20	...mapping from word to index.
2017:03:19 11:12:20	...map word to index.
2017:03:19 11:12:20	...save processed data to file.
2017:03:19 11:12:21	get data info.
2017:03:19 11:12:21	init batch data.
2017:03:19 11:12:21	...number of batches: 390
2017:03:19 11:12:21	num of sentence: 24960, sentence length: 25, vocab size: 23702
2017:03:19 11:12:26	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489921946

2017:03:19 11:12:28	------ training ------ 

2017:03:19 11:12:28	train epoch 0
2017:03:19 11:12:48	use DataLoaderShakespeare to init data.
2017:03:19 11:12:48	reading and processing the text file.
2017:03:19 11:12:48	preprocess the dataset.
2017:03:19 11:13:06	build a vocabulary.
2017:03:19 11:13:06	...flatmap a list of sentence list to a list of sentence.
2017:03:19 11:13:06	...mapping from index to word.
2017:03:19 11:13:06	...mapping from word to index.
2017:03:19 11:13:06	...map word to index.
2017:03:19 11:13:06	...save processed data to file.
2017:03:19 11:13:07	get data info.
2017:03:19 11:13:07	init batch data.
2017:03:19 11:13:07	...number of batches: 152
2017:03:19 11:13:07	the number of sentence is 9728, the vocab size is 14004
2017:03:19 11:13:11	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderShakespeare/code.model.textG.TextG/1489921991

2017:03:19 11:13:14	------ training ------ 

2017:03:19 11:13:14	train epoch 0
2017:03:19 12:04:05	use DataLoaderBBCV to init data.
2017:03:19 12:04:49	use DataLoaderBBCV to init data.
2017:03:19 12:04:49	reading and processing the text file.
2017:03:19 12:04:49	preprocess the dataset.
2017:03:19 12:04:49	load data.
2017:03:19 12:04:49	init content from raw.
2017:03:19 12:04:49	init data from the raw dataset.
2017:03:19 12:04:50	load context for further preprocessing.
2017:03:19 12:04:51	...build vocab and map word to index.
2017:03:19 12:04:51	build a vocabulary.
2017:03:19 12:04:51	...flatmap a list of sentence list to a list of sentence.
2017:03:19 12:04:52	...mapping from index to word.
2017:03:19 12:04:52	...mapping from word to index.
2017:03:19 12:04:52	...save processed data to file.
2017:03:19 12:04:54	get data info.
2017:03:19 12:04:54	init batch data.
2017:03:19 12:04:54	...number of batches: 589
2017:03:19 12:04:54	num of sentence: 37696, sentence length: 25, vocab size: 34008
2017:03:19 12:04:54	reading and processing the text file.
2017:03:19 12:04:54	preprocess the dataset.
2017:03:19 12:04:54	load data.
2017:03:19 12:04:54	init content from raw.
2017:03:19 12:04:54	init data from the raw dataset.
2017:03:19 12:04:54	load context for further preprocessing.
2017:03:19 12:04:55	...build vocab and map word to index.
2017:03:19 12:04:55	build a vocabulary.
2017:03:19 12:04:55	...flatmap a list of sentence list to a list of sentence.
2017:03:19 12:04:55	...mapping from index to word.
2017:03:19 12:04:55	...mapping from word to index.
2017:03:19 12:04:55	...save processed data to file.
2017:03:19 12:04:57	get data info.
2017:03:19 12:04:57	init batch data.
2017:03:19 12:04:57	...number of batches: 589
2017:03:19 12:04:57	num of sentence: 37696, sentence length: 25, vocab size: 34008
2017:03:19 12:05:02	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBCV/code.model.textG.TextG/1489925102

2017:03:19 12:05:06	------ training ------ 

2017:03:19 12:05:06	train epoch 0
2017:03:19 12:05:42	use DataLoaderBBC to init data.
2017:03:19 12:05:42	reading and processing the text file.
2017:03:19 12:05:42	preprocess the dataset.
2017:03:19 12:05:42	load data.
2017:03:19 12:05:42	init content from raw.
2017:03:19 12:05:42	init data from the raw dataset.
2017:03:19 12:05:43	load context for further preprocessing.
2017:03:19 12:05:43	clean data.
2017:03:19 12:05:44	...mask and pad the sentence.
2017:03:19 12:05:44	......max len:363, median len:22.0, min len:2
2017:03:19 12:05:44	build a vocabulary.
2017:03:19 12:05:44	...flatmap a list of sentence list to a list of sentence.
2017:03:19 12:05:44	...mapping from index to word.
2017:03:19 12:05:44	...mapping from word to index.
2017:03:19 12:05:44	...map word to index.
2017:03:19 12:05:45	...save processed data to file.
2017:03:19 12:05:46	get data info.
2017:03:19 12:05:46	init batch data.
2017:03:19 12:05:46	...number of batches: 390
2017:03:19 12:05:46	num of sentence: 24960, sentence length: 25, vocab size: 23702
2017:03:19 12:05:50	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1489925150

2017:03:19 12:05:52	------ training ------ 

2017:03:19 12:05:52	train epoch 0
2017:03:19 23:07:35	use DataLoaderBBCV to init data.
2017:03:19 23:07:35	reading and processing the text file.
2017:03:19 23:07:35	preprocess the dataset.
2017:03:19 23:07:35	load data.
2017:03:19 23:07:35	load context for further preprocessing.
2017:03:19 23:07:36	...build vocab and map word to index.
2017:03:19 23:07:36	build a vocabulary.
2017:03:19 23:07:36	...flatmap a list of sentence list to a list of sentence.
2017:03:19 23:07:37	...mapping from index to word.
2017:03:19 23:07:37	...mapping from word to index.
2017:03:19 23:07:38	...save processed data to file.
2017:03:19 23:07:40	get data info.
2017:03:19 23:07:40	init batch data.
2017:03:19 23:07:40	...number of batches: 565
2017:03:19 23:07:40	num of sentence: 36160, sentence length: 25, vocab size: 30233
2017:03:19 23:08:10	use DataLoaderBBCV to init data.
2017:03:19 23:08:10	loading preprocessed files.
2017:03:19 23:08:14	get data info.
2017:03:19 23:08:14	init batch data.
2017:03:19 23:08:14	...number of batches: 565
2017:03:19 23:08:14	num of sentence: 36160, sentence length: 25, vocab size: 30233
2017:03:19 23:09:09	use DataLoaderBBCV to init data.
2017:03:19 23:09:09	loading preprocessed files.
2017:03:19 23:09:13	get data info.
2017:03:19 23:09:13	init batch data.
2017:03:19 23:09:13	...number of batches: 565
2017:03:19 23:09:13	num of sentence: 36160, sentence length: 25, vocab size: 30233
2017:03:19 23:09:22	use DataLoaderBBCV to init data.
2017:03:19 23:09:22	loading preprocessed files.
2017:03:19 23:09:26	get data info.
2017:03:19 23:09:26	init batch data.
2017:03:19 23:09:26	...number of batches: 565
2017:03:19 23:09:26	num of sentence: 36160, sentence length: 25, vocab size: 30233
2017:03:19 23:10:17	use DataLoaderBBCV to init data.
2017:03:19 23:10:17	loading preprocessed files.
2017:03:19 23:10:21	get data info.
2017:03:19 23:10:21	init batch data.
2017:03:19 23:10:21	...number of batches: 565
2017:03:19 23:10:21	num of sentence: 36160, sentence length: 25, vocab size: 30233
2017:03:19 23:10:33	use DataLoaderBBCV to init data.
2017:03:19 23:10:33	loading preprocessed files.
2017:03:19 23:10:36	get data info.
2017:03:19 23:10:36	init batch data.
2017:03:19 23:10:36	...number of batches: 565
2017:03:19 23:10:36	num of sentence: 36160, sentence length: 25, vocab size: 30233
2017:03:19 23:10:48	use DataLoaderBBCV to init data.
2017:03:19 23:10:48	loading preprocessed files.
2017:03:19 23:10:51	get data info.
2017:03:19 23:10:51	init batch data.
2017:03:19 23:10:51	...number of batches: 565
2017:03:19 23:10:51	num of sentence: 36160, sentence length: 25, vocab size: 30233
2017:03:19 23:11:44	use DataLoaderBBCV to init data.
2017:03:19 23:11:44	loading preprocessed files.
2017:03:19 23:11:48	get data info.
2017:03:19 23:11:48	init batch data.
2017:03:19 23:11:48	...number of batches: 565
2017:03:19 23:11:48	num of sentence: 36160, sentence length: 25, vocab size: 30233
2017:03:19 23:11:52	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBCV/code.model.textG.TextG/1489965112

2017:03:19 23:11:56	------ training ------ 

2017:03:19 23:11:56	train epoch 0
2017:03:20 11:37:22	use DataLoaderBBCV to init data.
2017:03:20 11:37:22	loading preprocessed files.
2017:03:20 11:37:26	get data info.
2017:03:20 11:37:26	init batch data.
2017:03:20 11:37:26	...number of batches: 565
2017:03:20 11:37:26	num of sentence: 36160, sentence length: 25, vocab size: 30233
2017:03:20 11:37:31	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBCV/code.model.textG.TextG/1490009851

2017:03:20 11:37:34	------ training ------ 

2017:03:20 11:37:34	train epoch 0
2017:03:20 11:37:58	use DataLoaderBBCV to init data.
2017:03:20 11:37:58	loading preprocessed files.
2017:03:20 11:38:01	get data info.
2017:03:20 11:38:01	init batch data.
2017:03:20 11:38:01	...number of batches: 565
2017:03:20 11:38:01	num of sentence: 36160, sentence length: 25, vocab size: 30233
2017:03:20 11:38:06	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBCV/code.model.textG.TextG/1490009885

2017:03:20 11:38:08	------ training ------ 

2017:03:20 11:38:08	train epoch 0
2017:03:20 11:38:36	use DataLoaderBBCV to init data.
2017:03:20 11:38:36	loading preprocessed files.
2017:03:20 11:38:39	get data info.
2017:03:20 11:38:39	init batch data.
2017:03:20 11:38:39	...number of batches: 565
2017:03:20 11:38:39	num of sentence: 36160, sentence length: 25, vocab size: 30233
2017:03:20 11:38:43	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBCV/code.model.textG.TextG/1490009923

2017:03:20 11:38:46	------ training ------ 

2017:03:20 11:38:46	train epoch 0
2017:03:20 11:39:17	use DataLoaderBBCV to init data.
2017:03:20 11:39:17	loading preprocessed files.
2017:03:20 11:39:21	get data info.
2017:03:20 11:39:21	init batch data.
2017:03:20 11:39:21	...number of batches: 565
2017:03:20 11:39:21	num of sentence: 36160, sentence length: 25, vocab size: 30233
2017:03:20 11:39:25	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBCV/code.model.textG.TextG/1490009965

2017:03:20 11:39:28	------ training ------ 

2017:03:20 11:39:28	train epoch 0
2017:03:20 13:47:21	use DataLoaderBBCV to init data.
2017:03:20 13:47:21	loading preprocessed files.
2017:03:20 13:47:25	get data info.
2017:03:20 13:47:25	init batch data.
2017:03:20 13:47:25	...number of batches: 724
2017:03:20 13:47:25	num of sentence: 36200, sentence length: 25, vocab size: 30233
2017:03:20 13:47:30	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBCV/code.model.textG.TextG/1490017649

2017:03:20 13:47:33	------ training ------ 

2017:03:20 13:47:33	train epoch 0
2017:03:20 17:10:53	use DataLoaderBBCV to init data.
2017:03:20 17:10:53	loading preprocessed files.
2017:03:20 17:10:57	get data info.
2017:03:20 17:10:57	init batch data.
2017:03:20 17:10:57	...number of batches: 724
2017:03:20 17:10:57	num of sentence: 36200, sentence length: 25, vocab size: 30233
2017:03:20 17:11:28	use DataLoaderBBCV to init data.
2017:03:20 17:11:28	loading preprocessed files.
2017:03:20 17:11:31	get data info.
2017:03:20 17:11:31	init batch data.
2017:03:20 17:11:31	...number of batches: 724
2017:03:20 17:11:31	num of sentence: 36200, sentence length: 25, vocab size: 30233
2017:03:20 17:12:22	use DataLoaderBBCV to init data.
2017:03:20 17:12:22	loading preprocessed files.
2017:03:20 17:12:25	get data info.
2017:03:20 17:12:25	init batch data.
2017:03:20 17:12:25	...number of batches: 724
2017:03:20 17:12:25	num of sentence: 36200, sentence length: 25, vocab size: 30233
2017:03:20 17:13:16	use DataLoaderBBCV to init data.
2017:03:20 17:13:16	loading preprocessed files.
2017:03:20 17:13:20	get data info.
2017:03:20 17:13:20	init batch data.
2017:03:20 17:13:20	...number of batches: 724
2017:03:20 17:13:20	num of sentence: 36200, sentence length: 25, vocab size: 30233
2017:03:20 17:13:45	use DataLoaderBBCV to init data.
2017:03:20 17:13:45	loading preprocessed files.
2017:03:20 17:13:48	get data info.
2017:03:20 17:13:48	init batch data.
2017:03:20 17:13:48	...number of batches: 724
2017:03:20 17:13:48	num of sentence: 36200, sentence length: 25, vocab size: 30233
2017:03:20 17:13:51	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBCV/code.model.textGV.TextGV/1490030031

2017:03:20 17:13:54	------ training ------ 

2017:03:20 17:13:54	train epoch 0
2017:03:21 09:21:05	use DataLoaderBBC to init data.
2017:03:21 09:21:05	loading preprocessed files.
2017:03:21 09:21:07	get data info.
2017:03:21 09:21:07	init batch data.
2017:03:21 09:21:07	...number of batches: 500
2017:03:21 09:21:07	num of sentence: 25000, sentence length: 25, vocab size: 23702
2017:03:21 09:21:12	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490088072

2017:03:21 09:21:16	------ training ------ 

2017:03:21 09:21:16	train epoch 0
2017:03:21 09:21:34	use DataLoaderBBC to init data.
2017:03:21 09:21:34	reading and processing the text file.
2017:03:21 09:21:34	preprocess the dataset.
2017:03:21 09:21:34	load data.
2017:03:21 09:21:34	init content from raw.
2017:03:21 09:21:34	init data from the raw dataset.
2017:03:21 09:21:36	load context for further preprocessing.
2017:03:21 09:21:36	clean data.
2017:03:21 09:21:36	...mask and pad the sentence.
2017:03:21 09:21:36	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:21:36	build a vocabulary.
2017:03:21 09:21:36	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:28:17	use DataLoaderBBC to init data.
2017:03:21 09:28:17	reading and processing the text file.
2017:03:21 09:28:17	preprocess the dataset.
2017:03:21 09:28:17	load data.
2017:03:21 09:28:17	init content from raw.
2017:03:21 09:28:17	init data from the raw dataset.
2017:03:21 09:28:18	load context for further preprocessing.
2017:03:21 09:28:18	clean data.
2017:03:21 09:29:05	use DataLoaderBBC to init data.
2017:03:21 09:29:05	reading and processing the text file.
2017:03:21 09:29:05	preprocess the dataset.
2017:03:21 09:29:05	load data.
2017:03:21 09:29:05	init content from raw.
2017:03:21 09:29:05	init data from the raw dataset.
2017:03:21 09:29:06	load context for further preprocessing.
2017:03:21 09:29:06	clean data.
2017:03:21 09:29:37	use DataLoaderBBC to init data.
2017:03:21 09:29:37	reading and processing the text file.
2017:03:21 09:29:37	preprocess the dataset.
2017:03:21 09:29:37	load data.
2017:03:21 09:29:37	init content from raw.
2017:03:21 09:29:37	init data from the raw dataset.
2017:03:21 09:29:38	load context for further preprocessing.
2017:03:21 09:29:38	clean data.
2017:03:21 09:30:32	use DataLoaderBBC to init data.
2017:03:21 09:30:32	reading and processing the text file.
2017:03:21 09:30:32	preprocess the dataset.
2017:03:21 09:30:32	load data.
2017:03:21 09:30:32	init content from raw.
2017:03:21 09:30:32	init data from the raw dataset.
2017:03:21 09:30:32	load context for further preprocessing.
2017:03:21 09:30:32	clean data.
2017:03:21 09:30:34	...mask and pad the sentence.
2017:03:21 09:30:34	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:30:34	build a vocabulary.
2017:03:21 09:30:34	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:31:05	use DataLoaderBBC to init data.
2017:03:21 09:31:05	reading and processing the text file.
2017:03:21 09:31:05	preprocess the dataset.
2017:03:21 09:31:05	load data.
2017:03:21 09:31:05	init content from raw.
2017:03:21 09:31:05	init data from the raw dataset.
2017:03:21 09:31:05	load context for further preprocessing.
2017:03:21 09:31:05	clean data.
2017:03:21 09:31:06	...mask and pad the sentence.
2017:03:21 09:31:06	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:31:06	build a vocabulary.
2017:03:21 09:31:06	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:31:18	use DataLoaderBBC to init data.
2017:03:21 09:31:18	reading and processing the text file.
2017:03:21 09:31:18	preprocess the dataset.
2017:03:21 09:31:18	load data.
2017:03:21 09:31:18	init content from raw.
2017:03:21 09:31:18	init data from the raw dataset.
2017:03:21 09:31:18	load context for further preprocessing.
2017:03:21 09:31:18	clean data.
2017:03:21 09:31:19	...mask and pad the sentence.
2017:03:21 09:31:19	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:31:19	build a vocabulary.
2017:03:21 09:31:19	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:31:48	use DataLoaderBBC to init data.
2017:03:21 09:31:48	reading and processing the text file.
2017:03:21 09:31:48	preprocess the dataset.
2017:03:21 09:31:48	load data.
2017:03:21 09:31:48	init content from raw.
2017:03:21 09:31:48	init data from the raw dataset.
2017:03:21 09:31:49	load context for further preprocessing.
2017:03:21 09:31:49	clean data.
2017:03:21 09:31:50	...mask and pad the sentence.
2017:03:21 09:31:50	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:31:50	build a vocabulary.
2017:03:21 09:31:50	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:35:04	use DataLoaderBBC to init data.
2017:03:21 09:35:04	reading and processing the text file.
2017:03:21 09:35:04	preprocess the dataset.
2017:03:21 09:35:04	load data.
2017:03:21 09:35:04	init content from raw.
2017:03:21 09:35:04	init data from the raw dataset.
2017:03:21 09:35:05	load context for further preprocessing.
2017:03:21 09:35:05	clean data.
2017:03:21 09:35:06	...mask and pad the sentence.
2017:03:21 09:35:06	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:35:06	build a vocabulary.
2017:03:21 09:35:06	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:36:08	use DataLoaderBBC to init data.
2017:03:21 09:36:08	reading and processing the text file.
2017:03:21 09:36:08	preprocess the dataset.
2017:03:21 09:36:08	load data.
2017:03:21 09:36:08	init content from raw.
2017:03:21 09:36:08	init data from the raw dataset.
2017:03:21 09:36:09	load context for further preprocessing.
2017:03:21 09:36:09	clean data.
2017:03:21 09:36:10	...mask and pad the sentence.
2017:03:21 09:36:10	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:36:10	build a vocabulary.
2017:03:21 09:36:10	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:37:12	use DataLoaderBBC to init data.
2017:03:21 09:37:12	reading and processing the text file.
2017:03:21 09:37:12	preprocess the dataset.
2017:03:21 09:37:12	load data.
2017:03:21 09:37:12	init content from raw.
2017:03:21 09:37:12	init data from the raw dataset.
2017:03:21 09:37:13	load context for further preprocessing.
2017:03:21 09:37:13	clean data.
2017:03:21 09:37:14	...mask and pad the sentence.
2017:03:21 09:37:14	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:37:14	build a vocabulary.
2017:03:21 09:37:14	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:37:50	use DataLoaderBBC to init data.
2017:03:21 09:37:50	reading and processing the text file.
2017:03:21 09:37:50	preprocess the dataset.
2017:03:21 09:37:50	load data.
2017:03:21 09:37:50	init content from raw.
2017:03:21 09:37:50	init data from the raw dataset.
2017:03:21 09:37:50	load context for further preprocessing.
2017:03:21 09:37:50	clean data.
2017:03:21 09:37:51	...mask and pad the sentence.
2017:03:21 09:37:51	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:37:51	build a vocabulary.
2017:03:21 09:37:51	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:38:17	use DataLoaderBBC to init data.
2017:03:21 09:38:17	reading and processing the text file.
2017:03:21 09:38:17	preprocess the dataset.
2017:03:21 09:38:17	load data.
2017:03:21 09:38:17	init content from raw.
2017:03:21 09:38:17	init data from the raw dataset.
2017:03:21 09:38:17	load context for further preprocessing.
2017:03:21 09:38:17	clean data.
2017:03:21 09:38:18	...mask and pad the sentence.
2017:03:21 09:38:18	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:38:18	build a vocabulary.
2017:03:21 09:38:18	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:38:34	use DataLoaderBBC to init data.
2017:03:21 09:38:34	reading and processing the text file.
2017:03:21 09:38:34	preprocess the dataset.
2017:03:21 09:38:34	load data.
2017:03:21 09:38:34	init content from raw.
2017:03:21 09:38:34	init data from the raw dataset.
2017:03:21 09:38:34	load context for further preprocessing.
2017:03:21 09:38:34	clean data.
2017:03:21 09:38:35	...mask and pad the sentence.
2017:03:21 09:38:35	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:38:35	build a vocabulary.
2017:03:21 09:38:35	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:38:50	use DataLoaderBBC to init data.
2017:03:21 09:38:50	reading and processing the text file.
2017:03:21 09:38:50	preprocess the dataset.
2017:03:21 09:38:50	load data.
2017:03:21 09:38:50	init content from raw.
2017:03:21 09:38:50	init data from the raw dataset.
2017:03:21 09:38:51	load context for further preprocessing.
2017:03:21 09:38:51	clean data.
2017:03:21 09:38:52	...mask and pad the sentence.
2017:03:21 09:38:52	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:38:52	build a vocabulary.
2017:03:21 09:38:52	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:41:18	use DataLoaderBBC to init data.
2017:03:21 09:41:18	reading and processing the text file.
2017:03:21 09:41:18	preprocess the dataset.
2017:03:21 09:41:18	load data.
2017:03:21 09:41:18	init content from raw.
2017:03:21 09:41:18	init data from the raw dataset.
2017:03:21 09:41:20	load context for further preprocessing.
2017:03:21 09:41:20	clean data.
2017:03:21 09:41:21	...mask and pad the sentence.
2017:03:21 09:41:21	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:41:21	build a vocabulary.
2017:03:21 09:41:21	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:42:11	use DataLoaderBBC to init data.
2017:03:21 09:42:11	reading and processing the text file.
2017:03:21 09:42:11	preprocess the dataset.
2017:03:21 09:42:11	load data.
2017:03:21 09:42:11	init content from raw.
2017:03:21 09:42:11	init data from the raw dataset.
2017:03:21 09:42:11	load context for further preprocessing.
2017:03:21 09:42:11	clean data.
2017:03:21 09:42:12	...mask and pad the sentence.
2017:03:21 09:42:12	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:42:12	build a vocabulary.
2017:03:21 09:42:12	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:42:37	use DataLoaderBBC to init data.
2017:03:21 09:42:37	reading and processing the text file.
2017:03:21 09:42:37	preprocess the dataset.
2017:03:21 09:42:37	load data.
2017:03:21 09:42:37	init content from raw.
2017:03:21 09:42:37	init data from the raw dataset.
2017:03:21 09:42:37	load context for further preprocessing.
2017:03:21 09:42:37	clean data.
2017:03:21 09:42:38	...mask and pad the sentence.
2017:03:21 09:42:38	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:42:38	build a vocabulary.
2017:03:21 09:42:38	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:44:08	use DataLoaderBBC to init data.
2017:03:21 09:44:08	loading preprocessed files.
2017:03:21 09:44:11	get data info.
2017:03:21 09:44:11	init batch data.
2017:03:21 09:44:11	...number of batches: 500
2017:03:21 09:44:11	num of sentence: 25000, sentence length: 25, vocab size: 23702
2017:03:21 09:44:15	use DataLoaderBBC to init data.
2017:03:21 09:44:15	reading and processing the text file.
2017:03:21 09:44:15	preprocess the dataset.
2017:03:21 09:44:15	load data.
2017:03:21 09:44:15	init content from raw.
2017:03:21 09:44:15	init data from the raw dataset.
2017:03:21 09:44:17	load context for further preprocessing.
2017:03:21 09:44:17	clean data.
2017:03:21 09:44:18	...mask and pad the sentence.
2017:03:21 09:44:18	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:44:18	build a vocabulary.
2017:03:21 09:44:18	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:44:34	use DataLoaderBBC to init data.
2017:03:21 09:44:34	reading and processing the text file.
2017:03:21 09:44:34	preprocess the dataset.
2017:03:21 09:44:34	load data.
2017:03:21 09:44:34	init content from raw.
2017:03:21 09:44:34	init data from the raw dataset.
2017:03:21 09:44:34	load context for further preprocessing.
2017:03:21 09:44:34	clean data.
2017:03:21 09:44:34	...mask and pad the sentence.
2017:03:21 09:44:34	......max len:26560, median len:1052.0, min len:3
2017:03:21 09:44:34	build a vocabulary.
2017:03:21 09:44:34	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:44:34	...mapping from index to word.
2017:03:21 09:44:34	...mapping from word to index.
2017:03:21 09:44:34	...map word to index.
2017:03:21 09:44:34	...save processed data to file.
2017:03:21 09:44:34	get data info.
2017:03:21 09:44:34	init batch data.
2017:03:21 09:44:34	...number of batches: 0
2017:03:21 09:44:34	num of sentence: 0, sentence length: 25, vocab size: 219
2017:03:21 09:45:22	use DataLoaderBBC to init data.
2017:03:21 09:45:22	reading and processing the text file.
2017:03:21 09:45:22	preprocess the dataset.
2017:03:21 09:45:22	load data.
2017:03:21 09:45:22	init content from raw.
2017:03:21 09:45:22	init data from the raw dataset.
2017:03:21 09:45:23	load context for further preprocessing.
2017:03:21 09:45:23	clean data.
2017:03:21 09:45:24	...mask and pad the sentence.
2017:03:21 09:45:24	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:45:24	build a vocabulary.
2017:03:21 09:45:24	...flatmap a list of sentence list to a list of sentence.
2017:03:21 09:45:30	use DataLoaderBBC to init data.
2017:03:21 09:45:30	reading and processing the text file.
2017:03:21 09:45:30	preprocess the dataset.
2017:03:21 09:45:30	load data.
2017:03:21 09:45:30	init content from raw.
2017:03:21 09:45:30	init data from the raw dataset.
2017:03:21 09:45:31	load context for further preprocessing.
2017:03:21 09:45:31	clean data.
2017:03:21 09:45:31	...mask and pad the sentence.
2017:03:21 09:45:31	......max len:894111, median len:894111.0, min len:894111
2017:03:21 09:45:31	build a vocabulary.
2017:03:21 09:45:31	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:40:57	use DataLoaderBBC to init data.
2017:03:21 12:40:57	reading and processing the text file.
2017:03:21 12:40:57	preprocess the dataset.
2017:03:21 12:40:57	load data.
2017:03:21 12:40:57	init content from raw.
2017:03:21 12:40:57	init data from the raw dataset.
2017:03:21 12:40:58	load context for further preprocessing.
2017:03:21 12:40:58	clean data.
2017:03:21 12:40:59	...mask and pad the sentence.
2017:03:21 12:40:59	......max len:26560, median len:1052.0, min len:3
2017:03:21 12:40:59	build a vocabulary.
2017:03:21 12:40:59	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:40:59	...mapping from index to word.
2017:03:21 12:40:59	...mapping from word to index.
2017:03:21 12:40:59	...map word to index.
2017:03:21 12:40:59	...save processed data to file.
2017:03:21 12:40:59	get data info.
2017:03:21 12:40:59	init batch data.
2017:03:21 12:40:59	...number of batches: 0
2017:03:21 12:40:59	num of sentence: 0, sentence length: 25, vocab size: 219
2017:03:21 12:41:58	use DataLoaderBBC to init data.
2017:03:21 12:41:58	reading and processing the text file.
2017:03:21 12:41:58	preprocess the dataset.
2017:03:21 12:41:58	load data.
2017:03:21 12:41:58	init content from raw.
2017:03:21 12:41:58	init data from the raw dataset.
2017:03:21 12:41:59	load context for further preprocessing.
2017:03:21 12:41:59	clean data.
2017:03:21 12:41:59	...mask and pad the sentence.
2017:03:21 12:41:59	......max len:26560, median len:1052.0, min len:3
2017:03:21 12:41:59	build a vocabulary.
2017:03:21 12:41:59	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:41:59	...mapping from index to word.
2017:03:21 12:41:59	...mapping from word to index.
2017:03:21 12:41:59	...map word to index.
2017:03:21 12:41:59	...save processed data to file.
2017:03:21 12:41:59	get data info.
2017:03:21 12:41:59	init batch data.
2017:03:21 12:41:59	...number of batches: 0
2017:03:21 12:41:59	num of sentence: 0, sentence length: 25, vocab size: 219
2017:03:21 12:42:10	use DataLoaderBBC to init data.
2017:03:21 12:42:10	reading and processing the text file.
2017:03:21 12:42:10	preprocess the dataset.
2017:03:21 12:42:10	load data.
2017:03:21 12:42:10	init content from raw.
2017:03:21 12:42:10	init data from the raw dataset.
2017:03:21 12:42:11	load context for further preprocessing.
2017:03:21 12:42:11	clean data.
2017:03:21 12:42:11	...mask and pad the sentence.
2017:03:21 12:42:11	......max len:26560, median len:1052.0, min len:3
2017:03:21 12:42:11	build a vocabulary.
2017:03:21 12:42:11	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:42:11	...mapping from index to word.
2017:03:21 12:42:11	...mapping from word to index.
2017:03:21 12:42:11	...map word to index.
2017:03:21 12:42:11	...save processed data to file.
2017:03:21 12:42:11	get data info.
2017:03:21 12:42:11	init batch data.
2017:03:21 12:42:11	...number of batches: 0
2017:03:21 12:42:11	num of sentence: 0, sentence length: 25, vocab size: 219
2017:03:21 12:42:24	use DataLoaderBBC to init data.
2017:03:21 12:42:24	loading preprocessed files.
2017:03:21 12:42:24	get data info.
2017:03:21 12:42:24	init batch data.
2017:03:21 12:42:24	...number of batches: 0
2017:03:21 12:42:24	num of sentence: 0, sentence length: 25, vocab size: 219
2017:03:21 12:42:28	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490100148

2017:03:21 12:42:34	use DataLoaderBBC to init data.
2017:03:21 12:42:34	reading and processing the text file.
2017:03:21 12:42:34	preprocess the dataset.
2017:03:21 12:42:34	load data.
2017:03:21 12:42:34	init content from raw.
2017:03:21 12:42:34	init data from the raw dataset.
2017:03:21 12:42:35	load context for further preprocessing.
2017:03:21 12:42:35	clean data.
2017:03:21 12:42:37	...mask and pad the sentence.
2017:03:21 12:42:37	......max len:894111, median len:894111.0, min len:894111
2017:03:21 12:42:37	build a vocabulary.
2017:03:21 12:42:37	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:42:53	use DataLoaderBBC to init data.
2017:03:21 12:42:53	reading and processing the text file.
2017:03:21 12:42:53	preprocess the dataset.
2017:03:21 12:42:53	load data.
2017:03:21 12:42:53	init content from raw.
2017:03:21 12:42:53	init data from the raw dataset.
2017:03:21 12:43:03	load context for further preprocessing.
2017:03:21 12:43:03	clean data.
2017:03:21 12:43:04	...mask and pad the sentence.
2017:03:21 12:43:04	......max len:894111, median len:894111.0, min len:894111
2017:03:21 12:43:04	build a vocabulary.
2017:03:21 12:43:04	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:43:11	use DataLoaderBBC to init data.
2017:03:21 12:43:11	reading and processing the text file.
2017:03:21 12:43:11	preprocess the dataset.
2017:03:21 12:43:11	load data.
2017:03:21 12:43:11	init content from raw.
2017:03:21 12:43:11	init data from the raw dataset.
2017:03:21 12:43:12	load context for further preprocessing.
2017:03:21 12:43:12	clean data.
2017:03:21 12:43:13	...mask and pad the sentence.
2017:03:21 12:43:13	......max len:894111, median len:894111.0, min len:894111
2017:03:21 12:43:13	build a vocabulary.
2017:03:21 12:43:13	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:47:37	use DataLoaderBBC to init data.
2017:03:21 12:47:37	loading preprocessed files.
2017:03:21 12:47:37	get data info.
2017:03:21 12:47:37	init batch data.
2017:03:21 12:47:37	...number of batches: 0
2017:03:21 12:47:37	num of sentence: 0, sentence length: 25, vocab size: 219
2017:03:21 12:47:43	use DataLoaderBBC to init data.
2017:03:21 12:47:43	reading and processing the text file.
2017:03:21 12:47:43	preprocess the dataset.
2017:03:21 12:47:43	load data.
2017:03:21 12:47:43	init content from raw.
2017:03:21 12:47:43	init data from the raw dataset.
2017:03:21 12:47:44	load context for further preprocessing.
2017:03:21 12:47:44	clean data.
2017:03:21 12:47:44	...mask and pad the sentence.
2017:03:21 12:47:44	......max len:26560, median len:1052.0, min len:3
2017:03:21 12:47:44	build a vocabulary.
2017:03:21 12:47:44	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:47:44	...mapping from index to word.
2017:03:21 12:47:44	...mapping from word to index.
2017:03:21 12:47:44	...map word to index.
2017:03:21 12:47:44	...save processed data to file.
2017:03:21 12:47:44	get data info.
2017:03:21 12:47:44	init batch data.
2017:03:21 12:47:44	...number of batches: 0
2017:03:21 12:47:44	num of sentence: 0, sentence length: 25, vocab size: 219
2017:03:21 12:48:29	use DataLoaderBBC to init data.
2017:03:21 12:48:29	reading and processing the text file.
2017:03:21 12:48:29	preprocess the dataset.
2017:03:21 12:48:29	load data.
2017:03:21 12:48:29	init content from raw.
2017:03:21 12:48:29	init data from the raw dataset.
2017:03:21 12:48:30	load context for further preprocessing.
2017:03:21 12:48:30	clean data.
2017:03:21 12:48:31	...mask and pad the sentence.
2017:03:21 12:48:31	......max len:26560, median len:1052.0, min len:3
2017:03:21 12:48:31	build a vocabulary.
2017:03:21 12:48:31	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:48:31	...mapping from index to word.
2017:03:21 12:48:31	...mapping from word to index.
2017:03:21 12:48:31	...map word to index.
2017:03:21 12:48:31	...save processed data to file.
2017:03:21 12:48:31	get data info.
2017:03:21 12:48:31	init batch data.
2017:03:21 12:48:31	...number of batches: 0
2017:03:21 12:48:31	num of sentence: 0, sentence length: 25, vocab size: 219
2017:03:21 12:48:36	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490100516

2017:03:21 12:48:40	use DataLoaderBBC to init data.
2017:03:21 12:48:40	loading preprocessed files.
2017:03:21 12:48:40	get data info.
2017:03:21 12:48:40	init batch data.
2017:03:21 12:48:40	...number of batches: 0
2017:03:21 12:48:40	num of sentence: 0, sentence length: 25, vocab size: 219
2017:03:21 12:48:45	use DataLoaderBBC to init data.
2017:03:21 12:48:45	reading and processing the text file.
2017:03:21 12:48:45	preprocess the dataset.
2017:03:21 12:48:45	load data.
2017:03:21 12:48:45	init content from raw.
2017:03:21 12:48:45	init data from the raw dataset.
2017:03:21 12:48:47	load context for further preprocessing.
2017:03:21 12:48:47	clean data.
2017:03:21 12:48:47	...mask and pad the sentence.
2017:03:21 12:48:47	......max len:26560, median len:1052.0, min len:3
2017:03:21 12:48:47	build a vocabulary.
2017:03:21 12:48:47	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:48:47	...mapping from index to word.
2017:03:21 12:48:47	...mapping from word to index.
2017:03:21 12:48:47	...map word to index.
2017:03:21 12:48:47	...save processed data to file.
2017:03:21 12:48:47	get data info.
2017:03:21 12:48:47	init batch data.
2017:03:21 12:48:47	...number of batches: 0
2017:03:21 12:48:47	num of sentence: 0, sentence length: 25, vocab size: 219
2017:03:21 12:49:13	use DataLoaderBBC to init data.
2017:03:21 12:49:13	reading and processing the text file.
2017:03:21 12:49:13	preprocess the dataset.
2017:03:21 12:49:13	load data.
2017:03:21 12:49:13	init content from raw.
2017:03:21 12:49:13	init data from the raw dataset.
2017:03:21 12:49:15	load context for further preprocessing.
2017:03:21 12:49:15	clean data.
2017:03:21 12:49:16	...mask and pad the sentence.
2017:03:21 12:49:16	......max len:26560, median len:1052.0, min len:3
2017:03:21 12:49:16	build a vocabulary.
2017:03:21 12:49:16	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:49:16	...mapping from index to word.
2017:03:21 12:49:16	...mapping from word to index.
2017:03:21 12:49:16	...map word to index.
2017:03:21 12:49:16	...save processed data to file.
2017:03:21 12:49:16	get data info.
2017:03:21 12:49:16	init batch data.
2017:03:21 12:49:16	...number of batches: 0
2017:03:21 12:49:16	num of sentence: 0, sentence length: 25, vocab size: 219
2017:03:21 12:49:36	use DataLoaderBBC to init data.
2017:03:21 12:49:36	reading and processing the text file.
2017:03:21 12:49:36	preprocess the dataset.
2017:03:21 12:49:36	load data.
2017:03:21 12:49:36	init content from raw.
2017:03:21 12:49:36	init data from the raw dataset.
2017:03:21 12:49:44	load context for further preprocessing.
2017:03:21 12:49:44	clean data.
2017:03:21 12:49:44	...mask and pad the sentence.
2017:03:21 12:49:44	......max len:26560, median len:1052.0, min len:3
2017:03:21 12:49:44	build a vocabulary.
2017:03:21 12:49:44	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:49:44	...mapping from index to word.
2017:03:21 12:49:44	...mapping from word to index.
2017:03:21 12:49:44	...map word to index.
2017:03:21 12:49:44	...save processed data to file.
2017:03:21 12:49:44	get data info.
2017:03:21 12:49:44	init batch data.
2017:03:21 12:49:44	...number of batches: 0
2017:03:21 12:49:44	num of sentence: 0, sentence length: 25, vocab size: 219
2017:03:21 12:50:01	use DataLoaderBBC to init data.
2017:03:21 12:50:01	reading and processing the text file.
2017:03:21 12:50:01	preprocess the dataset.
2017:03:21 12:50:01	load data.
2017:03:21 12:50:01	init content from raw.
2017:03:21 12:50:01	init data from the raw dataset.
2017:03:21 12:50:03	load context for further preprocessing.
2017:03:21 12:50:03	clean data.
2017:03:21 12:50:16	use DataLoaderBBC to init data.
2017:03:21 12:50:16	reading and processing the text file.
2017:03:21 12:50:16	preprocess the dataset.
2017:03:21 12:50:16	load data.
2017:03:21 12:50:16	init content from raw.
2017:03:21 12:50:16	init data from the raw dataset.
2017:03:21 12:50:18	load context for further preprocessing.
2017:03:21 12:50:18	clean data.
2017:03:21 12:50:19	...mask and pad the sentence.
2017:03:21 12:50:19	......max len:359, median len:22.0, min len:2
2017:03:21 12:50:19	build a vocabulary.
2017:03:21 12:50:19	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:50:19	...mapping from index to word.
2017:03:21 12:50:28	use DataLoaderBBC to init data.
2017:03:21 12:50:28	reading and processing the text file.
2017:03:21 12:50:28	preprocess the dataset.
2017:03:21 12:50:28	load data.
2017:03:21 12:50:28	init content from raw.
2017:03:21 12:50:28	init data from the raw dataset.
2017:03:21 12:50:28	load context for further preprocessing.
2017:03:21 12:50:28	clean data.
2017:03:21 12:50:28	...mask and pad the sentence.
2017:03:21 12:50:28	......max len:359, median len:22.0, min len:2
2017:03:21 12:50:29	build a vocabulary.
2017:03:21 12:50:29	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:50:29	...mapping from index to word.
2017:03:21 12:50:29	...mapping from word to index.
2017:03:21 12:50:29	...map word to index.
2017:03:21 12:50:29	...save processed data to file.
2017:03:21 12:50:46	use DataLoaderBBC to init data.
2017:03:21 12:50:46	reading and processing the text file.
2017:03:21 12:50:46	preprocess the dataset.
2017:03:21 12:50:46	load data.
2017:03:21 12:50:46	init content from raw.
2017:03:21 12:50:46	init data from the raw dataset.
2017:03:21 12:50:47	load context for further preprocessing.
2017:03:21 12:50:47	clean data.
2017:03:21 12:50:47	...mask and pad the sentence.
2017:03:21 12:50:47	......max len:359, median len:22.0, min len:2
2017:03:21 12:50:47	build a vocabulary.
2017:03:21 12:50:47	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:50:48	...mapping from index to word.
2017:03:21 12:50:48	...mapping from word to index.
2017:03:21 12:50:48	...map word to index.
2017:03:21 12:50:48	...save processed data to file.
2017:03:21 12:50:49	get data info.
2017:03:21 12:50:49	init batch data.
2017:03:21 12:50:49	...number of batches: 503
2017:03:21 12:50:49	num of sentence: 25150, sentence length: 25, vocab size: 26991
2017:03:21 12:52:48	use DataLoaderBBC to init data.
2017:03:21 12:52:48	reading and processing the text file.
2017:03:21 12:52:48	preprocess the dataset.
2017:03:21 12:52:48	load data.
2017:03:21 12:52:48	init content from raw.
2017:03:21 12:52:48	init data from the raw dataset.
2017:03:21 12:52:48	load context for further preprocessing.
2017:03:21 12:52:48	clean data.
2017:03:21 12:52:49	...mask and pad the sentence.
2017:03:21 12:52:49	......max len:359, median len:22.0, min len:2
2017:03:21 12:52:49	build a vocabulary.
2017:03:21 12:52:49	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:52:50	...mapping from index to word.
2017:03:21 12:52:50	...mapping from word to index.
2017:03:21 12:52:50	...map word to index.
2017:03:21 12:52:50	...save processed data to file.
2017:03:21 12:52:51	get data info.
2017:03:21 12:52:51	init batch data.
2017:03:21 12:52:51	...number of batches: 402
2017:03:21 12:52:51	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:21 12:53:46	use DataLoaderBBC to init data.
2017:03:21 12:53:46	reading and processing the text file.
2017:03:21 12:53:46	preprocess the dataset.
2017:03:21 12:53:46	load data.
2017:03:21 12:53:46	init content from raw.
2017:03:21 12:53:46	init data from the raw dataset.
2017:03:21 12:53:46	load context for further preprocessing.
2017:03:21 12:53:46	clean data.
2017:03:21 12:53:47	...mask and pad the sentence.
2017:03:21 12:53:47	......max len:359, median len:22.0, min len:2
2017:03:21 12:53:47	......filter sentence and bound them in the range of 25
2017:03:21 12:53:47	build a vocabulary.
2017:03:21 12:53:47	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:53:47	...mapping from index to word.
2017:03:21 12:53:47	...mapping from word to index.
2017:03:21 12:53:47	...map word to index.
2017:03:21 12:53:48	...save processed data to file.
2017:03:21 12:53:49	get data info.
2017:03:21 12:53:49	init batch data.
2017:03:21 12:53:49	...number of batches: 402
2017:03:21 12:53:49	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:21 12:53:53	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490100833

2017:03:21 12:53:57	------ training ------ 

2017:03:21 12:53:57	train epoch 0
2017:03:21 12:55:25	use DataLoaderBBC to init data.
2017:03:21 12:55:25	reading and processing the text file.
2017:03:21 12:55:25	preprocess the dataset.
2017:03:21 12:55:25	load data.
2017:03:21 12:55:25	init content from raw.
2017:03:21 12:55:25	init data from the raw dataset.
2017:03:21 12:55:27	load context for further preprocessing.
2017:03:21 12:55:27	clean data.
2017:03:21 12:55:28	...mask and pad the sentence.
2017:03:21 12:55:28	......max len:359, median len:22.0, min len:2
2017:03:21 12:55:28	......filter sentence and bound them in the range of 25.
2017:03:21 12:55:28	build a vocabulary.
2017:03:21 12:55:28	...flatmap a list of sentence list to a list of sentence.
2017:03:21 12:55:28	...mapping from index to word.
2017:03:21 12:55:28	...mapping from word to index.
2017:03:21 12:55:28	...map word to index.
2017:03:21 12:55:28	...save processed data to file.
2017:03:21 12:55:29	get data info.
2017:03:21 12:55:29	init batch data.
2017:03:21 12:55:29	...number of batches: 402
2017:03:21 12:55:29	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:21 12:55:33	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490100933

2017:03:21 12:55:37	------ training ------ 

2017:03:21 12:55:37	train epoch 0
2017:03:21 12:55:59	use DataLoaderBBC to init data.
2017:03:21 12:55:59	loading preprocessed files.
2017:03:21 12:56:01	get data info.
2017:03:21 12:56:01	init batch data.
2017:03:21 12:56:01	...number of batches: 402
2017:03:21 12:56:01	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:21 12:56:05	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490100965

2017:03:21 12:56:07	------ training ------ 

2017:03:21 12:56:07	train epoch 0
2017:03:21 13:29:12	use DataLoaderBBC to init data.
2017:03:21 13:29:12	loading preprocessed files.
2017:03:21 13:29:14	get data info.
2017:03:21 13:29:14	init batch data.
2017:03:21 13:29:14	...number of batches: 402
2017:03:21 13:29:14	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:21 13:29:19	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490102959

2017:03:21 13:29:22	------ training ------ 

2017:03:21 13:29:22	train epoch 0
2017:03:21 13:29:38	use DataLoaderBBC to init data.
2017:03:21 13:29:38	reading and processing the text file.
2017:03:21 13:29:38	preprocess the dataset.
2017:03:21 13:29:38	load data.
2017:03:21 13:29:38	init content from raw.
2017:03:21 13:29:38	init data from the raw dataset.
2017:03:21 13:29:39	load context for further preprocessing.
2017:03:21 13:29:39	clean data.
2017:03:21 13:29:41	...mask and pad the sentence.
2017:03:21 13:29:41	......max len:894111, median len:894111.0, min len:894111
2017:03:21 13:29:41	......filter sentence and bound them in the range of 25.
2017:03:21 13:29:41	build a vocabulary.
2017:03:21 13:29:41	...flatmap a list of sentence list to a list of sentence.
2017:03:21 13:30:51	use DataLoaderBBC to init data.
2017:03:21 13:30:51	reading and processing the text file.
2017:03:21 13:30:51	preprocess the dataset.
2017:03:21 13:30:51	load data.
2017:03:21 13:30:51	init content from raw.
2017:03:21 13:30:51	init data from the raw dataset.
2017:03:21 13:30:51	load context for further preprocessing.
2017:03:21 13:30:51	clean data.
2017:03:21 13:30:52	...mask and pad the sentence.
2017:03:21 13:30:52	......max len:363, median len:22.0, min len:2
2017:03:21 13:30:52	......filter sentence and bound them in the range of 25.
2017:03:21 13:30:52	build a vocabulary.
2017:03:21 13:30:52	...flatmap a list of sentence list to a list of sentence.
2017:03:21 13:30:52	...mapping from index to word.
2017:03:21 13:30:52	...mapping from word to index.
2017:03:21 13:30:52	...map word to index.
2017:03:21 13:30:52	...save processed data to file.
2017:03:21 13:30:53	get data info.
2017:03:21 13:30:53	init batch data.
2017:03:21 13:30:53	...number of batches: 396
2017:03:21 13:30:53	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 13:30:57	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103057

2017:03:21 13:31:00	------ training ------ 

2017:03:21 13:31:00	train epoch 0
2017:03:21 13:32:15	use DataLoaderBBC to init data.
2017:03:21 13:32:15	loading preprocessed files.
2017:03:21 13:32:17	get data info.
2017:03:21 13:32:17	init batch data.
2017:03:21 13:32:17	...number of batches: 396
2017:03:21 13:32:17	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 13:32:23	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 13:32:28	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142/checkpoints/bestmodel.

2017:03:21 13:32:28	------ training ------ 

2017:03:21 13:32:28	train epoch 0
2017:03:21 13:48:43	use DataLoaderBBC to init data.
2017:03:21 13:48:43	loading preprocessed files.
2017:03:21 13:48:45	get data info.
2017:03:21 13:48:45	init batch data.
2017:03:21 13:48:45	...number of batches: 396
2017:03:21 13:48:45	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 13:48:46	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 13:48:48	above content is generated from lstm.
2017:03:21 13:48:48	total execution time: 5
2017:03:21 13:50:00	use DataLoaderBBC to init data.
2017:03:21 13:50:00	loading preprocessed files.
2017:03:21 13:50:02	get data info.
2017:03:21 13:50:02	init batch data.
2017:03:21 13:50:02	...number of batches: 396
2017:03:21 13:50:02	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 13:50:03	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 13:50:05	above content is generated from lstm.
2017:03:21 13:50:05	total execution time: 4
2017:03:21 13:50:24	use DataLoaderBBC to init data.
2017:03:21 13:50:24	loading preprocessed files.
2017:03:21 13:50:25	get data info.
2017:03:21 13:50:25	init batch data.
2017:03:21 13:50:25	...number of batches: 396
2017:03:21 13:50:25	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 13:50:26	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 13:50:28	generate sentence. is beam search: True?
2017:03:21 13:52:18	use DataLoaderBBC to init data.
2017:03:21 13:52:18	loading preprocessed files.
2017:03:21 13:52:20	get data info.
2017:03:21 13:52:20	init batch data.
2017:03:21 13:52:20	...number of batches: 396
2017:03:21 13:52:20	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 13:52:21	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 13:52:22	generate sentence. is beam search: True.
2017:03:21 13:55:46	use DataLoaderBBC to init data.
2017:03:21 13:55:46	loading preprocessed files.
2017:03:21 13:55:47	get data info.
2017:03:21 13:55:47	init batch data.
2017:03:21 13:55:47	...number of batches: 396
2017:03:21 13:55:47	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 13:55:48	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 13:56:17	use DataLoaderBBC to init data.
2017:03:21 13:56:17	loading preprocessed files.
2017:03:21 13:56:19	get data info.
2017:03:21 13:56:19	init batch data.
2017:03:21 13:56:19	...number of batches: 396
2017:03:21 13:56:19	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 13:56:20	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 13:56:28	use DataLoaderBBC to init data.
2017:03:21 13:56:28	loading preprocessed files.
2017:03:21 13:56:30	get data info.
2017:03:21 13:56:30	init batch data.
2017:03:21 13:56:30	...number of batches: 396
2017:03:21 13:56:30	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 13:56:31	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 13:56:31	generate sentence. is beam search: True.
2017:03:21 13:58:08	use DataLoaderBBC to init data.
2017:03:21 13:58:08	loading preprocessed files.
2017:03:21 13:58:10	get data info.
2017:03:21 13:58:10	init batch data.
2017:03:21 13:58:10	...number of batches: 396
2017:03:21 13:58:10	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 13:58:11	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 13:58:11	generate sentence. is beam search: True.
2017:03:21 14:00:01	use DataLoaderBBC to init data.
2017:03:21 14:00:01	loading preprocessed files.
2017:03:21 14:00:02	get data info.
2017:03:21 14:00:02	init batch data.
2017:03:21 14:00:02	...number of batches: 396
2017:03:21 14:00:02	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:00:03	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:00:04	generate sentence. is beam search: True.
2017:03:21 14:00:36	use DataLoaderBBC to init data.
2017:03:21 14:00:36	loading preprocessed files.
2017:03:21 14:00:37	get data info.
2017:03:21 14:00:37	init batch data.
2017:03:21 14:00:37	...number of batches: 396
2017:03:21 14:00:37	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:00:38	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:00:39	generate sentence. is beam search: True.
2017:03:21 14:03:31	use DataLoaderBBC to init data.
2017:03:21 14:03:31	loading preprocessed files.
2017:03:21 14:03:32	get data info.
2017:03:21 14:03:32	init batch data.
2017:03:21 14:03:32	...number of batches: 396
2017:03:21 14:03:32	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:03:33	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:03:34	generate sentence. is beam search: True.
2017:03:21 14:03:58	use DataLoaderBBC to init data.
2017:03:21 14:03:58	loading preprocessed files.
2017:03:21 14:04:00	get data info.
2017:03:21 14:04:00	init batch data.
2017:03:21 14:04:00	...number of batches: 396
2017:03:21 14:04:00	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:04:01	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:04:11	use DataLoaderBBC to init data.
2017:03:21 14:04:11	loading preprocessed files.
2017:03:21 14:04:13	get data info.
2017:03:21 14:04:13	init batch data.
2017:03:21 14:04:13	...number of batches: 396
2017:03:21 14:04:13	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:04:14	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:04:14	generate sentence. is beam search: True.
2017:03:21 14:04:14	above content is generated from lstm.
2017:03:21 14:04:14	total execution time: 3
2017:03:21 14:06:45	use DataLoaderBBC to init data.
2017:03:21 14:06:45	loading preprocessed files.
2017:03:21 14:06:47	get data info.
2017:03:21 14:06:47	init batch data.
2017:03:21 14:06:47	...number of batches: 396
2017:03:21 14:06:47	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:06:49	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:06:50	generate sentence. is beam search: True.
2017:03:21 14:06:50	above content is generated from lstm.
2017:03:21 14:06:50	total execution time: 4
2017:03:21 14:08:42	use DataLoaderBBC to init data.
2017:03:21 14:08:42	loading preprocessed files.
2017:03:21 14:08:44	get data info.
2017:03:21 14:08:44	init batch data.
2017:03:21 14:08:44	...number of batches: 396
2017:03:21 14:08:44	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:08:45	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:08:45	generate sentence. is beam search: True.
2017:03:21 14:12:40	use DataLoaderBBC to init data.
2017:03:21 14:12:40	loading preprocessed files.
2017:03:21 14:12:41	get data info.
2017:03:21 14:12:41	init batch data.
2017:03:21 14:12:41	...number of batches: 396
2017:03:21 14:12:42	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:12:42	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:12:43	generate sentence. is beam search: True.
2017:03:21 14:12:43	...process the given sentence.
2017:03:21 14:13:09	use DataLoaderBBC to init data.
2017:03:21 14:13:09	loading preprocessed files.
2017:03:21 14:13:11	get data info.
2017:03:21 14:13:11	init batch data.
2017:03:21 14:13:11	...number of batches: 396
2017:03:21 14:13:11	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:13:12	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:13:13	generate sentence. is beam search: True.
2017:03:21 14:13:13	...process the given sentence.
2017:03:21 14:16:39	use DataLoaderBBC to init data.
2017:03:21 14:16:39	loading preprocessed files.
2017:03:21 14:16:41	get data info.
2017:03:21 14:16:41	init batch data.
2017:03:21 14:16:41	...number of batches: 396
2017:03:21 14:16:41	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:16:42	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:16:44	generate sentence. is beam search: True.
2017:03:21 14:16:44	...process the given sentence.
2017:03:21 14:18:22	use DataLoaderBBC to init data.
2017:03:21 14:18:22	loading preprocessed files.
2017:03:21 14:18:24	get data info.
2017:03:21 14:18:24	init batch data.
2017:03:21 14:18:24	...number of batches: 396
2017:03:21 14:18:24	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:18:25	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:18:26	generate sentence. is beam search: True.
2017:03:21 14:18:26	...process the given sentence.
2017:03:21 14:19:03	use DataLoaderBBC to init data.
2017:03:21 14:19:03	loading preprocessed files.
2017:03:21 14:19:05	get data info.
2017:03:21 14:19:05	init batch data.
2017:03:21 14:19:05	...number of batches: 396
2017:03:21 14:19:05	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:19:06	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:19:07	generate sentence. is beam search: True.
2017:03:21 14:19:07	...process the given sentence.
2017:03:21 14:19:25	use DataLoaderBBC to init data.
2017:03:21 14:19:25	loading preprocessed files.
2017:03:21 14:19:27	get data info.
2017:03:21 14:19:27	init batch data.
2017:03:21 14:19:27	...number of batches: 396
2017:03:21 14:19:27	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:19:28	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:19:30	generate sentence. is beam search: True.
2017:03:21 14:19:30	...process the given sentence.
2017:03:21 14:20:39	use DataLoaderBBC to init data.
2017:03:21 14:20:39	loading preprocessed files.
2017:03:21 14:20:41	get data info.
2017:03:21 14:20:41	init batch data.
2017:03:21 14:20:41	...number of batches: 396
2017:03:21 14:20:41	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:20:42	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:20:44	generate sentence. is beam search: True.
2017:03:21 14:20:44	...process the given sentence.
2017:03:21 14:21:14	use DataLoaderBBC to init data.
2017:03:21 14:21:14	loading preprocessed files.
2017:03:21 14:21:16	get data info.
2017:03:21 14:21:16	init batch data.
2017:03:21 14:21:16	...number of batches: 396
2017:03:21 14:21:16	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:21:17	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:21:18	generate sentence. is beam search: True.
2017:03:21 14:21:18	...process the given sentence.
2017:03:21 14:21:38	use DataLoaderBBC to init data.
2017:03:21 14:21:38	loading preprocessed files.
2017:03:21 14:21:40	get data info.
2017:03:21 14:21:40	init batch data.
2017:03:21 14:21:40	...number of batches: 396
2017:03:21 14:21:40	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:21:41	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:21:43	generate sentence. is beam search: True.
2017:03:21 14:21:43	...process the given sentence.
2017:03:21 14:21:58	use DataLoaderBBC to init data.
2017:03:21 14:21:58	loading preprocessed files.
2017:03:21 14:22:00	get data info.
2017:03:21 14:22:00	init batch data.
2017:03:21 14:22:00	...number of batches: 396
2017:03:21 14:22:00	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:22:01	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:22:03	generate sentence. is beam search: True.
2017:03:21 14:22:03	...process the given sentence.
2017:03:21 14:22:30	use DataLoaderBBC to init data.
2017:03:21 14:22:30	loading preprocessed files.
2017:03:21 14:22:32	get data info.
2017:03:21 14:22:32	init batch data.
2017:03:21 14:22:32	...number of batches: 396
2017:03:21 14:22:32	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:22:33	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:22:34	generate sentence. is beam search: True.
2017:03:21 14:22:34	...process the given sentence.
2017:03:21 14:22:35	above content is generated from lstm.
2017:03:21 14:22:35	total execution time: 5
2017:03:21 14:23:24	use DataLoaderBBC to init data.
2017:03:21 14:23:24	loading preprocessed files.
2017:03:21 14:23:26	get data info.
2017:03:21 14:23:26	init batch data.
2017:03:21 14:23:26	...number of batches: 396
2017:03:21 14:23:26	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:23:27	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:23:28	generate sentence. is beam search: True.
2017:03:21 14:23:28	...process the given sentence.
2017:03:21 14:23:29	above content is generated from lstm.
2017:03:21 14:23:29	total execution time: 5
2017:03:21 14:27:53	use DataLoaderBBC to init data.
2017:03:21 14:27:53	loading preprocessed files.
2017:03:21 14:27:55	get data info.
2017:03:21 14:27:55	init batch data.
2017:03:21 14:27:55	...number of batches: 396
2017:03:21 14:27:55	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:27:56	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:27:57	generate sentence. is beam search: True.
2017:03:21 14:27:57	...process the given sentence.
2017:03:21 14:27:58	above content is generated from lstm.
2017:03:21 14:27:58	total execution time: 5
2017:03:21 14:28:17	use DataLoaderBBC to init data.
2017:03:21 14:28:17	loading preprocessed files.
2017:03:21 14:28:19	get data info.
2017:03:21 14:28:19	init batch data.
2017:03:21 14:28:19	...number of batches: 396
2017:03:21 14:28:19	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:28:19	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:28:21	generate sentence. is beam search: True.
2017:03:21 14:28:21	...process the given sentence.
2017:03:21 14:28:22	above content is generated from lstm.
2017:03:21 14:28:22	total execution time: 5
2017:03:21 14:28:44	use DataLoaderBBC to init data.
2017:03:21 14:28:44	loading preprocessed files.
2017:03:21 14:28:46	get data info.
2017:03:21 14:28:46	init batch data.
2017:03:21 14:28:46	...number of batches: 396
2017:03:21 14:28:46	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:28:47	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:28:48	generate sentence. is beam search: False.
2017:03:21 14:28:48	...process the given sentence.
2017:03:21 14:28:48	above content is generated from lstm.
2017:03:21 14:28:48	total execution time: 3
2017:03:21 14:29:30	use DataLoaderBBC to init data.
2017:03:21 14:29:30	loading preprocessed files.
2017:03:21 14:29:32	get data info.
2017:03:21 14:29:32	init batch data.
2017:03:21 14:29:32	...number of batches: 396
2017:03:21 14:29:32	num of sentence: 19800, sentence length: 25, vocab size: 23976
2017:03:21 14:29:32	writing to /home/tlin/notebooks/code/demo2_v/data/training/runs/DataLoaderBBC/code.model.textG.TextG/1490103142

2017:03:21 14:29:33	generate sentence. is beam search: False.
2017:03:21 14:29:33	...process the given sentence.
2017:03:21 14:29:34	above content is generated from lstm.
2017:03:21 14:29:34	total execution time: 4
2017:03:21 22:06:25	use DataLoaderBBCV to init data.
2017:03:21 22:06:25	loading preprocessed files.
2017:03:21 22:06:29	get data info.
2017:03:21 22:06:29	init batch data.
2017:03:21 22:06:29	...number of batches: 724
2017:03:21 22:06:29	num of sentence: 36200, sentence length: 25, vocab size: 30233
2017:03:21 22:07:53	use DataLoaderBBCV to init data.
2017:03:21 22:07:53	loading preprocessed files.
2017:03:21 22:07:57	get data info.
2017:03:21 22:07:57	init batch data.
2017:03:21 22:07:57	...number of batches: 724
2017:03:21 22:07:57	num of sentence: 36200, sentence length: 25, vocab size: 30233
2017:03:21 22:08:24	use DataLoaderBBCV to init data.
2017:03:21 22:08:24	loading preprocessed files.
2017:03:21 22:08:28	get data info.
2017:03:21 22:08:28	init batch data.
2017:03:21 22:08:28	...number of batches: 724
2017:03:21 22:08:28	num of sentence: 36200, sentence length: 25, vocab size: 30233
2017:03:21 22:56:31	use DataLoaderBBCV to init data.
2017:03:21 22:56:32	loading preprocessed files.
2017:03:21 22:56:35	get data info.
2017:03:21 22:56:35	init batch data.
2017:03:21 22:56:35	...number of batches: 724
2017:03:21 22:56:35	num of sentence: 36200, sentence length: 25, vocab size: 30233
2017:03:21 23:02:01	use DataLoaderBBCV to init data.
2017:03:21 23:02:01	reading and processing the text file.
2017:03:21 23:02:01	preprocess the dataset.
2017:03:21 23:02:01	load data.
2017:03:21 23:02:01	init content from raw.
2017:03:21 23:02:01	init data from the raw dataset.
2017:03:21 23:02:02	load context for further preprocessing.
2017:03:21 23:02:02	...build vocab and map word to index.
2017:03:21 23:02:02	build a vocabulary.
2017:03:21 23:02:02	...flatmap a list of sentence list to a list of sentence.
2017:03:21 23:02:03	...mapping from index to word.
2017:03:21 23:02:03	...add additional <go> and <eos>.
2017:03:21 23:02:03	...mapping from word to index.
2017:03:21 23:02:03	...save processed data to file.
2017:03:21 23:02:05	get data info.
2017:03:21 23:02:05	init batch data.
2017:03:21 23:02:05	...number of batches: 674
2017:03:21 23:02:05	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:21 23:05:46	use DataLoaderBBCV to init data.
2017:03:21 23:05:46	reading and processing the text file.
2017:03:21 23:05:46	preprocess the dataset.
2017:03:21 23:05:46	load data.
2017:03:21 23:05:46	init content from raw.
2017:03:21 23:05:46	init data from the raw dataset.
2017:03:21 23:05:48	load context for further preprocessing.
2017:03:21 23:05:48	...build vocab and map word to index.
2017:03:21 23:05:48	build a vocabulary.
2017:03:21 23:05:48	...flatmap a list of sentence list to a list of sentence.
2017:03:21 23:05:48	...mapping from index to word.
2017:03:21 23:05:49	...add additional <go> and <eos>.
2017:03:21 23:05:49	...mapping from word to index.
2017:03:21 23:05:49	...save processed data to file.
2017:03:21 23:05:51	get data info.
2017:03:21 23:05:51	init batch data.
2017:03:21 23:05:51	...number of batches: 674
2017:03:21 23:05:51	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:21 23:05:55	use DataLoaderBBCV to init data.
2017:03:21 23:05:55	loading preprocessed files.
2017:03:21 23:05:58	get data info.
2017:03:21 23:05:58	init batch data.
2017:03:21 23:05:58	...number of batches: 674
2017:03:21 23:05:58	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:21 23:06:03	use DataLoaderBBCV to init data.
2017:03:21 23:06:03	loading preprocessed files.
2017:03:21 23:06:06	get data info.
2017:03:21 23:06:06	init batch data.
2017:03:21 23:06:06	...number of batches: 674
2017:03:21 23:06:06	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:21 23:06:27	use DataLoaderBBCV to init data.
2017:03:21 23:06:27	loading preprocessed files.
2017:03:21 23:06:30	get data info.
2017:03:21 23:06:30	init batch data.
2017:03:21 23:06:30	...number of batches: 674
2017:03:21 23:06:30	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:21 23:07:17	use DataLoaderBBCV to init data.
2017:03:21 23:07:17	loading preprocessed files.
2017:03:21 23:07:20	get data info.
2017:03:21 23:07:20	init batch data.
2017:03:21 23:07:20	...number of batches: 674
2017:03:21 23:07:20	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:21 23:08:31	use DataLoaderBBCV to init data.
2017:03:21 23:08:31	loading preprocessed files.
2017:03:21 23:08:34	get data info.
2017:03:21 23:08:34	init batch data.
2017:03:21 23:08:34	...number of batches: 674
2017:03:21 23:08:34	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:21 23:09:57	use DataLoaderBBCV to init data.
2017:03:21 23:09:57	loading preprocessed files.
2017:03:21 23:10:00	get data info.
2017:03:21 23:10:00	init batch data.
2017:03:21 23:10:00	...number of batches: 674
2017:03:21 23:10:00	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:21 23:11:46	use DataLoaderBBCV to init data.
2017:03:21 23:11:46	loading preprocessed files.
2017:03:21 23:11:50	get data info.
2017:03:21 23:11:50	init batch data.
2017:03:21 23:11:50	...number of batches: 674
2017:03:21 23:11:50	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:21 23:12:35	use DataLoaderBBCV to init data.
2017:03:21 23:12:35	loading preprocessed files.
2017:03:21 23:12:39	get data info.
2017:03:21 23:12:39	init batch data.
2017:03:21 23:12:39	...number of batches: 674
2017:03:21 23:12:39	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:21 23:13:07	use DataLoaderBBCV to init data.
2017:03:21 23:13:07	loading preprocessed files.
2017:03:21 23:13:11	get data info.
2017:03:21 23:13:11	init batch data.
2017:03:21 23:13:11	...number of batches: 674
2017:03:21 23:13:11	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:21 23:14:35	use DataLoaderBBCV to init data.
2017:03:21 23:14:35	loading preprocessed files.
2017:03:21 23:14:38	get data info.
2017:03:21 23:14:38	init batch data.
2017:03:21 23:14:38	...number of batches: 674
2017:03:21 23:14:38	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:21 23:14:54	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBCV/code.model.textGAN.TextGAN/1490138094

2017:03:21 23:15:09	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBCV/code.model.textGAN.TextGAN/1490138094/checkpoints/bestmodel.

2017:03:21 23:15:09	------ training ------ 

2017:03:21 23:15:09	train epoch 0
2017:03:21 23:16:00	use DataLoaderBBCV to init data.
2017:03:21 23:16:00	loading preprocessed files.
2017:03:21 23:16:04	get data info.
2017:03:21 23:16:04	init batch data.
2017:03:21 23:16:04	...number of batches: 674
2017:03:21 23:16:04	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:21 23:16:20	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBCV/code.model.textGAN.TextGAN/1490138179

2017:03:21 23:16:29	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBCV/code.model.textGAN.TextGAN/1490138179/checkpoints/bestmodel.

2017:03:21 23:16:29	------ training ------ 

2017:03:21 23:16:30	train epoch 0
2017:03:21 23:17:25	use DataLoaderBBCV to init data.
2017:03:21 23:17:25	loading preprocessed files.
2017:03:21 23:17:28	get data info.
2017:03:21 23:17:28	init batch data.
2017:03:21 23:17:28	...number of batches: 674
2017:03:21 23:17:28	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:21 23:17:42	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBCV/code.model.textGAN.TextGAN/1490138261

2017:03:21 23:17:51	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBCV/code.model.textGAN.TextGAN/1490138261/checkpoints/bestmodel.

2017:03:21 23:17:51	------ training ------ 

2017:03:21 23:17:51	train epoch 0
2017:03:21 23:18:25	use DataLoaderBBCV to init data.
2017:03:21 23:18:25	loading preprocessed files.
2017:03:21 23:18:29	get data info.
2017:03:21 23:18:29	init batch data.
2017:03:21 23:18:29	...number of batches: 674
2017:03:21 23:18:29	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:21 23:18:41	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBCV/code.model.textGAN.TextGAN/1490138321

2017:03:21 23:18:52	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBCV/code.model.textGAN.TextGAN/1490138321/checkpoints/bestmodel.

2017:03:21 23:18:52	------ training ------ 

2017:03:21 23:18:52	train epoch 0
2017:03:21 23:34:51	use DataLoaderBBCV to init data.
2017:03:21 23:34:51	loading preprocessed files.
2017:03:21 23:34:54	get data info.
2017:03:21 23:34:54	init batch data.
2017:03:21 23:34:54	...number of batches: 674
2017:03:21 23:35:06	use DataLoaderBBCV to init data.
2017:03:21 23:35:06	loading preprocessed files.
2017:03:21 23:35:09	get data info.
2017:03:21 23:35:09	init batch data.
2017:03:21 23:35:09	...number of batches: 674
2017:03:21 23:35:22	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:21 23:35:37	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBCV/code.model.textGAN.TextGAN/1490139337

2017:03:21 23:35:48	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBCV/code.model.textGAN.TextGAN/1490139337/checkpoints/bestmodel.

2017:03:21 23:35:49	------ training ------ 

2017:03:21 23:35:49	train epoch 0
2017:03:21 23:37:27	use DataLoaderBBCV to init data.
2017:03:21 23:37:27	loading preprocessed files.
2017:03:21 23:37:31	get data info.
2017:03:21 23:37:31	init batch data.
2017:03:21 23:37:31	...number of batches: 674
2017:03:21 23:37:31	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:21 23:37:43	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBCV/code.model.textGAN.TextGAN/1490139463

2017:03:21 23:37:51	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBCV/code.model.textGAN.TextGAN/1490139463/checkpoints/bestmodel.

2017:03:21 23:37:51	------ training ------ 

2017:03:21 23:37:51	train epoch 0
2017:03:22 19:12:59	use DataLoaderBBCV to init data.
2017:03:22 19:12:59	reading and processing the text file.
2017:03:22 19:12:59	preprocess the dataset.
2017:03:22 19:12:59	load data.
2017:03:22 19:12:59	init content from raw.
2017:03:22 19:12:59	init data from the raw dataset.
2017:03:22 19:13:00	load context for further preprocessing.
2017:03:22 19:13:00	...build vocab and map word to index.
2017:03:22 19:13:00	build a vocabulary.
2017:03:22 19:13:00	...flatmap a list of sentence list to a list of sentence.
2017:03:22 19:13:01	...mapping from index to word.
2017:03:22 19:13:01	...add additional <go> and <eos>.
2017:03:22 19:13:01	...mapping from word to index.
2017:03:22 19:13:01	...save processed data to file.
2017:03:22 19:13:03	get data info.
2017:03:22 19:13:03	init batch data.
2017:03:22 19:13:03	...number of batches: 674
2017:03:22 19:13:04	num of sentence: 33700, sentence length: 25, vocab size: 59969
2017:03:22 19:14:25	use DataLoaderBBC to init data.
2017:03:22 19:14:25	reading and processing the text file.
2017:03:22 19:14:25	preprocess the dataset.
2017:03:22 19:14:25	load data.
2017:03:22 19:14:25	init content from raw.
2017:03:22 19:14:25	init data from the raw dataset.
2017:03:22 19:14:26	load context for further preprocessing.
2017:03:22 19:14:26	clean data.
2017:03:22 19:14:27	...mask and pad the sentence.
2017:03:22 19:14:27	......max len:359, median len:22.0, min len:2
2017:03:22 19:14:27	......filter sentence and bound them in the range of 25.
2017:03:22 19:14:27	build a vocabulary.
2017:03:22 19:14:27	...flatmap a list of sentence list to a list of sentence.
2017:03:22 19:14:27	...mapping from index to word.
2017:03:22 19:14:27	...mapping from word to index.
2017:03:22 19:14:27	...map word to index.
2017:03:22 19:14:27	...save processed data to file.
2017:03:22 19:14:28	get data info.
2017:03:22 19:14:28	init batch data.
2017:03:22 19:14:28	...number of batches: 402
2017:03:22 19:14:29	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:22 22:44:28	use DataLoaderBBC to init data.
2017:03:22 22:44:28	loading preprocessed files.
2017:03:22 22:44:30	get data info.
2017:03:22 22:44:30	init batch data.
2017:03:22 22:44:30	...number of batches: 402
2017:03:22 22:44:30	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:22 22:44:43	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGANV1.TextGANV1/1490222682

2017:03:22 22:44:51	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGANV1.TextGANV1/1490222682/checkpoints/bestmodel.

2017:03:22 22:44:51	------ training ------ 

2017:03:22 22:44:51	train epoch 0
2017:03:22 22:45:29	use DataLoaderBBC to init data.
2017:03:22 22:45:29	loading preprocessed files.
2017:03:22 22:45:31	get data info.
2017:03:22 22:45:31	init batch data.
2017:03:22 22:45:31	...number of batches: 402
2017:03:22 22:45:32	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:22 22:45:43	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGANV1.TextGANV1/1490222743

2017:03:22 22:45:52	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGANV1.TextGANV1/1490222743/checkpoints/bestmodel.

2017:03:22 22:45:52	------ training ------ 

2017:03:22 22:45:52	train epoch 0
2017:03:23 09:20:20	use DataLoaderBBC to init data.
2017:03:23 09:20:20	loading preprocessed files.
2017:03:23 09:20:22	get data info.
2017:03:23 09:20:22	init batch data.
2017:03:23 09:20:22	...number of batches: 402
2017:03:23 09:20:22	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:23 09:20:33	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGANV1.TextGANV1/1490260833

2017:03:23 09:20:41	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGANV1.TextGANV1/1490260833/checkpoints/bestmodel.

2017:03:23 09:20:41	------ training ------ 

2017:03:23 09:20:41	train epoch 0
2017:03:23 09:25:15	use DataLoaderBBC to init data.
2017:03:23 09:25:15	loading preprocessed files.
2017:03:23 09:25:17	get data info.
2017:03:23 09:25:17	init batch data.
2017:03:23 09:25:17	...number of batches: 402
2017:03:23 09:25:17	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:23 09:25:29	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGANV1.TextGANV1/1490261129

2017:03:23 09:25:36	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGANV1.TextGANV1/1490261129/checkpoints/bestmodel.

2017:03:23 09:25:36	------ training ------ 

2017:03:23 09:25:36	train epoch 0
2017:03:23 09:26:05	use DataLoaderBBC to init data.
2017:03:23 09:26:05	loading preprocessed files.
2017:03:23 09:26:07	get data info.
2017:03:23 09:26:07	init batch data.
2017:03:23 09:26:07	...number of batches: 402
2017:03:23 09:26:07	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:23 09:26:19	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGANV1.TextGANV1/1490261178

2017:03:23 09:26:26	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGANV1.TextGANV1/1490261178/checkpoints/bestmodel.

2017:03:23 09:26:26	------ training ------ 

2017:03:23 09:26:26	train epoch 0
2017:03:23 09:29:27	use DataLoaderBBC to init data.
2017:03:23 09:29:27	loading preprocessed files.
2017:03:23 09:29:29	get data info.
2017:03:23 09:29:29	init batch data.
2017:03:23 09:29:29	...number of batches: 402
2017:03:23 09:29:29	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:23 09:29:45	use DataLoaderBBC to init data.
2017:03:23 09:29:45	loading preprocessed files.
2017:03:23 09:29:46	get data info.
2017:03:23 09:29:46	init batch data.
2017:03:23 09:29:46	...number of batches: 402
2017:03:23 09:29:47	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:23 09:30:36	use DataLoaderBBC to init data.
2017:03:23 09:30:36	loading preprocessed files.
2017:03:23 09:30:37	get data info.
2017:03:23 09:30:37	init batch data.
2017:03:23 09:30:37	...number of batches: 402
2017:03:23 09:30:38	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:23 09:30:49	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490261449

2017:03:23 09:30:58	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490261449/checkpoints/bestmodel.

2017:03:23 09:30:58	------ training ------ 

2017:03:23 09:30:58	train epoch 0
2017:03:23 09:59:31	use DataLoaderBBC to init data.
2017:03:23 09:59:31	loading preprocessed files.
2017:03:23 09:59:33	get data info.
2017:03:23 09:59:33	init batch data.
2017:03:23 09:59:33	...number of batches: 402
2017:03:23 09:59:33	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:23 09:59:46	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490263186

2017:03:23 09:59:54	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490263186/checkpoints/bestmodel.

2017:03:23 09:59:54	------ training ------ 

2017:03:23 09:59:54	train epoch 0
2017:03:23 13:08:30	use DataLoaderBBC to init data.
2017:03:23 13:08:30	loading preprocessed files.
2017:03:23 13:08:32	get data info.
2017:03:23 13:08:32	init batch data.
2017:03:23 13:08:32	...number of batches: 402
2017:03:23 13:08:32	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:23 13:09:18	use DataLoaderBBC to init data.
2017:03:23 13:09:18	loading preprocessed files.
2017:03:23 13:09:19	get data info.
2017:03:23 13:09:19	init batch data.
2017:03:23 13:09:19	...number of batches: 402
2017:03:23 13:09:19	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:23 13:09:31	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490274571

2017:03:23 13:09:39	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490274571/checkpoints/bestmodel.

2017:03:23 13:09:39	------ training ------ 

2017:03:23 13:09:39	train epoch 0
2017:03:23 13:10:40	use DataLoaderBBC to init data.
2017:03:23 13:10:40	loading preprocessed files.
2017:03:23 13:10:42	get data info.
2017:03:23 13:10:42	init batch data.
2017:03:23 13:10:42	...number of batches: 402
2017:03:23 13:10:43	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:23 13:10:55	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490274654

2017:03:23 13:11:02	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490274654/checkpoints/bestmodel.

2017:03:23 13:11:02	------ training ------ 

2017:03:23 13:11:03	train epoch 0
2017:03:23 13:11:22	use DataLoaderBBC to init data.
2017:03:23 13:11:22	loading preprocessed files.
2017:03:23 13:11:24	get data info.
2017:03:23 13:11:24	init batch data.
2017:03:23 13:11:24	...number of batches: 402
2017:03:23 13:11:24	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:23 13:11:36	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490274696

2017:03:23 13:11:44	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490274696/checkpoints/bestmodel.

2017:03:23 13:11:44	------ training ------ 

2017:03:23 13:11:44	train epoch 0
2017:03:23 13:12:24	use DataLoaderBBC to init data.
2017:03:23 13:12:24	loading preprocessed files.
2017:03:23 13:12:26	get data info.
2017:03:23 13:12:26	init batch data.
2017:03:23 13:12:26	...number of batches: 402
2017:03:23 13:12:26	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:23 13:12:38	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490274757

2017:03:23 13:12:45	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490274757/checkpoints/bestmodel.

2017:03:23 13:12:45	------ training ------ 

2017:03:23 13:12:45	train epoch 0
2017:03:23 13:13:26	use DataLoaderBBC to init data.
2017:03:23 13:13:26	loading preprocessed files.
2017:03:23 13:13:28	get data info.
2017:03:23 13:13:28	init batch data.
2017:03:23 13:13:28	...number of batches: 402
2017:03:23 13:13:28	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:23 13:13:40	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490274820

2017:03:23 13:13:48	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490274820/checkpoints/bestmodel.

2017:03:23 13:13:48	------ training ------ 

2017:03:23 13:13:48	train epoch 0
2017:03:23 13:14:21	use DataLoaderBBC to init data.
2017:03:23 13:14:21	loading preprocessed files.
2017:03:23 13:14:23	get data info.
2017:03:23 13:14:23	init batch data.
2017:03:23 13:14:23	...number of batches: 402
2017:03:23 13:14:23	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:23 13:14:37	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490274877

2017:03:23 13:14:45	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490274877/checkpoints/bestmodel.

2017:03:23 13:14:45	------ training ------ 

2017:03:23 13:14:45	train epoch 0
2017:03:23 17:37:03	use DataLoaderBBC to init data.
2017:03:23 17:37:03	loading preprocessed files.
2017:03:23 17:37:05	get data info.
2017:03:23 17:37:05	init batch data.
2017:03:23 17:37:05	...number of batches: 402
2017:03:23 17:37:05	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:23 17:37:17	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490290637

2017:03:23 17:37:25	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490290637/checkpoints/bestmodel.

2017:03:23 17:37:25	------ training ------ 

2017:03:23 17:37:25	train epoch 0
2017:03:23 17:38:00	use DataLoaderBBC to init data.
2017:03:23 17:38:00	loading preprocessed files.
2017:03:23 17:38:02	get data info.
2017:03:23 17:38:02	init batch data.
2017:03:23 17:38:02	...number of batches: 402
2017:03:23 17:38:02	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:23 17:38:15	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490290695

2017:03:23 17:38:23	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490290695/checkpoints/bestmodel.

2017:03:23 17:38:23	------ training ------ 

2017:03:23 17:38:23	train epoch 0
2017:03:23 17:39:21	use DataLoaderBBC to init data.
2017:03:23 17:39:21	loading preprocessed files.
2017:03:23 17:39:23	get data info.
2017:03:23 17:39:23	init batch data.
2017:03:23 17:39:23	...number of batches: 402
2017:03:23 17:39:23	num of sentence: 20100, sentence length: 25, vocab size: 25281
2017:03:23 17:39:37	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490290776

2017:03:23 17:39:45	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490290776/checkpoints/bestmodel.

2017:03:23 17:39:45	------ training ------ 

2017:03:23 17:39:45	train epoch 0
2017:03:23 17:41:39	use DataLoaderBBC to init data.
2017:03:23 17:41:39	reading and processing the text file.
2017:03:23 17:41:39	preprocess the dataset.
2017:03:23 17:41:39	load data.
2017:03:23 17:41:39	init content from raw.
2017:03:23 17:41:39	init data from the raw dataset.
2017:03:23 17:41:40	load context for further preprocessing.
2017:03:23 17:41:40	clean data.
2017:03:23 17:41:41	...mask and pad the sentence.
2017:03:23 17:41:41	......max len:359, median len:22.0, min len:2
2017:03:23 17:41:41	......filter sentence and bound them in the range of 25.
2017:03:23 17:41:41	build a vocabulary.
2017:03:23 17:41:41	...flatmap a list of sentence list to a list of sentence.
2017:03:23 17:41:41	...mapping from index to word.
2017:03:23 17:41:42	...mapping from word to index.
2017:03:23 17:41:42	...load existing embedding
2017:03:23 17:42:18	use DataLoaderBBC to init data.
2017:03:23 17:42:18	reading and processing the text file.
2017:03:23 17:42:18	preprocess the dataset.
2017:03:23 17:42:18	load data.
2017:03:23 17:42:18	init content from raw.
2017:03:23 17:42:18	init data from the raw dataset.
2017:03:23 17:42:19	load context for further preprocessing.
2017:03:23 17:42:19	clean data.
2017:03:23 17:42:20	...mask and pad the sentence.
2017:03:23 17:42:20	......max len:359, median len:22.0, min len:2
2017:03:23 17:42:20	......filter sentence and bound them in the range of 25.
2017:03:23 17:42:20	build a vocabulary.
2017:03:23 17:42:20	...flatmap a list of sentence list to a list of sentence.
2017:03:23 17:42:21	...mapping from index to word.
2017:03:23 17:42:21	...mapping from word to index.
2017:03:23 17:42:21	...load existing embedding
2017:03:23 17:43:58	use DataLoaderBBC to init data.
2017:03:23 17:43:58	reading and processing the text file.
2017:03:23 17:43:58	preprocess the dataset.
2017:03:23 17:43:58	load data.
2017:03:23 17:43:58	init content from raw.
2017:03:23 17:43:58	init data from the raw dataset.
2017:03:23 17:43:59	load context for further preprocessing.
2017:03:23 17:43:59	clean data.
2017:03:23 17:44:00	...mask and pad the sentence.
2017:03:23 17:44:00	......max len:359, median len:22.0, min len:2
2017:03:23 17:44:00	......filter sentence and bound them in the range of 25.
2017:03:23 17:44:00	build a vocabulary.
2017:03:23 17:44:00	...flatmap a list of sentence list to a list of sentence.
2017:03:23 17:44:01	...mapping from index to word.
2017:03:23 17:44:01	...mapping from word to index.
2017:03:23 17:44:01	...load existing embedding
2017:03:23 17:44:46	use DataLoaderBBC to init data.
2017:03:23 17:44:46	reading and processing the text file.
2017:03:23 17:44:46	preprocess the dataset.
2017:03:23 17:44:46	load data.
2017:03:23 17:44:46	init content from raw.
2017:03:23 17:44:46	init data from the raw dataset.
2017:03:23 17:44:47	load context for further preprocessing.
2017:03:23 17:44:47	clean data.
2017:03:23 17:44:48	...mask and pad the sentence.
2017:03:23 17:44:48	......max len:359, median len:22.0, min len:2
2017:03:23 17:44:48	......filter sentence and bound them in the range of 25.
2017:03:23 17:44:48	build a vocabulary.
2017:03:23 17:44:48	...flatmap a list of sentence list to a list of sentence.
2017:03:23 17:44:49	...mapping from index to word.
2017:03:23 17:44:49	...mapping from word to index.
2017:03:23 17:44:49	...load existing embedding
2017:03:23 17:45:04	# of vocabulary:25278, # of existing_words:19901, num of missing
2017:03:23 17:45:05	...map word to index.
2017:03:23 17:45:05	...save processed data to file.
2017:03:23 17:45:06	get data info.
2017:03:23 17:45:06	init batch data.
2017:03:23 17:45:06	...number of batches: 402
2017:03:23 17:45:06	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:23 17:48:03	use DataLoaderBBC to init data.
2017:03:23 17:48:03	reading and processing the text file.
2017:03:23 17:48:03	preprocess the dataset.
2017:03:23 17:48:03	load data.
2017:03:23 17:48:03	init content from raw.
2017:03:23 17:48:03	init data from the raw dataset.
2017:03:23 17:48:04	load context for further preprocessing.
2017:03:23 17:48:04	clean data.
2017:03:23 17:48:05	...mask and pad the sentence.
2017:03:23 17:48:05	......max len:359, median len:22.0, min len:2
2017:03:23 17:48:05	......filter sentence and bound them in the range of 25.
2017:03:23 17:48:05	build a vocabulary.
2017:03:23 17:48:05	...flatmap a list of sentence list to a list of sentence.
2017:03:23 17:48:05	...mapping from index to word.
2017:03:23 17:48:05	...mapping from word to index.
2017:03:23 17:48:05	...load existing embedding
2017:03:23 17:48:20	# of vocabulary:25278, # of existing_words:19901, # of missing: 5377
2017:03:23 17:48:21	...map word to index.
2017:03:23 17:48:22	...save processed data to file.
2017:03:23 17:48:23	get data info.
2017:03:23 17:48:23	init batch data.
2017:03:23 17:48:23	...number of batches: 402
2017:03:23 17:48:23	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:23 17:48:38	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490291317

2017:03:23 18:59:12	use DataLoaderBBC to init data.
2017:03:23 18:59:12	loading preprocessed files.
2017:03:23 18:59:15	get data info.
2017:03:23 18:59:15	init batch data.
2017:03:23 18:59:15	...number of batches: 402
2017:03:23 18:59:15	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:23 18:59:31	use DataLoaderBBC to init data.
2017:03:23 18:59:31	reading and processing the text file.
2017:03:23 18:59:31	preprocess the dataset.
2017:03:23 18:59:31	load data.
2017:03:23 18:59:31	load context for further preprocessing.
2017:03:23 18:59:31	clean data.
2017:03:23 18:59:32	...mask and pad the sentence.
2017:03:23 18:59:32	......max len:359, median len:22.0, min len:2
2017:03:23 18:59:32	......filter sentence and bound them in the range of 25.
2017:03:23 18:59:32	build a vocabulary.
2017:03:23 18:59:32	...flatmap a list of sentence list to a list of sentence.
2017:03:23 18:59:32	...mapping from index to word.
2017:03:23 18:59:32	...mapping from word to index.
2017:03:23 18:59:32	...load existing embedding
2017:03:23 18:59:46	# of vocabulary:25278, # of existing_words:19902, # of missing: 5376
2017:03:23 19:00:24	use DataLoaderBBC to init data.
2017:03:23 19:00:24	reading and processing the text file.
2017:03:23 19:00:24	preprocess the dataset.
2017:03:23 19:00:24	load data.
2017:03:23 19:00:24	load context for further preprocessing.
2017:03:23 19:00:24	clean data.
2017:03:23 19:00:25	...mask and pad the sentence.
2017:03:23 19:00:25	......max len:359, median len:22.0, min len:2
2017:03:23 19:00:25	......filter sentence and bound them in the range of 25.
2017:03:23 19:00:25	build a vocabulary.
2017:03:23 19:00:25	...flatmap a list of sentence list to a list of sentence.
2017:03:23 19:00:25	...mapping from index to word.
2017:03:23 19:00:26	...mapping from word to index.
2017:03:23 19:00:26	...load existing embedding
2017:03:23 19:00:40	# of vocabulary:25278, # of existing_words:19902, # of missing: 5376
2017:03:23 19:00:53	...map word to index.
2017:03:23 19:00:54	...save processed data to file.
2017:03:23 19:00:55	get data info.
2017:03:23 19:00:55	init batch data.
2017:03:23 19:00:55	...number of batches: 402
2017:03:23 19:00:55	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:23 19:01:09	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490295668

2017:03:23 19:01:17	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490295668/checkpoints/bestmodel.

2017:03:23 19:01:17	------ training ------ 

2017:03:23 19:01:18	train epoch 0
2017:03:23 19:03:13	use DataLoaderBBC to init data.
2017:03:23 19:03:13	reading and processing the text file.
2017:03:23 19:03:13	preprocess the dataset.
2017:03:23 19:03:13	load data.
2017:03:23 19:03:13	load context for further preprocessing.
2017:03:23 19:03:13	clean data.
2017:03:23 19:03:14	...mask and pad the sentence.
2017:03:23 19:03:14	......max len:359, median len:22.0, min len:2
2017:03:23 19:03:14	......filter sentence and bound them in the range of 25.
2017:03:23 19:03:14	build a vocabulary.
2017:03:23 19:03:14	...flatmap a list of sentence list to a list of sentence.
2017:03:23 19:03:15	...mapping from index to word.
2017:03:23 19:03:15	...mapping from word to index.
2017:03:23 19:03:15	...load existing embedding
2017:03:23 19:03:30	# of vocabulary:25278, # of existing_words:19902, # of missing: 5376
2017:03:23 19:03:47	size of vocabulary: (19902, 50)
2017:03:23 19:03:48	...map word to index.
2017:03:23 19:03:49	...save processed data to file.
2017:03:23 19:03:50	get data info.
2017:03:23 19:03:50	init batch data.
2017:03:23 19:03:50	...number of batches: 402
2017:03:23 19:03:50	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:23 19:27:56	use DataLoaderBBC to init data.
2017:03:23 19:27:56	reading and processing the text file.
2017:03:23 19:27:56	preprocess the dataset.
2017:03:23 19:27:56	load data.
2017:03:23 19:27:56	load context for further preprocessing.
2017:03:23 19:27:56	clean data.
2017:03:23 19:27:57	...mask and pad the sentence.
2017:03:23 19:27:57	......max len:359, median len:22.0, min len:2
2017:03:23 19:27:57	......filter sentence and bound them in the range of 25.
2017:03:23 19:27:57	build a vocabulary.
2017:03:23 19:27:57	...flatmap a list of sentence list to a list of sentence.
2017:03:23 19:27:57	...mapping from index to word.
2017:03:23 19:27:57	...mapping from word to index.
2017:03:23 19:27:57	...load existing embedding
2017:03:23 19:28:11	# of vocabulary:25278, # of existing_words:19902, # of missing: 5376
2017:03:23 19:28:25	size of vocabulary: (19902, 50)
2017:03:23 19:28:26	...map word to index.
2017:03:23 19:28:26	...save processed data to file.
2017:03:23 19:28:27	get data info.
2017:03:23 19:28:27	init batch data.
2017:03:23 19:28:27	...number of batches: 402
2017:03:23 19:28:27	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:23 19:28:42	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490297321

2017:03:23 19:28:50	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490297321/checkpoints/bestmodel.

2017:03:23 19:28:50	------ training ------ 

2017:03:23 19:28:50	train epoch 0
2017:03:23 19:31:47	use DataLoaderBBC to init data.
2017:03:23 19:31:47	reading and processing the text file.
2017:03:23 19:31:47	preprocess the dataset.
2017:03:23 19:31:47	load data.
2017:03:23 19:31:47	load context for further preprocessing.
2017:03:23 19:31:47	clean data.
2017:03:23 19:31:48	...mask and pad the sentence.
2017:03:23 19:31:48	......max len:359, median len:22.0, min len:2
2017:03:23 19:31:48	......filter sentence and bound them in the range of 25.
2017:03:23 19:31:48	build a vocabulary.
2017:03:23 19:31:48	...flatmap a list of sentence list to a list of sentence.
2017:03:23 19:31:48	...mapping from index to word.
2017:03:23 19:31:48	...mapping from word to index.
2017:03:23 19:31:48	...load existing embedding
2017:03:23 19:32:06	# of vocabulary:25278, # of existing_words:19902, # of missing: 5376
2017:03:23 19:32:22	size of vocabulary: (19902, 50)
2017:03:23 19:32:23	...map word to index.
2017:03:23 19:32:24	...save processed data to file.
2017:03:23 19:32:25	get data info.
2017:03:23 19:32:25	init batch data.
2017:03:23 19:32:25	...number of batches: 402
2017:03:23 19:32:25	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:23 19:32:46	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490297565

2017:03:23 19:33:01	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490297565/checkpoints/bestmodel.

2017:03:23 19:33:01	------ training ------ 

2017:03:23 19:33:01	train epoch 0
2017:03:23 19:33:59	use DataLoaderBBC to init data.
2017:03:23 19:33:59	reading and processing the text file.
2017:03:23 19:33:59	preprocess the dataset.
2017:03:23 19:33:59	load data.
2017:03:23 19:34:00	load context for further preprocessing.
2017:03:23 19:34:00	clean data.
2017:03:23 19:34:01	...mask and pad the sentence.
2017:03:23 19:34:01	......max len:359, median len:22.0, min len:2
2017:03:23 19:34:01	......filter sentence and bound them in the range of 25.
2017:03:23 19:34:01	build a vocabulary.
2017:03:23 19:34:01	...flatmap a list of sentence list to a list of sentence.
2017:03:23 19:34:01	...mapping from index to word.
2017:03:23 19:34:01	...mapping from word to index.
2017:03:23 19:34:01	...load existing embedding
2017:03:23 19:34:25	# of vocabulary:25278, # of existing_words:19902, # of missing: 5376
2017:03:23 19:34:59	size of vocabulary: (19902, 50)
2017:03:23 19:35:01	...map word to index.
2017:03:23 19:35:02	...save processed data to file.
2017:03:23 19:35:03	get data info.
2017:03:23 19:35:03	init batch data.
2017:03:23 19:35:03	...number of batches: 402
2017:03:23 19:35:03	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:23 19:35:26	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490297726

2017:03:23 19:35:36	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490297726/checkpoints/bestmodel.

2017:03:23 19:35:36	------ training ------ 

2017:03:23 19:35:36	train epoch 0
2017:03:24 10:15:32	use DataLoaderBBC to init data.
2017:03:24 10:15:32	loading preprocessed files.
2017:03:24 10:15:34	get data info.
2017:03:24 10:15:34	init batch data.
2017:03:24 10:15:34	...number of batches: 402
2017:03:24 10:15:34	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:24 10:16:32	use DataLoaderBBC to init data.
2017:03:24 10:16:32	loading preprocessed files.
2017:03:24 10:16:34	get data info.
2017:03:24 10:16:34	init batch data.
2017:03:24 10:16:34	...number of batches: 402
2017:03:24 10:16:34	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:24 10:16:35	enter standard generator mode.
2017:03:24 10:16:35	enter GAN's generator mode.
2017:03:24 10:18:07	use DataLoaderBBC to init data.
2017:03:24 10:18:07	loading preprocessed files.
2017:03:24 10:18:10	get data info.
2017:03:24 10:18:10	init batch data.
2017:03:24 10:18:10	...number of batches: 402
2017:03:24 10:18:10	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:24 10:18:10	enter standard generator mode.
2017:03:24 10:18:11	enter GAN's generator mode.
2017:03:24 10:18:32	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490350711

2017:03:24 10:18:35	use DataLoaderBBC to init data.
2017:03:24 10:18:35	loading preprocessed files.
2017:03:24 10:18:36	get data info.
2017:03:24 10:18:36	init batch data.
2017:03:24 10:18:36	...number of batches: 402
2017:03:24 10:18:36	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:24 10:18:37	enter standard generator mode.
2017:03:24 10:18:38	enter GAN's generator mode.
2017:03:24 10:18:54	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490350734

2017:03:24 10:18:58	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490350734

2017:03:24 10:19:10	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490350734/checkpoints/bestmodel.

2017:03:24 10:19:10	------ pretraining ------ 

2017:03:24 10:19:10	train epoch 0
2017:03:24 10:24:54	use DataLoaderBBC to init data.
2017:03:24 10:24:54	loading preprocessed files.
2017:03:24 10:24:56	get data info.
2017:03:24 10:24:56	init batch data.
2017:03:24 10:24:56	...number of batches: 402
2017:03:24 10:24:56	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:24 10:24:56	enter standard generator mode.
2017:03:24 10:24:57	enter GAN's generator mode.
2017:03:24 10:25:13	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490351112

2017:03:24 10:25:17	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490351112

2017:03:24 10:25:29	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490351112/checkpoints/bestmodel.

2017:03:24 10:25:29	------ pretraining ------ 

2017:03:24 10:25:29	train epoch 0
2017:03:24 10:26:16	use DataLoaderBBC to init data.
2017:03:24 10:26:16	loading preprocessed files.
2017:03:24 10:26:18	get data info.
2017:03:24 10:26:18	init batch data.
2017:03:24 10:26:18	...number of batches: 402
2017:03:24 10:26:18	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:24 10:26:18	enter standard generator mode.
2017:03:24 10:26:19	enter GAN's generator mode.
2017:03:24 10:26:37	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490351196

2017:03:24 10:26:53	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490351196/checkpoints/bestmodel.

2017:03:24 10:26:53	------ pretraining ------ 

2017:03:24 10:26:53	pretrain epoch 0
2017:03:24 10:30:58	use DataLoaderBBC to init data.
2017:03:24 10:30:58	loading preprocessed files.
2017:03:24 10:31:00	get data info.
2017:03:24 10:31:00	init batch data.
2017:03:24 10:31:00	...number of batches: 402
2017:03:24 10:31:00	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:24 10:31:01	enter standard generator mode.
2017:03:24 10:31:01	enter GAN's generator mode.
2017:03:24 10:31:22	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490351481

2017:03:24 10:31:37	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490351481/checkpoints/bestmodel.

2017:03:24 10:31:37	------ pretraining ------ 

2017:03:24 10:31:37	pretrain epoch 0
2017:03:24 10:36:25	use DataLoaderBBC to init data.
2017:03:24 10:36:25	loading preprocessed files.
2017:03:24 10:36:27	get data info.
2017:03:24 10:36:27	init batch data.
2017:03:24 10:36:27	...number of batches: 20
2017:03:24 10:36:27	num of sentence: 1000, sentence length: 25, vocab size: 25278
2017:03:24 10:36:28	enter standard generator mode.
2017:03:24 10:36:28	enter GAN's generator mode.
2017:03:24 10:36:45	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490351804

2017:03:24 10:37:00	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490351804/checkpoints/bestmodel.

2017:03:24 10:37:00	------ pretraining ------ 

2017:03:24 10:37:00	pretrain epoch 0
2017:03:24 10:37:30	pretrain loss d: 0.655119776726, pretrain loss g: 9.99217224121, execution speed: 1.45 seconds/batch
2017:03:24 10:37:32	save 2-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490351804/checkpoints/bestmodel.

2017:03:24 10:37:32	------ training ------ 

2017:03:24 10:37:32	train epoch 0
2017:03:24 10:57:51	use DataLoaderBBC to init data.
2017:03:24 10:57:51	reading and processing the text file.
2017:03:24 10:57:51	preprocess the dataset.
2017:03:24 10:57:51	load data.
2017:03:24 10:57:51	load context for further preprocessing.
2017:03:24 10:57:51	clean data.
2017:03:24 10:57:52	...mask and pad the sentence.
2017:03:24 10:57:52	......max len:359, median len:22.0, min len:2
2017:03:24 10:57:52	......filter sentence and bound them in the range of 25.
2017:03:24 10:57:52	build a vocabulary.
2017:03:24 10:57:52	...flatmap a list of sentence list to a list of sentence.
2017:03:24 10:57:53	...mapping from index to word.
2017:03:24 10:57:53	...mapping from word to index.
2017:03:24 10:57:53	...map word to index.
2017:03:24 10:57:53	...save processed data to file.
2017:03:24 10:57:54	get data info.
2017:03:24 10:57:54	init batch data.
2017:03:24 10:57:54	...number of batches: 20
2017:03:24 10:57:54	num of sentence: 1000, sentence length: 25, vocab size: 25278
2017:03:24 10:58:02	use DataLoaderBBC to init data.
2017:03:24 10:58:02	reading and processing the text file.
2017:03:24 10:58:02	preprocess the dataset.
2017:03:24 10:58:02	load data.
2017:03:24 10:58:02	load context for further preprocessing.
2017:03:24 10:58:02	clean data.
2017:03:24 10:58:03	...mask and pad the sentence.
2017:03:24 10:58:03	......max len:359, median len:22.0, min len:2
2017:03:24 10:58:03	......filter sentence and bound them in the range of 25.
2017:03:24 10:58:03	build a vocabulary.
2017:03:24 10:58:03	...flatmap a list of sentence list to a list of sentence.
2017:03:24 10:58:03	...mapping from index to word.
2017:03:24 10:58:03	...mapping from word to index.
2017:03:24 10:58:03	...map word to index.
2017:03:24 10:58:04	...save processed data to file.
2017:03:24 10:58:05	get data info.
2017:03:24 10:58:05	init batch data.
2017:03:24 10:58:05	...number of batches: 10
2017:03:24 10:58:05	num of sentence: 500, sentence length: 25, vocab size: 25278
2017:03:24 10:59:41	use DataLoaderBBC to init data.
2017:03:24 10:59:41	reading and processing the text file.
2017:03:24 10:59:41	preprocess the dataset.
2017:03:24 10:59:41	load data.
2017:03:24 10:59:41	load context for further preprocessing.
2017:03:24 10:59:41	clean data.
2017:03:24 10:59:42	...mask and pad the sentence.
2017:03:24 10:59:42	......max len:359, median len:22.0, min len:2
2017:03:24 10:59:42	......filter sentence and bound them in the range of 25.
2017:03:24 10:59:42	build a vocabulary.
2017:03:24 10:59:42	...flatmap a list of sentence list to a list of sentence.
2017:03:24 10:59:42	...mapping from index to word.
2017:03:24 10:59:42	...mapping from word to index.
2017:03:24 10:59:42	...load existing embedding
2017:03:24 10:59:56	# of vocabulary:25278, # of existing_words:19902, # of missing: 5376
2017:03:24 11:00:07	size of vocabulary: (19902, 50)
2017:03:24 11:00:08	...map word to index.
2017:03:24 11:00:08	...save processed data to file.
2017:03:24 11:00:09	get data info.
2017:03:24 11:00:09	init batch data.
2017:03:24 11:00:09	...number of batches: 10
2017:03:24 11:00:09	num of sentence: 500, sentence length: 25, vocab size: 25278
2017:03:24 11:01:32	use DataLoaderBBC to init data.
2017:03:24 11:01:32	reading and processing the text file.
2017:03:24 11:01:32	preprocess the dataset.
2017:03:24 11:01:32	load data.
2017:03:24 11:01:32	load context for further preprocessing.
2017:03:24 11:01:32	clean data.
2017:03:24 11:01:33	...mask and pad the sentence.
2017:03:24 11:01:33	......max len:359, median len:22.0, min len:2
2017:03:24 11:01:33	......filter sentence and bound them in the range of 25.
2017:03:24 11:01:33	build a vocabulary.
2017:03:24 11:01:33	...flatmap a list of sentence list to a list of sentence.
2017:03:24 11:01:33	...mapping from index to word.
2017:03:24 11:01:33	...mapping from word to index.
2017:03:24 11:01:33	...load existing embedding
2017:03:24 11:01:47	# of vocabulary:25278, # of existing_words:19902, # of missing: 5376
2017:03:24 11:01:57	size of vocabulary: (19902, 50)
2017:03:24 11:01:58	...map word to index.
2017:03:24 11:01:58	...save processed data to file.
2017:03:24 11:01:59	get data info.
2017:03:24 11:01:59	init batch data.
2017:03:24 11:01:59	...number of batches: 10
2017:03:24 11:01:59	num of sentence: 500, sentence length: 25, vocab size: 25278
2017:03:24 11:02:00	enter standard generator mode.
2017:03:24 11:02:01	enter GAN's generator mode.
2017:03:24 11:02:20	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490353339

2017:03:24 11:02:37	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490353339/checkpoints/bestmodel.

2017:03:24 11:02:37	------ pretraining ------ 

2017:03:24 11:02:37	pretrain epoch 0
2017:03:24 11:02:57	pretrain loss d: 0.735584616661, pretrain loss g: 10.1250476837, execution speed: 1.90 seconds/batch
2017:03:24 11:02:59	save 2-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490353339/checkpoints/bestmodel.

2017:03:24 11:02:59	------ training ------ 

2017:03:24 11:02:59	train epoch 0
2017:03:24 11:03:12	train loss d: 0.172809556127, train loss g: 3.39446926117, execution speed: 1.20 seconds/batch
2017:03:24 11:03:14	val loss d: 0.0278111603111, val loss g: 4.93084049225, execution speed: 0.20 seconds/batch

2017:03:24 11:03:16	save 3-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490353339/checkpoints/bestmodel.

2017:03:24 11:03:16	train epoch 1
2017:03:24 11:17:02	use DataLoaderBBC to init data.
2017:03:24 11:17:02	reading and processing the text file.
2017:03:24 11:17:02	preprocess the dataset.
2017:03:24 11:17:02	load data.
2017:03:24 11:17:02	load context for further preprocessing.
2017:03:24 11:17:02	clean data.
2017:03:24 11:17:03	...mask and pad the sentence.
2017:03:24 11:17:03	......max len:359, median len:22.0, min len:2
2017:03:24 11:17:03	......filter sentence and bound them in the range of 25.
2017:03:24 11:17:03	build a vocabulary.
2017:03:24 11:17:03	...flatmap a list of sentence list to a list of sentence.
2017:03:24 11:17:03	...mapping from index to word.
2017:03:24 11:17:06	use DataLoaderBBC to init data.
2017:03:24 11:17:06	reading and processing the text file.
2017:03:24 11:17:06	preprocess the dataset.
2017:03:24 11:17:06	load data.
2017:03:24 11:17:06	load context for further preprocessing.
2017:03:24 11:17:06	clean data.
2017:03:24 11:17:07	...mask and pad the sentence.
2017:03:24 11:17:07	......max len:359, median len:22.0, min len:2
2017:03:24 11:17:07	......filter sentence and bound them in the range of 25.
2017:03:24 11:17:07	build a vocabulary.
2017:03:24 11:17:07	...flatmap a list of sentence list to a list of sentence.
2017:03:24 11:17:07	...mapping from index to word.
2017:03:24 11:17:07	...mapping from word to index.
2017:03:24 11:17:07	...load existing embedding
2017:03:24 11:17:21	# of vocabulary:25278, # of existing_words:19902, # of missing: 5376
2017:03:24 11:17:32	size of vocabulary: (19902, 50)
2017:03:24 11:17:33	...map word to index.
2017:03:24 11:17:33	...save processed data to file.
2017:03:24 11:17:34	get data info.
2017:03:24 11:17:34	init batch data.
2017:03:24 11:17:34	...number of batches: 10
2017:03:24 11:17:34	num of sentence: 500, sentence length: 25, vocab size: 25278
2017:03:24 11:17:35	enter standard seq2seq's generator mode.
2017:03:24 11:17:35	enter GAN's generator mode.
2017:03:24 11:17:51	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490354271

2017:03:24 11:18:07	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490354271/checkpoints/bestmodel.

2017:03:24 11:18:07	------ pretraining ------ 

2017:03:24 11:18:08	pretrain epoch 0
2017:03:24 11:20:06	use DataLoaderBBC to init data.
2017:03:24 11:20:06	reading and processing the text file.
2017:03:24 11:20:06	preprocess the dataset.
2017:03:24 11:20:06	load data.
2017:03:24 11:20:06	load context for further preprocessing.
2017:03:24 11:20:06	clean data.
2017:03:24 11:20:07	...mask and pad the sentence.
2017:03:24 11:20:07	......max len:359, median len:22.0, min len:2
2017:03:24 11:20:07	......filter sentence and bound them in the range of 25.
2017:03:24 11:20:07	build a vocabulary.
2017:03:24 11:20:07	...flatmap a list of sentence list to a list of sentence.
2017:03:24 11:20:07	...mapping from index to word.
2017:03:24 11:20:07	...mapping from word to index.
2017:03:24 11:20:07	...load existing embedding
2017:03:24 11:20:20	# of vocabulary:25278, # of existing_words:19902, # of missing: 5376
2017:03:24 11:20:31	size of vocabulary: (19902, 50)
2017:03:24 11:20:32	...map word to index.
2017:03:24 11:20:32	...save processed data to file.
2017:03:24 11:20:33	get data info.
2017:03:24 11:20:33	init batch data.
2017:03:24 11:20:33	...number of batches: 10
2017:03:24 11:20:33	num of sentence: 500, sentence length: 25, vocab size: 25278
2017:03:24 11:20:34	enter standard seq2seq's generator mode.
2017:03:24 11:20:35	enter GAN's generator mode.
2017:03:24 11:20:51	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490354450

2017:03:24 11:21:07	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490354450/checkpoints/bestmodel.

2017:03:24 11:21:07	------ pretraining ------ 

2017:03:24 11:21:07	pretrain epoch 0
2017:03:24 11:22:01	use DataLoaderBBC to init data.
2017:03:24 11:22:01	reading and processing the text file.
2017:03:24 11:22:01	preprocess the dataset.
2017:03:24 11:22:01	load data.
2017:03:24 11:22:01	load context for further preprocessing.
2017:03:24 11:22:01	clean data.
2017:03:24 11:22:02	...mask and pad the sentence.
2017:03:24 11:22:02	......max len:359, median len:22.0, min len:2
2017:03:24 11:22:02	......filter sentence and bound them in the range of 25.
2017:03:24 11:22:02	build a vocabulary.
2017:03:24 11:22:02	...flatmap a list of sentence list to a list of sentence.
2017:03:24 11:22:02	...mapping from index to word.
2017:03:24 11:22:02	...mapping from word to index.
2017:03:24 11:22:02	...load existing embedding
2017:03:24 11:22:16	# of vocabulary:25278, # of existing_words:19902, # of missing: 5376
2017:03:24 11:22:28	size of vocabulary: (19902, 50)
2017:03:24 11:22:29	...map word to index.
2017:03:24 11:22:30	...save processed data to file.
2017:03:24 11:22:31	get data info.
2017:03:24 11:22:31	init batch data.
2017:03:24 11:22:31	...number of batches: 10
2017:03:24 11:22:31	num of sentence: 500, sentence length: 25, vocab size: 25278
2017:03:24 11:22:31	enter standard seq2seq's generator mode.
2017:03:24 11:22:32	enter GAN's generator mode.
2017:03:24 11:22:49	writing to /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490354569

2017:03:24 11:23:06	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490354569/checkpoints/bestmodel.

2017:03:24 11:23:06	------ pretraining ------ 

2017:03:24 11:23:06	pretrain epoch 0
2017:03:24 11:44:55	use DataLoaderBBC to init data.
2017:03:24 11:44:55	reading and processing the text file.
2017:03:24 11:44:55	preprocess the dataset.
2017:03:24 11:44:55	load data.
2017:03:24 11:44:55	load context for further preprocessing.
2017:03:24 11:44:55	clean data.
2017:03:24 11:44:56	...mask and pad the sentence.
2017:03:24 11:44:56	......max len:359, median len:22.0, min len:2
2017:03:24 11:44:56	......filter sentence and bound them in the range of 25.
2017:03:24 11:44:56	build a vocabulary.
2017:03:24 11:44:56	...flatmap a list of sentence list to a list of sentence.
2017:03:24 11:44:56	...mapping from index to word.
2017:03:24 11:44:56	...mapping from word to index.
2017:03:24 11:44:56	...load existing embedding
2017:03:24 11:45:14	# of vocabulary:25278, # of existing_words:19902, # of missing: 5376
2017:03:24 11:45:28	size of vocabulary: (19902, 50)
2017:03:24 11:45:30	...map word to index.
2017:03:24 11:45:31	...save processed data to file.
2017:03:24 11:45:32	get data info.
2017:03:24 11:45:32	init batch data.
2017:03:24 11:45:32	...number of batches: 10
2017:03:24 11:45:32	num of sentence: 500, sentence length: 25, vocab size: 25278
2017:03:24 11:45:32	enter standard seq2seq's generator mode.
2017:03:24 11:45:33	enter GAN's generator mode.
2017:03:24 11:46:09	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490355949/checkpoints/bestmodel.

2017:03:24 11:46:09	------ pretraining ------ 

2017:03:24 11:46:10	pretrain epoch 0
2017:03:24 11:47:39	use DataLoaderBBC to init data.
2017:03:24 11:47:39	reading and processing the text file.
2017:03:24 11:47:39	preprocess the dataset.
2017:03:24 11:47:39	load data.
2017:03:24 11:47:39	load context for further preprocessing.
2017:03:24 11:47:39	clean data.
2017:03:24 11:47:39	...mask and pad the sentence.
2017:03:24 11:47:39	......max len:359, median len:22.0, min len:2
2017:03:24 11:47:40	......filter sentence and bound them in the range of 25.
2017:03:24 11:47:40	build a vocabulary.
2017:03:24 11:47:40	...flatmap a list of sentence list to a list of sentence.
2017:03:24 11:47:40	...mapping from index to word.
2017:03:24 11:47:40	...mapping from word to index.
2017:03:24 11:47:40	...load existing embedding
2017:03:24 11:47:55	# of vocabulary:25278, # of existing_words:19902, # of missing: 5376
2017:03:24 11:48:16	size of vocabulary: (19902, 50)
2017:03:24 11:48:32	...map word to index.
2017:03:24 11:48:33	...save processed data to file.
2017:03:24 11:48:34	get data info.
2017:03:24 11:48:34	init batch data.
2017:03:24 11:48:34	...number of batches: 10
2017:03:24 11:48:34	num of sentence: 500, sentence length: 25, vocab size: 25278
2017:03:24 11:48:35	enter standard seq2seq's generator mode.
2017:03:24 11:48:35	enter GAN's generator mode.
2017:03:24 11:49:57	use DataLoaderBBC to init data.
2017:03:24 11:49:57	reading and processing the text file.
2017:03:24 11:49:57	preprocess the dataset.
2017:03:24 11:49:57	load data.
2017:03:24 11:49:57	load context for further preprocessing.
2017:03:24 11:49:57	clean data.
2017:03:24 11:49:58	...mask and pad the sentence.
2017:03:24 11:49:58	......max len:359, median len:22.0, min len:2
2017:03:24 11:49:58	......filter sentence and bound them in the range of 25.
2017:03:24 11:49:58	build a vocabulary.
2017:03:24 11:49:58	...flatmap a list of sentence list to a list of sentence.
2017:03:24 11:49:59	...mapping from index to word.
2017:03:24 11:49:59	...mapping from word to index.
2017:03:24 11:49:59	...load existing embedding
2017:03:24 11:50:14	# of vocabulary:25278, # of existing_words:19902, # of missing: 5376
2017:03:24 11:50:27	size of vocabulary: (19902, 50)
2017:03:24 11:50:28	...map word to index.
2017:03:24 11:50:28	...save processed data to file.
2017:03:24 11:50:29	get data info.
2017:03:24 11:50:29	init batch data.
2017:03:24 11:50:29	...number of batches: 10
2017:03:24 11:50:29	num of sentence: 500, sentence length: 25, vocab size: 25278
2017:03:24 11:50:30	enter standard seq2seq's generator mode.
2017:03:24 11:50:31	enter GAN's generator mode.
2017:03:24 11:51:07	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490356250/checkpoints/bestmodel.

2017:03:24 11:51:07	------ pretraining ------ 

2017:03:24 11:51:07	pretrain epoch 0
2017:03:25 21:27:33	use DataLoaderBBC to init data.
2017:03:25 21:27:33	reading and processing the text file.
2017:03:25 21:27:33	preprocess the dataset.
2017:03:25 21:27:33	load data.
2017:03:25 21:27:33	init content from raw.
2017:03:25 21:27:33	init data from the raw dataset.
2017:03:25 21:28:25	use DataLoaderBBC to init data.
2017:03:25 21:28:25	reading and processing the text file.
2017:03:25 21:28:25	preprocess the dataset.
2017:03:25 21:28:25	load data.
2017:03:25 21:28:25	init content from raw.
2017:03:25 21:28:25	init data from the raw dataset.
2017:03:25 21:29:40	use DataLoaderBBC to init data.
2017:03:25 21:29:40	reading and processing the text file.
2017:03:25 21:29:40	preprocess the dataset.
2017:03:25 21:29:40	load data.
2017:03:25 21:29:40	init content from raw.
2017:03:25 21:29:40	init data from the raw dataset.
2017:03:25 21:33:28	use DataLoaderBBC to init data.
2017:03:25 21:33:28	reading and processing the text file.
2017:03:25 21:33:28	preprocess the dataset.
2017:03:25 21:33:28	load data.
2017:03:25 21:33:28	load context for further preprocessing.
2017:03:25 21:33:28	clean data.
2017:03:25 21:33:28	...mask and pad the sentence.
2017:03:25 21:33:28	......max len:359, median len:22.0, min len:2
2017:03:25 21:33:28	......filter sentence and bound them in the range of 25.
2017:03:25 21:33:28	build a vocabulary.
2017:03:25 21:33:28	...flatmap a list of sentence list to a list of sentence.
2017:03:25 21:33:29	...mapping from index to word.
2017:03:25 21:33:29	...mapping from word to index.
2017:03:25 21:33:29	...map word to index.
2017:03:25 21:33:29	...save processed data to file.
2017:03:25 21:33:30	get data info.
2017:03:25 21:33:30	init batch data.
2017:03:25 21:33:30	...number of batches: 402
2017:03:25 21:33:30	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:25 21:33:31	enter standard seq2seq's generator mode.
2017:03:25 21:33:32	enter GAN's generator mode.
2017:03:25 21:34:09	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490477630/checkpoints/bestmodel.

2017:03:25 21:34:09	------ pretraining ------ 

2017:03:25 21:34:10	pretrain epoch 0
2017:03:25 21:35:22	use DataLoaderBBC to init data.
2017:03:25 21:35:22	reading and processing the text file.
2017:03:25 21:35:22	preprocess the dataset.
2017:03:25 21:35:22	load data.
2017:03:25 21:35:22	load context for further preprocessing.
2017:03:25 21:35:22	clean data.
2017:03:25 21:35:23	...mask and pad the sentence.
2017:03:25 21:35:23	......max len:359, median len:22.0, min len:2
2017:03:25 21:35:23	......filter sentence and bound them in the range of 25.
2017:03:25 21:35:23	build a vocabulary.
2017:03:25 21:35:23	...flatmap a list of sentence list to a list of sentence.
2017:03:25 21:35:23	...mapping from index to word.
2017:03:25 21:35:23	...mapping from word to index.
2017:03:25 21:35:23	...map word to index.
2017:03:25 21:35:24	...save processed data to file.
2017:03:25 21:35:24	get data info.
2017:03:25 21:35:24	init batch data.
2017:03:25 21:35:24	...number of batches: 402
2017:03:25 21:35:25	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:25 21:35:25	enter standard seq2seq's generator mode.
2017:03:25 21:35:25	enter GAN's generator mode.
2017:03:25 21:36:27	use DataLoaderBBC to init data.
2017:03:25 21:36:27	reading and processing the text file.
2017:03:25 21:36:27	preprocess the dataset.
2017:03:25 21:36:27	load data.
2017:03:25 21:36:27	load context for further preprocessing.
2017:03:25 21:36:27	clean data.
2017:03:25 21:36:28	...mask and pad the sentence.
2017:03:25 21:36:28	......max len:359, median len:22.0, min len:2
2017:03:25 21:36:28	......filter sentence and bound them in the range of 25.
2017:03:25 21:36:28	build a vocabulary.
2017:03:25 21:36:28	...flatmap a list of sentence list to a list of sentence.
2017:03:25 21:36:28	...mapping from index to word.
2017:03:25 21:36:28	...mapping from word to index.
2017:03:25 21:36:28	...map word to index.
2017:03:25 21:36:28	...save processed data to file.
2017:03:25 21:36:29	get data info.
2017:03:25 21:36:29	init batch data.
2017:03:25 21:36:29	...number of batches: 402
2017:03:25 21:36:29	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:25 21:36:30	enter standard seq2seq's generator mode.
2017:03:25 21:36:30	enter GAN's generator mode.
2017:03:25 21:41:59	use DataLoaderBBC to init data.
2017:03:25 21:41:59	reading and processing the text file.
2017:03:25 21:41:59	preprocess the dataset.
2017:03:25 21:41:59	load data.
2017:03:25 21:41:59	load context for further preprocessing.
2017:03:25 21:41:59	clean data.
2017:03:25 21:41:59	...mask and pad the sentence.
2017:03:25 21:41:59	......max len:359, median len:22.0, min len:2
2017:03:25 21:41:59	......filter sentence and bound them in the range of 25.
2017:03:25 21:41:59	build a vocabulary.
2017:03:25 21:41:59	...flatmap a list of sentence list to a list of sentence.
2017:03:25 21:42:00	...mapping from index to word.
2017:03:25 21:42:00	...mapping from word to index.
2017:03:25 21:42:00	...map word to index.
2017:03:25 21:42:00	...save processed data to file.
2017:03:25 21:42:01	get data info.
2017:03:25 21:42:01	init batch data.
2017:03:25 21:42:01	...number of batches: 402
2017:03:25 21:42:01	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:25 21:42:02	enter standard seq2seq's generator mode.
2017:03:25 21:42:03	enter GAN's generator mode.
2017:03:25 21:43:24	use DataLoaderBBC to init data.
2017:03:25 21:43:24	reading and processing the text file.
2017:03:25 21:43:24	preprocess the dataset.
2017:03:25 21:43:24	load data.
2017:03:25 21:43:24	load context for further preprocessing.
2017:03:25 21:43:24	clean data.
2017:03:25 21:43:25	...mask and pad the sentence.
2017:03:25 21:43:25	......max len:359, median len:22.0, min len:2
2017:03:25 21:43:25	......filter sentence and bound them in the range of 25.
2017:03:25 21:43:25	build a vocabulary.
2017:03:25 21:43:25	...flatmap a list of sentence list to a list of sentence.
2017:03:25 21:43:25	...mapping from index to word.
2017:03:25 21:43:25	...mapping from word to index.
2017:03:25 21:43:25	...map word to index.
2017:03:25 21:43:25	...save processed data to file.
2017:03:25 21:43:26	get data info.
2017:03:25 21:43:26	init batch data.
2017:03:25 21:43:26	...number of batches: 402
2017:03:25 21:43:26	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:25 21:43:27	enter standard seq2seq's generator mode.
2017:03:25 21:43:27	enter GAN's generator mode.
2017:03:25 21:43:40	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490478214/checkpoints/bestmodel.

2017:03:25 21:43:40	------ pretraining ------ 

2017:03:25 21:43:40	pretrain epoch 0
2017:03:25 22:01:17	use DataLoaderBBC to init data.
2017:03:25 22:01:17	reading and processing the text file.
2017:03:25 22:01:17	preprocess the dataset.
2017:03:25 22:01:17	load data.
2017:03:25 22:01:17	load context for further preprocessing.
2017:03:25 22:01:17	clean data.
2017:03:25 22:01:18	...mask and pad the sentence.
2017:03:25 22:01:18	......max len:359, median len:22.0, min len:2
2017:03:25 22:01:18	......filter sentence and bound them in the range of 25.
2017:03:25 22:01:18	build a vocabulary.
2017:03:25 22:01:18	...flatmap a list of sentence list to a list of sentence.
2017:03:25 22:01:18	...mapping from index to word.
2017:03:25 22:01:18	...mapping from word to index.
2017:03:25 22:01:18	...map word to index.
2017:03:25 22:01:18	...save processed data to file.
2017:03:25 22:01:19	get data info.
2017:03:25 22:01:19	init batch data.
2017:03:25 22:01:19	...number of batches: 402
2017:03:25 22:01:19	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:25 22:01:20	enter standard seq2seq's generator mode.
2017:03:25 22:01:20	enter GAN's generator mode.
2017:03:25 22:05:11	use DataLoaderBBC to init data.
2017:03:25 22:05:11	reading and processing the text file.
2017:03:25 22:05:11	preprocess the dataset.
2017:03:25 22:05:11	load data.
2017:03:25 22:05:11	load context for further preprocessing.
2017:03:25 22:05:11	clean data.
2017:03:25 22:05:12	...mask and pad the sentence.
2017:03:25 22:05:12	......max len:359, median len:22.0, min len:2
2017:03:25 22:05:12	......filter sentence and bound them in the range of 25.
2017:03:25 22:05:12	build a vocabulary.
2017:03:25 22:05:12	...flatmap a list of sentence list to a list of sentence.
2017:03:25 22:05:12	...mapping from index to word.
2017:03:25 22:05:13	...mapping from word to index.
2017:03:25 22:05:13	...map word to index.
2017:03:25 22:05:13	...save processed data to file.
2017:03:25 22:05:14	get data info.
2017:03:25 22:05:14	init batch data.
2017:03:25 22:05:14	...number of batches: 402
2017:03:25 22:05:14	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:25 22:05:14	enter standard seq2seq's generator mode.
2017:03:25 22:05:15	enter GAN's generator mode.
2017:03:25 22:05:30	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490479522/checkpoints/bestmodel.

2017:03:25 22:05:30	------ pretraining ------ 

2017:03:25 22:05:30	pretrain epoch 0
2017:03:25 22:17:35	use DataLoaderBBC to init data.
2017:03:25 22:17:35	reading and processing the text file.
2017:03:25 22:17:35	preprocess the dataset.
2017:03:25 22:17:35	load data.
2017:03:25 22:17:35	load context for further preprocessing.
2017:03:25 22:17:35	clean data.
2017:03:25 22:17:36	...mask and pad the sentence.
2017:03:25 22:17:36	......max len:359, median len:22.0, min len:2
2017:03:25 22:17:36	......filter sentence and bound them in the range of 25.
2017:03:25 22:17:36	build a vocabulary.
2017:03:25 22:17:36	...flatmap a list of sentence list to a list of sentence.
2017:03:25 22:17:36	...mapping from index to word.
2017:03:25 22:17:36	...mapping from word to index.
2017:03:25 22:17:36	...map word to index.
2017:03:25 22:17:36	...save processed data to file.
2017:03:25 22:17:37	get data info.
2017:03:25 22:17:37	init batch data.
2017:03:25 22:17:37	...number of batches: 402
2017:03:25 22:17:37	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:25 22:17:38	enter standard seq2seq's generator mode.
2017:03:25 22:19:18	use DataLoaderBBC to init data.
2017:03:25 22:19:18	reading and processing the text file.
2017:03:25 22:19:18	preprocess the dataset.
2017:03:25 22:19:18	load data.
2017:03:25 22:19:18	load context for further preprocessing.
2017:03:25 22:19:18	clean data.
2017:03:25 22:19:19	...mask and pad the sentence.
2017:03:25 22:19:19	......max len:359, median len:22.0, min len:2
2017:03:25 22:19:19	......filter sentence and bound them in the range of 25.
2017:03:25 22:19:19	build a vocabulary.
2017:03:25 22:19:19	...flatmap a list of sentence list to a list of sentence.
2017:03:25 22:19:19	...mapping from index to word.
2017:03:25 22:19:19	...mapping from word to index.
2017:03:25 22:19:19	...map word to index.
2017:03:25 22:19:20	...save processed data to file.
2017:03:25 22:19:21	get data info.
2017:03:25 22:19:21	init batch data.
2017:03:25 22:19:21	...number of batches: 402
2017:03:25 22:19:21	num of sentence: 20100, sentence length: 25, vocab size: 25278
2017:03:25 22:19:21	enter standard seq2seq's generator mode.
2017:03:25 22:19:22	enter GAN's generator mode.
2017:03:25 22:19:41	save 1-th bestmodel to path: /home/tlin/notebooks/code/demo3_v/data/training/runs/DataLoaderBBC/code.model.textGAN.TextGAN/1490480371/checkpoints/bestmodel.

2017:03:25 22:19:41	------ pretraining ------ 

2017:03:25 22:19:41	------ training ------ 

2017:03:25 22:19:41	train epoch 0
